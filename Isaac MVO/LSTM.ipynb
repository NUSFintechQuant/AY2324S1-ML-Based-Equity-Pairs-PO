{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9e1027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.28)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from yfinance) (1.24.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2023.3)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2.3.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\user\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2022.9.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057bcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe52ce8",
   "metadata": {},
   "source": [
    "## Stock Feature Selection: Adj Close, Volume, RSI, ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9d9c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_csv('tickers.csv')\n",
    "tickers.sort_values(by='Market Cap', ascending=False, inplace=True)\n",
    "top18_tickers = tickers[:20]\n",
    "top18_tickers.reset_index(inplace=True)\n",
    "top18_tickers = top18_tickers.drop([7,8])\n",
    "stocks = top18_tickers['Symbol'].to_list()\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2019-01-01'\n",
    "data = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
    "data = data.reset_index()\n",
    "dates = data['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c7b4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_returns(list_of_stocks_tickers, start_date, end_date, interval='1d'):\n",
    "    stocks = list()\n",
    "    for ticker in list_of_stocks_tickers:\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, interval=interval)\n",
    "        data = {ticker: np.log(data['Adj Close']) - np.log(data['Adj Close'].shift(1))}\n",
    "        log_return = pd.DataFrame(data=data)\n",
    "        stocks.append(log_return)\n",
    "    all_stocks = reduce(lambda df1, df2: pd.merge(df1, df2, on='Date'), stocks)\n",
    "    all_stocks['Date'] = log_return.index\n",
    "    return all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5faccdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL',\n",
       " 'MSFT',\n",
       " 'GOOG',\n",
       " 'GOOGL',\n",
       " 'AMZN',\n",
       " 'NVDA',\n",
       " 'TSLA',\n",
       " 'META',\n",
       " 'HSBC',\n",
       " 'LLY',\n",
       " 'TSM',\n",
       " 'V',\n",
       " 'UNH',\n",
       " 'XOM',\n",
       " 'JPM',\n",
       " 'WMT',\n",
       " 'NVO',\n",
       " 'JNJ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e6f66a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "returns = get_log_returns(stocks, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "404f26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.to_csv(\"base_returns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c775bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(prices, n=14):\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed >= 0].sum()/n\n",
    "    down = -seed[seed < 0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1]  # The diff is 1 shorter\n",
    "\n",
    "        if delta > 0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a06ac46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, period=14):\n",
    "    # Calculate price differences\n",
    "    price_diff = np.diff(data)\n",
    "    \n",
    "    # Calculate initial seed values\n",
    "    seed = price_diff[:period + 1]\n",
    "    positive_seed = seed[seed >= 0].sum()\n",
    "    negative_seed = -seed[seed < 0].sum()\n",
    "    \n",
    "    # Calculate initial RS and RSI\n",
    "    initial_rs = positive_seed / negative_seed\n",
    "    initial_rsi = 100 - 100 / (1 + initial_rs)\n",
    "    \n",
    "    # Initialize up and down values\n",
    "    up = positive_seed / period\n",
    "    down = negative_seed / period\n",
    "    \n",
    "    # Initialize RSI array\n",
    "    rsi = np.zeros_like(data)\n",
    "    rsi[:period] = initial_rsi\n",
    "\n",
    "    for i in range(period, len(data)):\n",
    "        # Calculate delta for the current period\n",
    "        delta = price_diff[i - 1]\n",
    "        \n",
    "        if delta > 0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "        \n",
    "        # Update up and down values\n",
    "        up = (up * (period - 1) + upval) / period\n",
    "        down = (down * (period - 1) + downval) / period\n",
    "        \n",
    "        # Calculate RS and RSI for the current period\n",
    "        current_rs = up / down\n",
    "        rsi[i] = 100 - 100 / (1 + current_rs)\n",
    "\n",
    "    return rsi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5327a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atr(data, period=14):\n",
    "    data = np.array(data)\n",
    "    \n",
    "    high = data[:, 0]\n",
    "    low = data[:, 1]\n",
    "    close = data[:, 2]\n",
    "    \n",
    "    tr = np.maximum(high - low, np.abs(high - np.roll(close, 1)), np.abs(low - np.roll(close, 1)))\n",
    "    \n",
    "    atr = np.zeros_like(tr)\n",
    "    atr[period] = np.mean(tr[:period])\n",
    "    \n",
    "    for i in range(period + 1, len(tr)):\n",
    "        atr[i] = ((period - 1) * atr[i - 1] + tr[i]) / period\n",
    "    \n",
    "    return atr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e7307",
   "metadata": {},
   "source": [
    "## Model Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13669c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"BATCH_SIZE\": 50,\n",
    "    \"EPOCHS\": 10,\n",
    "    \"LR\": 0.00010000,\n",
    "    \"TIME_STEPS\": 60\n",
    "    }\n",
    "\n",
    "TIME_STEPS = params['TIME_STEPS']\n",
    "BATCH_SIZE = params['BATCH_SIZE']\n",
    "\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    \n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "\n",
    "    print(\"Length of inputs\", dim_0)\n",
    "\n",
    "    for i in range(dim_0):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "\n",
    "    print(\"length of time-series - inputs\", x.shape)\n",
    "    print(\"length of time-series - outputs\", y.shape)\n",
    "\n",
    "\n",
    "    return x, y\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71371983",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3ff5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_tuner in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: keras-core in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (0.1.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (2.31.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (1.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from keras-core->keras_tuner) (1.24.4)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (13.5.3)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (3.7.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (0.1.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->keras_tuner) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (2022.9.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras-core->keras_tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras-core->keras_tuner) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_tuner) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd0cdadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation,concatenate, Attention, Bidirectional,GlobalAveragePooling1D\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96234952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hyperparameters):\n",
    "    lstm_model = Sequential()\n",
    "    \n",
    "    hyperparameters_units = hyperparameters.Int('units', min_value=50, max_value=100, step=TIME_STEPS)\n",
    "    lstm_model.add(LSTM(units=hyperparameters_units, \n",
    "                        input_shape=(x_t.shape[1], x_t.shape[2]), \n",
    "                        return_sequences=True,\n",
    "                        kernel_initializer='he_normal'))\n",
    "    lstm_model.add(GlobalAveragePooling1D())\n",
    "    lstm_model.add(Dense(60,activation='relu'))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dropout(0.05))\n",
    "    lstm_model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    hyperparameters_learning_rate = hyperparameters.Choice('learning_rate', values=[0.01, 0.05, 0.1])\n",
    "    \n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(learning_rate=hyperparameters_learning_rate))\n",
    "    \n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9c990c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     23.009684  0.000000 2013-01-03  352965200 -0.012703\n",
      "2     23.009684  0.000000 2013-01-04  594333600 -0.028250\n",
      "3     23.009684  0.000000 2013-01-07  484156400 -0.005900\n",
      "4     23.009684  0.000000 2013-01-08  458707200  0.002688\n",
      "5     23.009684  0.000000 2013-01-09  407604400 -0.015752\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  22.983592  1.395621 2018-12-24  148676800 -0.026215\n",
      "1506  36.492563  1.483613 2018-12-26  234330000  0.068053\n",
      "1507  35.825001  1.497283 2018-12-27  212468400 -0.006511\n",
      "1508  35.924049  1.461227 2018-12-28  169165600  0.000512\n",
      "1509  37.872277  1.412747 2018-12-31  140014000  0.009619\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 2.30096839e+01  0.00000000e+00  3.52965200e+08 -1.27027104e-02]\n",
      " [ 2.30096839e+01  0.00000000e+00  5.94333600e+08 -2.82499616e-02]\n",
      " [ 2.30096839e+01  0.00000000e+00  4.84156400e+08 -5.89961289e-03]\n",
      " ...\n",
      " [ 4.67901812e+01  5.68823932e-01  8.59928000e+07  1.75808513e-04]\n",
      " [ 4.82718871e+01  5.52658122e-01  6.59208000e+07  2.80987110e-03]\n",
      " [ 4.32703819e+01  5.37646740e-01  1.03999600e+08 -1.08726587e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "AAPL None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9332WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 170ms/step - loss: 0.9332\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0490WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 167ms/step - loss: 0.0490\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0297WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 170ms/step - loss: 0.0297\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0263WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 171ms/step - loss: 0.0263\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0138WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0138\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0147WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 172ms/step - loss: 0.0147\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0148WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 189ms/step - loss: 0.0148\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0138WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 180ms/step - loss: 0.0138\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0153WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 164ms/step - loss: 0.0153\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0147WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0147\n",
      "6/6 [==============================] - 1s 55ms/step\n",
      "[[0.2826395 ]\n",
      " [0.28270802]\n",
      " [0.28302032]\n",
      " [0.28306913]\n",
      " [0.28338563]\n",
      " [0.2831688 ]\n",
      " [0.2837855 ]\n",
      " [0.28475127]\n",
      " [0.28507328]\n",
      " [0.28461617]\n",
      " [0.28224716]\n",
      " [0.2827154 ]\n",
      " [0.28274855]\n",
      " [0.28327158]\n",
      " [0.2829146 ]\n",
      " [0.28363976]\n",
      " [0.2848707 ]\n",
      " [0.2855465 ]\n",
      " [0.2858355 ]\n",
      " [0.28514686]\n",
      " [0.2852636 ]\n",
      " [0.28549814]\n",
      " [0.28581363]\n",
      " [0.28593636]\n",
      " [0.28606513]\n",
      " [0.28591534]\n",
      " [0.28585944]\n",
      " [0.28573817]\n",
      " [0.28479275]\n",
      " [0.28399602]\n",
      " [0.2844587 ]\n",
      " [0.2842319 ]\n",
      " [0.284335  ]\n",
      " [0.2848758 ]\n",
      " [0.28526458]\n",
      " [0.2854178 ]\n",
      " [0.28503826]\n",
      " [0.2844085 ]\n",
      " [0.28264764]\n",
      " [0.28332788]\n",
      " [0.2830367 ]\n",
      " [0.2826442 ]\n",
      " [0.28261518]\n",
      " [0.2823137 ]\n",
      " [0.28255418]\n",
      " [0.28273442]\n",
      " [0.28374892]\n",
      " [0.2837057 ]\n",
      " [0.28384906]\n",
      " [0.2839436 ]\n",
      " [0.28433126]\n",
      " [0.28500786]\n",
      " [0.28434396]\n",
      " [0.28431624]\n",
      " [0.28456324]\n",
      " [0.28450164]\n",
      " [0.2820475 ]\n",
      " [0.28237128]\n",
      " [0.2826293 ]\n",
      " [0.28178844]\n",
      " [0.28317493]\n",
      " [0.28315955]\n",
      " [0.2836782 ]\n",
      " [0.28430745]\n",
      " [0.28414196]\n",
      " [0.28410843]\n",
      " [0.28412202]\n",
      " [0.2846291 ]\n",
      " [0.2850494 ]\n",
      " [0.28463304]\n",
      " [0.28521684]\n",
      " [0.285327  ]\n",
      " [0.28491047]\n",
      " [0.28427038]\n",
      " [0.2839912 ]\n",
      " [0.2820027 ]\n",
      " [0.28071567]\n",
      " [0.28114027]\n",
      " [0.27939278]\n",
      " [0.27953508]\n",
      " [0.2801246 ]\n",
      " [0.28002763]\n",
      " [0.2760416 ]\n",
      " [0.2763838 ]\n",
      " [0.2803621 ]\n",
      " [0.27849188]\n",
      " [0.27755052]\n",
      " [0.27898163]\n",
      " [0.28090534]\n",
      " [0.2813069 ]\n",
      " [0.28195974]\n",
      " [0.28275982]\n",
      " [0.28283015]\n",
      " [0.2828016 ]\n",
      " [0.28264913]\n",
      " [0.28311905]\n",
      " [0.28367782]\n",
      " [0.28409854]\n",
      " [0.2839995 ]\n",
      " [0.28390408]\n",
      " [0.28282043]\n",
      " [0.2832418 ]\n",
      " [0.2835224 ]\n",
      " [0.28354552]\n",
      " [0.28307268]\n",
      " [0.2837162 ]\n",
      " [0.28423026]\n",
      " [0.28451014]\n",
      " [0.28393853]\n",
      " [0.2835516 ]\n",
      " [0.28367823]\n",
      " [0.2832214 ]\n",
      " [0.28233045]\n",
      " [0.2824054 ]\n",
      " [0.2806067 ]\n",
      " [0.27987644]\n",
      " [0.27899083]\n",
      " [0.28191498]\n",
      " [0.2801406 ]\n",
      " [0.27991578]\n",
      " [0.28045312]\n",
      " [0.2801336 ]\n",
      " [0.28093272]\n",
      " [0.2816581 ]\n",
      " [0.2819394 ]\n",
      " [0.280704  ]\n",
      " [0.28144297]\n",
      " [0.2822125 ]\n",
      " [0.28219613]\n",
      " [0.282662  ]\n",
      " [0.2828831 ]\n",
      " [0.28331095]\n",
      " [0.28374863]\n",
      " [0.2836308 ]\n",
      " [0.2816349 ]\n",
      " [0.27927312]\n",
      " [0.2800546 ]\n",
      " [0.27945554]\n",
      " [0.28006878]\n",
      " [0.2802446 ]\n",
      " [0.27961308]\n",
      " [0.2808673 ]\n",
      " [0.28181055]\n",
      " [0.2829219 ]\n",
      " [0.28349683]\n",
      " [0.2839938 ]\n",
      " [0.28430772]\n",
      " [0.28456175]\n",
      " [0.28479546]\n",
      " [0.28502   ]\n",
      " [0.28487188]\n",
      " [0.2848994 ]\n",
      " [0.28457826]\n",
      " [0.2848971 ]\n",
      " [0.28462097]\n",
      " [0.2845391 ]\n",
      " [0.28475177]\n",
      " [0.28469127]\n",
      " [0.28484678]\n",
      " [0.2847805 ]\n",
      " [0.28492865]\n",
      " [0.2846904 ]\n",
      " [0.28462046]\n",
      " [0.28436512]\n",
      " [0.2850898 ]\n",
      " [0.28532565]\n",
      " [0.28557804]\n",
      " [0.28566188]\n",
      " [0.28548378]\n",
      " [0.28483963]\n",
      " [0.28480378]\n",
      " [0.28503156]\n",
      " [0.28434658]\n",
      " [0.28424364]\n",
      " [0.28278592]\n",
      " [0.2832639 ]\n",
      " [0.28163445]\n",
      " [0.28230456]\n",
      " [0.2816954 ]\n",
      " [0.28142262]\n",
      " [0.28023612]\n",
      " [0.28171065]\n",
      " [0.28151757]\n",
      " [0.28230157]\n",
      " [0.2820592 ]\n",
      " [0.28294426]\n",
      " [0.28158608]\n",
      " [0.2824004 ]\n",
      " [0.28339195]\n",
      " [0.28412047]\n",
      " [0.28411698]\n",
      " [0.2832395 ]\n",
      " [0.28407982]\n",
      " [0.284335  ]\n",
      " [0.28418186]\n",
      " [0.28427488]\n",
      " [0.28390884]\n",
      " [0.28424165]\n",
      " [0.2841088 ]\n",
      " [0.28425765]\n",
      " [0.28468722]\n",
      " [0.2851269 ]\n",
      " [0.2847749 ]\n",
      " [0.28312972]\n",
      " [0.2826337 ]\n",
      " [0.28275087]\n",
      " [0.28459558]\n",
      " [0.28522828]\n",
      " [0.28539857]\n",
      " [0.28555477]\n",
      " [0.2851293 ]\n",
      " [0.28523242]\n",
      " [0.28541842]\n",
      " [0.28526604]\n",
      " [0.2854267 ]\n",
      " [0.2855499 ]\n",
      " [0.28558195]\n",
      " [0.2858371 ]\n",
      " [0.2859674 ]\n",
      " [0.28540543]\n",
      " [0.28540415]\n",
      " [0.28549096]\n",
      " [0.2855775 ]\n",
      " [0.28570142]\n",
      " [0.28587124]\n",
      " [0.2859913 ]\n",
      " [0.28610656]\n",
      " [0.28599244]\n",
      " [0.28612086]\n",
      " [0.28617856]\n",
      " [0.28572473]\n",
      " [0.28462017]\n",
      " [0.2840258 ]\n",
      " [0.28309348]\n",
      " [0.28393152]\n",
      " [0.28306624]\n",
      " [0.28384158]\n",
      " [0.28315997]\n",
      " [0.2816357 ]\n",
      " [0.28183994]\n",
      " [0.2819715 ]\n",
      " [0.28219098]\n",
      " [0.2808693 ]\n",
      " [0.2825431 ]\n",
      " [0.28287354]\n",
      " [0.2824967 ]\n",
      " [0.28326988]\n",
      " [0.28359166]\n",
      " [0.28386348]\n",
      " [0.28417927]\n",
      " [0.2843995 ]]\n",
      "[0.2826395  0.28270802 0.28302032 0.28306913 0.28338563 0.2831688\n",
      " 0.2837855  0.28475127 0.28507328 0.28461617 0.28224716 0.2827154\n",
      " 0.28274855 0.28327158 0.2829146  0.28363976 0.2848707  0.2855465\n",
      " 0.2858355  0.28514686 0.2852636  0.28549814 0.28581363 0.28593636\n",
      " 0.28606513 0.28591534 0.28585944 0.28573817 0.28479275 0.28399602\n",
      " 0.2844587  0.2842319  0.284335   0.2848758  0.28526458 0.2854178\n",
      " 0.28503826 0.2844085  0.28264764 0.28332788 0.2830367  0.2826442\n",
      " 0.28261518 0.2823137  0.28255418 0.28273442 0.28374892 0.2837057\n",
      " 0.28384906 0.2839436  0.28433126 0.28500786 0.28434396 0.28431624\n",
      " 0.28456324 0.28450164 0.2820475  0.28237128 0.2826293  0.28178844\n",
      " 0.28317493 0.28315955 0.2836782  0.28430745 0.28414196 0.28410843\n",
      " 0.28412202 0.2846291  0.2850494  0.28463304 0.28521684 0.285327\n",
      " 0.28491047 0.28427038 0.2839912  0.2820027  0.28071567 0.28114027\n",
      " 0.27939278 0.27953508 0.2801246  0.28002763 0.2760416  0.2763838\n",
      " 0.2803621  0.27849188 0.27755052 0.27898163 0.28090534 0.2813069\n",
      " 0.28195974 0.28275982 0.28283015 0.2828016  0.28264913 0.28311905\n",
      " 0.28367782 0.28409854 0.2839995  0.28390408 0.28282043 0.2832418\n",
      " 0.2835224  0.28354552 0.28307268 0.2837162  0.28423026 0.28451014\n",
      " 0.28393853 0.2835516  0.28367823 0.2832214  0.28233045 0.2824054\n",
      " 0.2806067  0.27987644 0.27899083 0.28191498 0.2801406  0.27991578\n",
      " 0.28045312 0.2801336  0.28093272 0.2816581  0.2819394  0.280704\n",
      " 0.28144297 0.2822125  0.28219613 0.282662   0.2828831  0.28331095\n",
      " 0.28374863 0.2836308  0.2816349  0.27927312 0.2800546  0.27945554\n",
      " 0.28006878 0.2802446  0.27961308 0.2808673  0.28181055 0.2829219\n",
      " 0.28349683 0.2839938  0.28430772 0.28456175 0.28479546 0.28502\n",
      " 0.28487188 0.2848994  0.28457826 0.2848971  0.28462097 0.2845391\n",
      " 0.28475177 0.28469127 0.28484678 0.2847805  0.28492865 0.2846904\n",
      " 0.28462046 0.28436512 0.2850898  0.28532565 0.28557804 0.28566188\n",
      " 0.28548378 0.28483963 0.28480378 0.28503156 0.28434658 0.28424364\n",
      " 0.28278592 0.2832639  0.28163445 0.28230456 0.2816954  0.28142262\n",
      " 0.28023612 0.28171065 0.28151757 0.28230157 0.2820592  0.28294426\n",
      " 0.28158608 0.2824004  0.28339195 0.28412047 0.28411698 0.2832395\n",
      " 0.28407982 0.284335   0.28418186 0.28427488 0.28390884 0.28424165\n",
      " 0.2841088  0.28425765 0.28468722 0.2851269  0.2847749  0.28312972\n",
      " 0.2826337  0.28275087 0.28459558 0.28522828 0.28539857 0.28555477\n",
      " 0.2851293  0.28523242 0.28541842 0.28526604 0.2854267  0.2855499\n",
      " 0.28558195 0.2858371  0.2859674  0.28540543 0.28540415 0.28549096\n",
      " 0.2855775  0.28570142 0.28587124 0.2859913  0.28610656 0.28599244\n",
      " 0.28612086 0.28617856 0.28572473 0.28462017 0.2840258  0.28309348\n",
      " 0.28393152 0.28306624 0.28384158 0.28315997 0.2816357  0.28183994\n",
      " 0.2819715  0.28219098 0.2808693  0.2825431  0.28287354 0.2824967\n",
      " 0.28326988 0.28359166 0.28386348 0.28417927 0.2843995 ]\n",
      "AAPL 0.09301694929043162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     50.157734  0.000000 2013-01-03  48294400 -0.013487\n",
      "2     50.157734  0.000000 2013-01-04  52521100 -0.018893\n",
      "3     50.157734  0.000000 2013-01-07  37110400 -0.001872\n",
      "4     50.157734  0.000000 2013-01-08  44703100 -0.005259\n",
      "5     50.157734  0.000000 2013-01-09  49047900  0.005634\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  29.476164  3.722587 2018-12-24  43935200 -0.042635\n",
      "1506  43.112583  3.937402 2018-12-26  51634800  0.066078\n",
      "1507  44.232346  3.998302 2018-12-27  49498500  0.006146\n",
      "1508  43.069046  3.919138 2018-12-28  38196300 -0.007838\n",
      "1509  45.379688  3.782771 2018-12-31  33173800  0.011685\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.01577338e+01  0.00000000e+00  4.82944000e+07 -1.34866373e-02]\n",
      " [ 5.01577338e+01  0.00000000e+00  5.25211000e+07 -1.88929878e-02]\n",
      " [ 5.01577338e+01  0.00000000e+00  3.71104000e+07 -1.87150602e-03]\n",
      " ...\n",
      " [ 5.88216115e+01  1.17761943e+00  1.46780000e+07  3.62352434e-03]\n",
      " [ 5.88745439e+01  1.12064642e+00  1.05943000e+07  1.16669558e-04]\n",
      " [ 5.74443864e+01  1.07988618e+00  1.87174000e+07 -2.10213749e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 392)           622496    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 392)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                23580     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 647317 (2.47 MB)\n",
      "Trainable params: 647317 (2.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "MSFT None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1712WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 134ms/step - loss: 0.1712\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0084WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 129ms/step - loss: 0.0084\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0031WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 131ms/step - loss: 0.0031\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0027WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 131ms/step - loss: 0.0027\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 151ms/step - loss: 0.0028\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0029WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 137ms/step - loss: 0.0029\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0031WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 132ms/step - loss: 0.0031\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0033WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 130ms/step - loss: 0.0033\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0037WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 135ms/step - loss: 0.0037\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0035WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 141ms/step - loss: 0.0035\n",
      "6/6 [==============================] - 1s 47ms/step\n",
      "[[0.17912278]\n",
      " [0.17906284]\n",
      " [0.17905557]\n",
      " [0.17905715]\n",
      " [0.17903359]\n",
      " [0.17904875]\n",
      " [0.17903787]\n",
      " [0.17902845]\n",
      " [0.17903247]\n",
      " [0.17899539]\n",
      " [0.178981  ]\n",
      " [0.1790415 ]\n",
      " [0.17902255]\n",
      " [0.17904395]\n",
      " [0.17905807]\n",
      " [0.17904212]\n",
      " [0.17930418]\n",
      " [0.17922986]\n",
      " [0.17919022]\n",
      " [0.17914593]\n",
      " [0.1791553 ]\n",
      " [0.17911994]\n",
      " [0.1791356 ]\n",
      " [0.1791065 ]\n",
      " [0.17914659]\n",
      " [0.17915694]\n",
      " [0.17912084]\n",
      " [0.17908388]\n",
      " [0.179159  ]\n",
      " [0.17911178]\n",
      " [0.17916715]\n",
      " [0.17910656]\n",
      " [0.17901978]\n",
      " [0.17912853]\n",
      " [0.17907113]\n",
      " [0.17897388]\n",
      " [0.17902303]\n",
      " [0.17916188]\n",
      " [0.17911083]\n",
      " [0.17916363]\n",
      " [0.17927141]\n",
      " [0.17920634]\n",
      " [0.17911653]\n",
      " [0.17917821]\n",
      " [0.17909929]\n",
      " [0.17914377]\n",
      " [0.17915738]\n",
      " [0.17917567]\n",
      " [0.17917365]\n",
      " [0.1790969 ]\n",
      " [0.17933096]\n",
      " [0.17918706]\n",
      " [0.17917839]\n",
      " [0.17916119]\n",
      " [0.17911014]\n",
      " [0.17907518]\n",
      " [0.17902046]\n",
      " [0.17904598]\n",
      " [0.17902169]\n",
      " [0.179034  ]\n",
      " [0.17904958]\n",
      " [0.17904413]\n",
      " [0.17901245]\n",
      " [0.1790444 ]\n",
      " [0.17903873]\n",
      " [0.17904139]\n",
      " [0.17899689]\n",
      " [0.17895353]\n",
      " [0.17911196]\n",
      " [0.17909408]\n",
      " [0.1790866 ]\n",
      " [0.17909086]\n",
      " [0.17908955]\n",
      " [0.17905486]\n",
      " [0.17906769]\n",
      " [0.17909881]\n",
      " [0.1790407 ]\n",
      " [0.17909442]\n",
      " [0.17917705]\n",
      " [0.17913295]\n",
      " [0.17925385]\n",
      " [0.17933577]\n",
      " [0.17937426]\n",
      " [0.17926943]\n",
      " [0.17950472]\n",
      " [0.17942697]\n",
      " [0.17936704]\n",
      " [0.17948721]\n",
      " [0.17935227]\n",
      " [0.17929384]\n",
      " [0.179346  ]\n",
      " [0.17935935]\n",
      " [0.17934372]\n",
      " [0.17937759]\n",
      " [0.17931317]\n",
      " [0.1792481 ]\n",
      " [0.17928746]\n",
      " [0.17935452]\n",
      " [0.17931342]\n",
      " [0.1793476 ]\n",
      " [0.1793533 ]\n",
      " [0.17932099]\n",
      " [0.17929114]\n",
      " [0.17925555]\n",
      " [0.17927206]\n",
      " [0.17923646]\n",
      " [0.1793288 ]\n",
      " [0.17934272]\n",
      " [0.17934908]\n",
      " [0.17929871]\n",
      " [0.17927468]\n",
      " [0.17942442]\n",
      " [0.17927934]\n",
      " [0.1792276 ]\n",
      " [0.17927158]\n",
      " [0.17931296]\n",
      " [0.1791784 ]\n",
      " [0.17953707]\n",
      " [0.1795181 ]\n",
      " [0.17945845]\n",
      " [0.1795254 ]\n",
      " [0.17946643]\n",
      " [0.17939024]\n",
      " [0.17946908]\n",
      " [0.179475  ]\n",
      " [0.17945856]\n",
      " [0.17939731]\n",
      " [0.17944047]\n",
      " [0.17937368]\n",
      " [0.17942727]\n",
      " [0.1793713 ]\n",
      " [0.17933777]\n",
      " [0.17940672]\n",
      " [0.17938456]\n",
      " [0.17940825]\n",
      " [0.17940384]\n",
      " [0.1794065 ]\n",
      " [0.17944598]\n",
      " [0.17938223]\n",
      " [0.17946276]\n",
      " [0.17958766]\n",
      " [0.17947876]\n",
      " [0.17948501]\n",
      " [0.17941229]\n",
      " [0.17942023]\n",
      " [0.17937982]\n",
      " [0.1794152 ]\n",
      " [0.17936681]\n",
      " [0.17938578]\n",
      " [0.1793682 ]\n",
      " [0.17931113]\n",
      " [0.1793415 ]\n",
      " [0.17933026]\n",
      " [0.17930321]\n",
      " [0.17925918]\n",
      " [0.17922378]\n",
      " [0.17926373]\n",
      " [0.17920977]\n",
      " [0.17929186]\n",
      " [0.1793108 ]\n",
      " [0.17927217]\n",
      " [0.17928538]\n",
      " [0.17927578]\n",
      " [0.17928489]\n",
      " [0.1792795 ]\n",
      " [0.1792678 ]\n",
      " [0.17923741]\n",
      " [0.17928714]\n",
      " [0.17925912]\n",
      " [0.17925566]\n",
      " [0.17922372]\n",
      " [0.17920983]\n",
      " [0.17923635]\n",
      " [0.17928055]\n",
      " [0.17940181]\n",
      " [0.17921136]\n",
      " [0.17921564]\n",
      " [0.17925578]\n",
      " [0.17924751]\n",
      " [0.1793493 ]\n",
      " [0.1792488 ]\n",
      " [0.17925861]\n",
      " [0.17921537]\n",
      " [0.1792309 ]\n",
      " [0.17920518]\n",
      " [0.17923275]\n",
      " [0.17915106]\n",
      " [0.17915937]\n",
      " [0.17919204]\n",
      " [0.17919475]\n",
      " [0.17920132]\n",
      " [0.1791478 ]\n",
      " [0.17920749]\n",
      " [0.17924975]\n",
      " [0.17920315]\n",
      " [0.17927736]\n",
      " [0.179297  ]\n",
      " [0.17929515]\n",
      " [0.17938015]\n",
      " [0.17929885]\n",
      " [0.17922816]\n",
      " [0.17934334]\n",
      " [0.17938706]\n",
      " [0.17944056]\n",
      " [0.17935416]\n",
      " [0.17930993]\n",
      " [0.17925997]\n",
      " [0.17930236]\n",
      " [0.17925791]\n",
      " [0.17923221]\n",
      " [0.17919359]\n",
      " [0.17918214]\n",
      " [0.17918316]\n",
      " [0.17920004]\n",
      " [0.17913508]\n",
      " [0.17920211]\n",
      " [0.17923343]\n",
      " [0.17917444]\n",
      " [0.17916249]\n",
      " [0.17915115]\n",
      " [0.17910892]\n",
      " [0.1791004 ]\n",
      " [0.17909281]\n",
      " [0.17907646]\n",
      " [0.17909889]\n",
      " [0.17906538]\n",
      " [0.17911163]\n",
      " [0.17910951]\n",
      " [0.17913833]\n",
      " [0.17921892]\n",
      " [0.17917718]\n",
      " [0.17913091]\n",
      " [0.17907289]\n",
      " [0.179061  ]\n",
      " [0.1791319 ]\n",
      " [0.17909138]\n",
      " [0.17914246]\n",
      " [0.17914909]\n",
      " [0.17909926]\n",
      " [0.17916992]\n",
      " [0.17909312]\n",
      " [0.17914873]\n",
      " [0.17935485]\n",
      " [0.17918405]\n",
      " [0.17915612]\n",
      " [0.17909986]\n",
      " [0.17909071]\n",
      " [0.17905945]\n",
      " [0.17908037]\n",
      " [0.17906927]\n",
      " [0.17912322]]\n",
      "[0.17912278 0.17906284 0.17905557 0.17905715 0.17903359 0.17904875\n",
      " 0.17903787 0.17902845 0.17903247 0.17899539 0.178981   0.1790415\n",
      " 0.17902255 0.17904395 0.17905807 0.17904212 0.17930418 0.17922986\n",
      " 0.17919022 0.17914593 0.1791553  0.17911994 0.1791356  0.1791065\n",
      " 0.17914659 0.17915694 0.17912084 0.17908388 0.179159   0.17911178\n",
      " 0.17916715 0.17910656 0.17901978 0.17912853 0.17907113 0.17897388\n",
      " 0.17902303 0.17916188 0.17911083 0.17916363 0.17927141 0.17920634\n",
      " 0.17911653 0.17917821 0.17909929 0.17914377 0.17915738 0.17917567\n",
      " 0.17917365 0.1790969  0.17933096 0.17918706 0.17917839 0.17916119\n",
      " 0.17911014 0.17907518 0.17902046 0.17904598 0.17902169 0.179034\n",
      " 0.17904958 0.17904413 0.17901245 0.1790444  0.17903873 0.17904139\n",
      " 0.17899689 0.17895353 0.17911196 0.17909408 0.1790866  0.17909086\n",
      " 0.17908955 0.17905486 0.17906769 0.17909881 0.1790407  0.17909442\n",
      " 0.17917705 0.17913295 0.17925385 0.17933577 0.17937426 0.17926943\n",
      " 0.17950472 0.17942697 0.17936704 0.17948721 0.17935227 0.17929384\n",
      " 0.179346   0.17935935 0.17934372 0.17937759 0.17931317 0.1792481\n",
      " 0.17928746 0.17935452 0.17931342 0.1793476  0.1793533  0.17932099\n",
      " 0.17929114 0.17925555 0.17927206 0.17923646 0.1793288  0.17934272\n",
      " 0.17934908 0.17929871 0.17927468 0.17942442 0.17927934 0.1792276\n",
      " 0.17927158 0.17931296 0.1791784  0.17953707 0.1795181  0.17945845\n",
      " 0.1795254  0.17946643 0.17939024 0.17946908 0.179475   0.17945856\n",
      " 0.17939731 0.17944047 0.17937368 0.17942727 0.1793713  0.17933777\n",
      " 0.17940672 0.17938456 0.17940825 0.17940384 0.1794065  0.17944598\n",
      " 0.17938223 0.17946276 0.17958766 0.17947876 0.17948501 0.17941229\n",
      " 0.17942023 0.17937982 0.1794152  0.17936681 0.17938578 0.1793682\n",
      " 0.17931113 0.1793415  0.17933026 0.17930321 0.17925918 0.17922378\n",
      " 0.17926373 0.17920977 0.17929186 0.1793108  0.17927217 0.17928538\n",
      " 0.17927578 0.17928489 0.1792795  0.1792678  0.17923741 0.17928714\n",
      " 0.17925912 0.17925566 0.17922372 0.17920983 0.17923635 0.17928055\n",
      " 0.17940181 0.17921136 0.17921564 0.17925578 0.17924751 0.1793493\n",
      " 0.1792488  0.17925861 0.17921537 0.1792309  0.17920518 0.17923275\n",
      " 0.17915106 0.17915937 0.17919204 0.17919475 0.17920132 0.1791478\n",
      " 0.17920749 0.17924975 0.17920315 0.17927736 0.179297   0.17929515\n",
      " 0.17938015 0.17929885 0.17922816 0.17934334 0.17938706 0.17944056\n",
      " 0.17935416 0.17930993 0.17925997 0.17930236 0.17925791 0.17923221\n",
      " 0.17919359 0.17918214 0.17918316 0.17920004 0.17913508 0.17920211\n",
      " 0.17923343 0.17917444 0.17916249 0.17915115 0.17910892 0.1791004\n",
      " 0.17909281 0.17907646 0.17909889 0.17906538 0.17911163 0.17910951\n",
      " 0.17913833 0.17921892 0.17917718 0.17913091 0.17907289 0.179061\n",
      " 0.1791319  0.17909138 0.17914246 0.17914909 0.17909926 0.17916992\n",
      " 0.17909312 0.17914873 0.17935485 0.17918405 0.17915612 0.17909986\n",
      " 0.17909071 0.17905945 0.17908037 0.17906927 0.17912322]\n",
      "MSFT 0.05772346649090973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     62.806101  0.000000 2013-01-03   93075567  0.000581\n",
      "2     62.806101  0.000000 2013-01-04  110954331  0.019568\n",
      "3     62.806101  0.000000 2013-01-07   66476239 -0.004373\n",
      "4     62.806101  0.000000 2013-01-08   67295297 -0.001975\n",
      "5     62.806101  0.000000 2013-01-09   81291563  0.006552\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  34.556117  1.742417 2018-12-24   31806000 -0.003395\n",
      "1506  49.665784  1.845744 2018-12-26   47466000  0.062769\n",
      "1507  50.525556  1.881370 2018-12-27   42196000  0.004243\n",
      "1508  49.135022  1.827200 2018-12-28   28296000 -0.006535\n",
      "1509  48.822206  1.800650 2018-12-31   29866000 -0.001418\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.28061011e+01  0.00000000e+00  9.30755670e+07  5.80599965e-04]\n",
      " [ 6.28061011e+01  0.00000000e+00  1.10954331e+08  1.95676820e-02]\n",
      " [ 6.28061011e+01  0.00000000e+00  6.64762390e+07 -4.37280770e-03]\n",
      " ...\n",
      " [ 5.41542902e+01  6.88277742e-01  2.54380000e+07 -6.99878116e-03]\n",
      " [ 5.34264521e+01  6.74757943e-01  1.67420000e+07 -1.17275707e-03]\n",
      " [ 5.23543999e+01  6.43703924e-01  1.77500000e+07 -1.66149741e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "GOOG None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.0250WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 165ms/step - loss: 1.0250\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0354WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 165ms/step - loss: 0.0354\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0142WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 169ms/step - loss: 0.0142\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0144WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 164ms/step - loss: 0.0144\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0134WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0134\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0141WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 165ms/step - loss: 0.0141\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0133WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0133\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0136WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 164ms/step - loss: 0.0136\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0134WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0134\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0141WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 174ms/step - loss: 0.0141\n",
      "6/6 [==============================] - 1s 62ms/step\n",
      "[[0.18999696]\n",
      " [0.19041461]\n",
      " [0.18983944]\n",
      " [0.18838246]\n",
      " [0.18981908]\n",
      " [0.18944138]\n",
      " [0.19023462]\n",
      " [0.19110319]\n",
      " [0.19017811]\n",
      " [0.19065641]\n",
      " [0.18707344]\n",
      " [0.18828727]\n",
      " [0.17998445]\n",
      " [0.18300779]\n",
      " [0.18294327]\n",
      " [0.17974721]\n",
      " [0.18222867]\n",
      " [0.18638071]\n",
      " [0.18844908]\n",
      " [0.18987137]\n",
      " [0.19014442]\n",
      " [0.19107337]\n",
      " [0.18878701]\n",
      " [0.19057879]\n",
      " [0.19100623]\n",
      " [0.1877894 ]\n",
      " [0.1888496 ]\n",
      " [0.18801185]\n",
      " [0.1879247 ]\n",
      " [0.18676288]\n",
      " [0.18855196]\n",
      " [0.18264471]\n",
      " [0.18524931]\n",
      " [0.18863887]\n",
      " [0.18896918]\n",
      " [0.19026054]\n",
      " [0.19022828]\n",
      " [0.18534543]\n",
      " [0.17406622]\n",
      " [0.17781608]\n",
      " [0.17145571]\n",
      " [0.17121753]\n",
      " [0.17575225]\n",
      " [0.18185537]\n",
      " [0.18350525]\n",
      " [0.18455507]\n",
      " [0.18521933]\n",
      " [0.18451259]\n",
      " [0.18471646]\n",
      " [0.18542913]\n",
      " [0.18379359]\n",
      " [0.18929671]\n",
      " [0.18717293]\n",
      " [0.1864384 ]\n",
      " [0.18734388]\n",
      " [0.18713331]\n",
      " [0.18595208]\n",
      " [0.18259682]\n",
      " [0.18401487]\n",
      " [0.18405263]\n",
      " [0.1877523 ]\n",
      " [0.18928945]\n",
      " [0.18993953]\n",
      " [0.19136146]\n",
      " [0.19170064]\n",
      " [0.19157784]\n",
      " [0.19036292]\n",
      " [0.19110885]\n",
      " [0.19135648]\n",
      " [0.19052507]\n",
      " [0.19266304]\n",
      " [0.19128586]\n",
      " [0.19113782]\n",
      " [0.19302571]\n",
      " [0.19395213]\n",
      " [0.1913857 ]\n",
      " [0.19215663]\n",
      " [0.19127338]\n",
      " [0.19217412]\n",
      " [0.18851176]\n",
      " [0.18986605]\n",
      " [0.18376209]\n",
      " [0.1637734 ]\n",
      " [0.16217673]\n",
      " [0.15714927]\n",
      " [0.1650776 ]\n",
      " [0.16624954]\n",
      " [0.16655275]\n",
      " [0.17038605]\n",
      " [0.17282325]\n",
      " [0.175801  ]\n",
      " [0.17711127]\n",
      " [0.1772111 ]\n",
      " [0.17890668]\n",
      " [0.17931224]\n",
      " [0.1792864 ]\n",
      " [0.1828806 ]\n",
      " [0.18242934]\n",
      " [0.17412804]\n",
      " [0.17054406]\n",
      " [0.16842772]\n",
      " [0.17323963]\n",
      " [0.17754513]\n",
      " [0.17729193]\n",
      " [0.18032955]\n",
      " [0.18186964]\n",
      " [0.1831398 ]\n",
      " [0.18040311]\n",
      " [0.1773171 ]\n",
      " [0.18188281]\n",
      " [0.17922981]\n",
      " [0.16897525]\n",
      " [0.16688171]\n",
      " [0.17015852]\n",
      " [0.1612595 ]\n",
      " [0.1629524 ]\n",
      " [0.16086948]\n",
      " [0.16100386]\n",
      " [0.16090892]\n",
      " [0.15863532]\n",
      " [0.16305205]\n",
      " [0.16262706]\n",
      " [0.16492422]\n",
      " [0.16636153]\n",
      " [0.16703965]\n",
      " [0.16656305]\n",
      " [0.169357  ]\n",
      " [0.17095749]\n",
      " [0.17071196]\n",
      " [0.17415549]\n",
      " [0.17385828]\n",
      " [0.1766573 ]\n",
      " [0.17928226]\n",
      " [0.17881623]\n",
      " [0.17975903]\n",
      " [0.1736414 ]\n",
      " [0.16673356]\n",
      " [0.16314276]\n",
      " [0.16953716]\n",
      " [0.17377949]\n",
      " [0.17074144]\n",
      " [0.17161904]\n",
      " [0.1762568 ]\n",
      " [0.1715764 ]\n",
      " [0.173649  ]\n",
      " [0.17828584]\n",
      " [0.17873785]\n",
      " [0.17950127]\n",
      " [0.18232478]\n",
      " [0.18395196]\n",
      " [0.18368042]\n",
      " [0.1828898 ]\n",
      " [0.17945735]\n",
      " [0.18240671]\n",
      " [0.18117252]\n",
      " [0.17869233]\n",
      " [0.18307747]\n",
      " [0.18037541]\n",
      " [0.18307891]\n",
      " [0.18293592]\n",
      " [0.18067557]\n",
      " [0.17559077]\n",
      " [0.18019795]\n",
      " [0.1786985 ]\n",
      " [0.18489434]\n",
      " [0.18655445]\n",
      " [0.18591121]\n",
      " [0.18509096]\n",
      " [0.18312815]\n",
      " [0.18440017]\n",
      " [0.18670917]\n",
      " [0.18777487]\n",
      " [0.18503319]\n",
      " [0.1879197 ]\n",
      " [0.18536359]\n",
      " [0.18949063]\n",
      " [0.18653342]\n",
      " [0.18691567]\n",
      " [0.18486224]\n",
      " [0.18367164]\n",
      " [0.17313989]\n",
      " [0.17539161]\n",
      " [0.17511173]\n",
      " [0.1789886 ]\n",
      " [0.17838617]\n",
      " [0.18020143]\n",
      " [0.17767495]\n",
      " [0.18201195]\n",
      " [0.18377931]\n",
      " [0.18522972]\n",
      " [0.18436487]\n",
      " [0.1842071 ]\n",
      " [0.18778677]\n",
      " [0.18678409]\n",
      " [0.18600428]\n",
      " [0.18672818]\n",
      " [0.18570556]\n",
      " [0.1843033 ]\n",
      " [0.1843315 ]\n",
      " [0.1838838 ]\n",
      " [0.18562987]\n",
      " [0.18736795]\n",
      " [0.18582067]\n",
      " [0.17960718]\n",
      " [0.17922442]\n",
      " [0.18076214]\n",
      " [0.18175292]\n",
      " [0.18271388]\n",
      " [0.18322164]\n",
      " [0.18394418]\n",
      " [0.18538563]\n",
      " [0.18545742]\n",
      " [0.18693045]\n",
      " [0.18373042]\n",
      " [0.1842549 ]\n",
      " [0.18368354]\n",
      " [0.17464899]\n",
      " [0.1762384 ]\n",
      " [0.17653729]\n",
      " [0.17960058]\n",
      " [0.17748754]\n",
      " [0.18013665]\n",
      " [0.18000641]\n",
      " [0.18390812]\n",
      " [0.18581848]\n",
      " [0.18229362]\n",
      " [0.18561257]\n",
      " [0.18065952]\n",
      " [0.17257528]\n",
      " [0.16759732]\n",
      " [0.16552438]\n",
      " [0.16643012]\n",
      " [0.16862847]\n",
      " [0.17067362]\n",
      " [0.17299123]\n",
      " [0.17090943]\n",
      " [0.1743788 ]\n",
      " [0.17216854]\n",
      " [0.17058147]\n",
      " [0.17384274]\n",
      " [0.17668095]\n",
      " [0.17676051]\n",
      " [0.1659297 ]\n",
      " [0.17751262]\n",
      " [0.17981157]\n",
      " [0.17761168]\n",
      " [0.18123831]\n",
      " [0.17974687]\n",
      " [0.1803325 ]\n",
      " [0.18048681]\n",
      " [0.1797876 ]]\n",
      "[0.18999696 0.19041461 0.18983944 0.18838246 0.18981908 0.18944138\n",
      " 0.19023462 0.19110319 0.19017811 0.19065641 0.18707344 0.18828727\n",
      " 0.17998445 0.18300779 0.18294327 0.17974721 0.18222867 0.18638071\n",
      " 0.18844908 0.18987137 0.19014442 0.19107337 0.18878701 0.19057879\n",
      " 0.19100623 0.1877894  0.1888496  0.18801185 0.1879247  0.18676288\n",
      " 0.18855196 0.18264471 0.18524931 0.18863887 0.18896918 0.19026054\n",
      " 0.19022828 0.18534543 0.17406622 0.17781608 0.17145571 0.17121753\n",
      " 0.17575225 0.18185537 0.18350525 0.18455507 0.18521933 0.18451259\n",
      " 0.18471646 0.18542913 0.18379359 0.18929671 0.18717293 0.1864384\n",
      " 0.18734388 0.18713331 0.18595208 0.18259682 0.18401487 0.18405263\n",
      " 0.1877523  0.18928945 0.18993953 0.19136146 0.19170064 0.19157784\n",
      " 0.19036292 0.19110885 0.19135648 0.19052507 0.19266304 0.19128586\n",
      " 0.19113782 0.19302571 0.19395213 0.1913857  0.19215663 0.19127338\n",
      " 0.19217412 0.18851176 0.18986605 0.18376209 0.1637734  0.16217673\n",
      " 0.15714927 0.1650776  0.16624954 0.16655275 0.17038605 0.17282325\n",
      " 0.175801   0.17711127 0.1772111  0.17890668 0.17931224 0.1792864\n",
      " 0.1828806  0.18242934 0.17412804 0.17054406 0.16842772 0.17323963\n",
      " 0.17754513 0.17729193 0.18032955 0.18186964 0.1831398  0.18040311\n",
      " 0.1773171  0.18188281 0.17922981 0.16897525 0.16688171 0.17015852\n",
      " 0.1612595  0.1629524  0.16086948 0.16100386 0.16090892 0.15863532\n",
      " 0.16305205 0.16262706 0.16492422 0.16636153 0.16703965 0.16656305\n",
      " 0.169357   0.17095749 0.17071196 0.17415549 0.17385828 0.1766573\n",
      " 0.17928226 0.17881623 0.17975903 0.1736414  0.16673356 0.16314276\n",
      " 0.16953716 0.17377949 0.17074144 0.17161904 0.1762568  0.1715764\n",
      " 0.173649   0.17828584 0.17873785 0.17950127 0.18232478 0.18395196\n",
      " 0.18368042 0.1828898  0.17945735 0.18240671 0.18117252 0.17869233\n",
      " 0.18307747 0.18037541 0.18307891 0.18293592 0.18067557 0.17559077\n",
      " 0.18019795 0.1786985  0.18489434 0.18655445 0.18591121 0.18509096\n",
      " 0.18312815 0.18440017 0.18670917 0.18777487 0.18503319 0.1879197\n",
      " 0.18536359 0.18949063 0.18653342 0.18691567 0.18486224 0.18367164\n",
      " 0.17313989 0.17539161 0.17511173 0.1789886  0.17838617 0.18020143\n",
      " 0.17767495 0.18201195 0.18377931 0.18522972 0.18436487 0.1842071\n",
      " 0.18778677 0.18678409 0.18600428 0.18672818 0.18570556 0.1843033\n",
      " 0.1843315  0.1838838  0.18562987 0.18736795 0.18582067 0.17960718\n",
      " 0.17922442 0.18076214 0.18175292 0.18271388 0.18322164 0.18394418\n",
      " 0.18538563 0.18545742 0.18693045 0.18373042 0.1842549  0.18368354\n",
      " 0.17464899 0.1762384  0.17653729 0.17960058 0.17748754 0.18013665\n",
      " 0.18000641 0.18390812 0.18581848 0.18229362 0.18561257 0.18065952\n",
      " 0.17257528 0.16759732 0.16552438 0.16643012 0.16862847 0.17067362\n",
      " 0.17299123 0.17090943 0.1743788  0.17216854 0.17058147 0.17384274\n",
      " 0.17668095 0.17676051 0.1659297  0.17751262 0.17981157 0.17761168\n",
      " 0.18123831 0.17974687 0.1803325  0.18048681 0.1797876 ]\n",
      "GOOG 0.09200506937998658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     62.806087  0.000000 2013-01-03   92635272  0.000581\n",
      "2     62.806087  0.000000 2013-01-04  110429460  0.019568\n",
      "3     62.806087  0.000000 2013-01-07   66161772 -0.004373\n",
      "4     62.806087  0.000000 2013-01-08   66976956 -0.001975\n",
      "5     62.806087  0.000000 2013-01-09   80907012  0.006552\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  34.590236  1.786591 2018-12-24   36360000 -0.006660\n",
      "1506  49.279060  1.886763 2018-12-26   46318000  0.062189\n",
      "1507  50.240917  1.917494 2018-12-27   45996000  0.004808\n",
      "1508  49.008156  1.859923 2018-12-28   34398000 -0.005925\n",
      "1509  48.652654  1.834036 2018-12-31   33110000 -0.001645\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.28060868e+01  0.00000000e+00  9.26352720e+07  5.80592580e-04]\n",
      " [ 6.28060868e+01  0.00000000e+00  1.10429460e+08  1.95677297e-02]\n",
      " [ 6.28060868e+01  0.00000000e+00  6.61617720e+07 -4.37292695e-03]\n",
      " ...\n",
      " [ 5.37599002e+01  6.99177784e-01  2.23240000e+07 -5.31505640e-03]\n",
      " [ 5.13225752e+01  6.90165218e-01  1.98840000e+07 -4.01669067e-03]\n",
      " [ 4.98618376e+01  6.59974976e-01  2.36060000e+07 -2.41785394e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 272)           301376    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 272)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                16380     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318997 (1.22 MB)\n",
      "Trainable params: 318997 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "GOOGL None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1474WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 96ms/step - loss: 0.1474\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0122WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.0122\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0071WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.0071\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0064WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.0064\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0061WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 97ms/step - loss: 0.0061\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0078WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 100ms/step - loss: 0.0078\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0077WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 97ms/step - loss: 0.0077\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0116WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0116\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0042WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 97ms/step - loss: 0.0042\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0073WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 96ms/step - loss: 0.0073\n",
      "6/6 [==============================] - 0s 28ms/step\n",
      "[[0.23297651]\n",
      " [0.2322548 ]\n",
      " [0.23141433]\n",
      " [0.23097003]\n",
      " [0.23134759]\n",
      " [0.23144644]\n",
      " [0.2317394 ]\n",
      " [0.23418528]\n",
      " [0.23366001]\n",
      " [0.23269325]\n",
      " [0.23133469]\n",
      " [0.2308376 ]\n",
      " [0.2303746 ]\n",
      " [0.2307293 ]\n",
      " [0.23313421]\n",
      " [0.2334083 ]\n",
      " [0.23341033]\n",
      " [0.23213759]\n",
      " [0.23212737]\n",
      " [0.23272353]\n",
      " [0.23317894]\n",
      " [0.23426074]\n",
      " [0.23348019]\n",
      " [0.23268947]\n",
      " [0.23201916]\n",
      " [0.23137236]\n",
      " [0.2311205 ]\n",
      " [0.23201194]\n",
      " [0.23218332]\n",
      " [0.23191208]\n",
      " [0.23262534]\n",
      " [0.23256466]\n",
      " [0.23369169]\n",
      " [0.2340481 ]\n",
      " [0.23473251]\n",
      " [0.23531249]\n",
      " [0.23492381]\n",
      " [0.23447375]\n",
      " [0.23349509]\n",
      " [0.23416618]\n",
      " [0.23363966]\n",
      " [0.23363319]\n",
      " [0.23524821]\n",
      " [0.23578906]\n",
      " [0.2362651 ]\n",
      " [0.23560607]\n",
      " [0.23611788]\n",
      " [0.23631749]\n",
      " [0.23724087]\n",
      " [0.23795104]\n",
      " [0.23782222]\n",
      " [0.23737966]\n",
      " [0.23735493]\n",
      " [0.23787272]\n",
      " [0.23833025]\n",
      " [0.23921958]\n",
      " [0.23833409]\n",
      " [0.23730563]\n",
      " [0.2368823 ]\n",
      " [0.23787591]\n",
      " [0.23896596]\n",
      " [0.24026652]\n",
      " [0.24147712]\n",
      " [0.24160695]\n",
      " [0.24121013]\n",
      " [0.24080643]\n",
      " [0.24097833]\n",
      " [0.24172024]\n",
      " [0.2444484 ]\n",
      " [0.24580076]\n",
      " [0.24674898]\n",
      " [0.24698588]\n",
      " [0.24719658]\n",
      " [0.24662983]\n",
      " [0.24631265]\n",
      " [0.24675532]\n",
      " [0.24546087]\n",
      " [0.24672657]\n",
      " [0.24821371]\n",
      " [0.24865952]\n",
      " [0.24663424]\n",
      " [0.24633318]\n",
      " [0.24331485]\n",
      " [0.24094988]\n",
      " [0.24406448]\n",
      " [0.24358161]\n",
      " [0.24525529]\n",
      " [0.2461333 ]\n",
      " [0.24594823]\n",
      " [0.24554735]\n",
      " [0.24524945]\n",
      " [0.24657676]\n",
      " [0.24943142]\n",
      " [0.24838555]\n",
      " [0.24839553]\n",
      " [0.247201  ]\n",
      " [0.2478401 ]\n",
      " [0.24643768]\n",
      " [0.2457243 ]\n",
      " [0.24695724]\n",
      " [0.24613601]\n",
      " [0.24625129]\n",
      " [0.24703607]\n",
      " [0.24755539]\n",
      " [0.24564311]\n",
      " [0.2452749 ]\n",
      " [0.24631634]\n",
      " [0.2469149 ]\n",
      " [0.2470552 ]\n",
      " [0.24585946]\n",
      " [0.24624726]\n",
      " [0.24588537]\n",
      " [0.2449189 ]\n",
      " [0.2456786 ]\n",
      " [0.2447797 ]\n",
      " [0.24319208]\n",
      " [0.24376154]\n",
      " [0.24440134]\n",
      " [0.24256113]\n",
      " [0.2437855 ]\n",
      " [0.24507132]\n",
      " [0.24324912]\n",
      " [0.24304998]\n",
      " [0.24303369]\n",
      " [0.24102962]\n",
      " [0.24049056]\n",
      " [0.24188149]\n",
      " [0.24295688]\n",
      " [0.24199592]\n",
      " [0.24218804]\n",
      " [0.24169666]\n",
      " [0.24132267]\n",
      " [0.2420328 ]\n",
      " [0.2435163 ]\n",
      " [0.24268943]\n",
      " [0.24226657]\n",
      " [0.24178037]\n",
      " [0.23976366]\n",
      " [0.24065366]\n",
      " [0.24057984]\n",
      " [0.24102661]\n",
      " [0.24019997]\n",
      " [0.24147251]\n",
      " [0.24062875]\n",
      " [0.24099913]\n",
      " [0.24135238]\n",
      " [0.2405574 ]\n",
      " [0.24093747]\n",
      " [0.2414772 ]\n",
      " [0.24197398]\n",
      " [0.24176249]\n",
      " [0.24140587]\n",
      " [0.24026072]\n",
      " [0.24039267]\n",
      " [0.24127387]\n",
      " [0.24156982]\n",
      " [0.24155767]\n",
      " [0.24117075]\n",
      " [0.24149713]\n",
      " [0.24100958]\n",
      " [0.24099064]\n",
      " [0.24021466]\n",
      " [0.241198  ]\n",
      " [0.24212891]\n",
      " [0.2424995 ]\n",
      " [0.24223988]\n",
      " [0.24185923]\n",
      " [0.2423071 ]\n",
      " [0.24224536]\n",
      " [0.24327499]\n",
      " [0.24296701]\n",
      " [0.24277991]\n",
      " [0.24264735]\n",
      " [0.24190374]\n",
      " [0.24117832]\n",
      " [0.24110097]\n",
      " [0.2412402 ]\n",
      " [0.2416741 ]\n",
      " [0.24111292]\n",
      " [0.24071208]\n",
      " [0.23940834]\n",
      " [0.24026763]\n",
      " [0.23970437]\n",
      " [0.24067378]\n",
      " [0.24160165]\n",
      " [0.24121568]\n",
      " [0.24005973]\n",
      " [0.24154264]\n",
      " [0.24174869]\n",
      " [0.24172163]\n",
      " [0.24284421]\n",
      " [0.24255583]\n",
      " [0.24239552]\n",
      " [0.24110845]\n",
      " [0.24079502]\n",
      " [0.24137783]\n",
      " [0.24120706]\n",
      " [0.24032088]\n",
      " [0.24074644]\n",
      " [0.24057141]\n",
      " [0.24023476]\n",
      " [0.24101344]\n",
      " [0.2423421 ]\n",
      " [0.2424323 ]\n",
      " [0.24246256]\n",
      " [0.2431207 ]\n",
      " [0.24180853]\n",
      " [0.23969544]\n",
      " [0.23980081]\n",
      " [0.24157192]\n",
      " [0.24259707]\n",
      " [0.24372771]\n",
      " [0.24202266]\n",
      " [0.24131432]\n",
      " [0.2405068 ]\n",
      " [0.24108401]\n",
      " [0.2409467 ]\n",
      " [0.2444448 ]\n",
      " [0.24599689]\n",
      " [0.24590355]\n",
      " [0.24472332]\n",
      " [0.24560314]\n",
      " [0.24544278]\n",
      " [0.24670815]\n",
      " [0.24793938]\n",
      " [0.24808933]\n",
      " [0.24817112]\n",
      " [0.24653941]\n",
      " [0.24553174]\n",
      " [0.24405435]\n",
      " [0.24519315]\n",
      " [0.2466186 ]\n",
      " [0.24777138]\n",
      " [0.24869797]\n",
      " [0.2491682 ]\n",
      " [0.24807481]\n",
      " [0.24817984]\n",
      " [0.24741438]\n",
      " [0.24761072]\n",
      " [0.2493426 ]\n",
      " [0.2505101 ]\n",
      " [0.25083655]\n",
      " [0.24986956]\n",
      " [0.24955651]\n",
      " [0.24945106]\n",
      " [0.24985835]\n",
      " [0.251579  ]\n",
      " [0.25214338]\n",
      " [0.24984293]\n",
      " [0.24998929]\n",
      " [0.2497948 ]]\n",
      "[0.23297651 0.2322548  0.23141433 0.23097003 0.23134759 0.23144644\n",
      " 0.2317394  0.23418528 0.23366001 0.23269325 0.23133469 0.2308376\n",
      " 0.2303746  0.2307293  0.23313421 0.2334083  0.23341033 0.23213759\n",
      " 0.23212737 0.23272353 0.23317894 0.23426074 0.23348019 0.23268947\n",
      " 0.23201916 0.23137236 0.2311205  0.23201194 0.23218332 0.23191208\n",
      " 0.23262534 0.23256466 0.23369169 0.2340481  0.23473251 0.23531249\n",
      " 0.23492381 0.23447375 0.23349509 0.23416618 0.23363966 0.23363319\n",
      " 0.23524821 0.23578906 0.2362651  0.23560607 0.23611788 0.23631749\n",
      " 0.23724087 0.23795104 0.23782222 0.23737966 0.23735493 0.23787272\n",
      " 0.23833025 0.23921958 0.23833409 0.23730563 0.2368823  0.23787591\n",
      " 0.23896596 0.24026652 0.24147712 0.24160695 0.24121013 0.24080643\n",
      " 0.24097833 0.24172024 0.2444484  0.24580076 0.24674898 0.24698588\n",
      " 0.24719658 0.24662983 0.24631265 0.24675532 0.24546087 0.24672657\n",
      " 0.24821371 0.24865952 0.24663424 0.24633318 0.24331485 0.24094988\n",
      " 0.24406448 0.24358161 0.24525529 0.2461333  0.24594823 0.24554735\n",
      " 0.24524945 0.24657676 0.24943142 0.24838555 0.24839553 0.247201\n",
      " 0.2478401  0.24643768 0.2457243  0.24695724 0.24613601 0.24625129\n",
      " 0.24703607 0.24755539 0.24564311 0.2452749  0.24631634 0.2469149\n",
      " 0.2470552  0.24585946 0.24624726 0.24588537 0.2449189  0.2456786\n",
      " 0.2447797  0.24319208 0.24376154 0.24440134 0.24256113 0.2437855\n",
      " 0.24507132 0.24324912 0.24304998 0.24303369 0.24102962 0.24049056\n",
      " 0.24188149 0.24295688 0.24199592 0.24218804 0.24169666 0.24132267\n",
      " 0.2420328  0.2435163  0.24268943 0.24226657 0.24178037 0.23976366\n",
      " 0.24065366 0.24057984 0.24102661 0.24019997 0.24147251 0.24062875\n",
      " 0.24099913 0.24135238 0.2405574  0.24093747 0.2414772  0.24197398\n",
      " 0.24176249 0.24140587 0.24026072 0.24039267 0.24127387 0.24156982\n",
      " 0.24155767 0.24117075 0.24149713 0.24100958 0.24099064 0.24021466\n",
      " 0.241198   0.24212891 0.2424995  0.24223988 0.24185923 0.2423071\n",
      " 0.24224536 0.24327499 0.24296701 0.24277991 0.24264735 0.24190374\n",
      " 0.24117832 0.24110097 0.2412402  0.2416741  0.24111292 0.24071208\n",
      " 0.23940834 0.24026763 0.23970437 0.24067378 0.24160165 0.24121568\n",
      " 0.24005973 0.24154264 0.24174869 0.24172163 0.24284421 0.24255583\n",
      " 0.24239552 0.24110845 0.24079502 0.24137783 0.24120706 0.24032088\n",
      " 0.24074644 0.24057141 0.24023476 0.24101344 0.2423421  0.2424323\n",
      " 0.24246256 0.2431207  0.24180853 0.23969544 0.23980081 0.24157192\n",
      " 0.24259707 0.24372771 0.24202266 0.24131432 0.2405068  0.24108401\n",
      " 0.2409467  0.2444448  0.24599689 0.24590355 0.24472332 0.24560314\n",
      " 0.24544278 0.24670815 0.24793938 0.24808933 0.24817112 0.24653941\n",
      " 0.24553174 0.24405435 0.24519315 0.2466186  0.24777138 0.24869797\n",
      " 0.2491682  0.24807481 0.24817984 0.24741438 0.24761072 0.2493426\n",
      " 0.2505101  0.25083655 0.24986956 0.24955651 0.24945106 0.24985835\n",
      " 0.251579   0.25214338 0.24984293 0.24998929 0.2497948 ]\n",
      "GOOGL 0.06522808846908855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     71.244403  0.000000 2013-01-03   55018000  0.004537\n",
      "2     71.244403  0.000000 2013-01-04   37484000  0.002589\n",
      "3     71.244403  0.000000 2013-01-07   98200000  0.035295\n",
      "4     71.244403  0.000000 2013-01-08   60214000 -0.007778\n",
      "5     71.244403  0.000000 2013-01-09   45312000 -0.000113\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  29.222786  3.722038 2018-12-24  144400000 -0.024613\n",
      "1506  42.165661  3.917606 2018-12-26  208236000  0.090254\n",
      "1507  41.568493  3.918813 2018-12-27  194440000 -0.006315\n",
      "1508  43.103510  3.869148 2018-12-28  176580000  0.011144\n",
      "1509  45.363596  3.745423 2018-12-31  139090000  0.016074\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 7.12444030e+01  0.00000000e+00  5.50180000e+07  4.53668633e-03]\n",
      " [ 7.12444030e+01  0.00000000e+00  3.74840000e+07  2.58877498e-03]\n",
      " [ 7.12444030e+01  0.00000000e+00  9.82000000e+07  3.52948721e-02]\n",
      " ...\n",
      " [ 6.07273695e+01  8.37492993e-01  3.73440000e+07  4.66292289e-03]\n",
      " [ 6.20195553e+01  8.05672216e-01  3.68340000e+07  3.24277580e-03]\n",
      " [ 5.37685562e+01  8.07052826e-01  5.37680000e+07 -1.41199765e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 512)           1058816   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 512)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                30780     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1090837 (4.16 MB)\n",
      "Trainable params: 1090837 (4.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "AMZN None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.1150WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 7s 221ms/step - loss: 1.1150\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0329WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 222ms/step - loss: 0.0329\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0111WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 221ms/step - loss: 0.0111\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0035WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 220ms/step - loss: 0.0035\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0038WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 224ms/step - loss: 0.0038\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0035WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 228ms/step - loss: 0.0035\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0046WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 225ms/step - loss: 0.0046\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0067WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 224ms/step - loss: 0.0067\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0050WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 223ms/step - loss: 0.0050\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0056WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 221ms/step - loss: 0.0056\n",
      "6/6 [==============================] - 1s 66ms/step\n",
      "[[0.23926085]\n",
      " [0.23902944]\n",
      " [0.23896585]\n",
      " [0.2387518 ]\n",
      " [0.23914094]\n",
      " [0.23891784]\n",
      " [0.23897251]\n",
      " [0.2390586 ]\n",
      " [0.23912852]\n",
      " [0.23874508]\n",
      " [0.2387021 ]\n",
      " [0.23911148]\n",
      " [0.2384469 ]\n",
      " [0.23938629]\n",
      " [0.2389681 ]\n",
      " [0.23829742]\n",
      " [0.24048017]\n",
      " [0.2388648 ]\n",
      " [0.23871686]\n",
      " [0.23880877]\n",
      " [0.23856688]\n",
      " [0.23911741]\n",
      " [0.23898529]\n",
      " [0.23890683]\n",
      " [0.23909855]\n",
      " [0.2387526 ]\n",
      " [0.23883852]\n",
      " [0.23894073]\n",
      " [0.23907758]\n",
      " [0.238558  ]\n",
      " [0.23916264]\n",
      " [0.23876765]\n",
      " [0.23882154]\n",
      " [0.23914254]\n",
      " [0.23912755]\n",
      " [0.23942566]\n",
      " [0.23890917]\n",
      " [0.23900244]\n",
      " [0.23757973]\n",
      " [0.23908141]\n",
      " [0.23852569]\n",
      " [0.23773481]\n",
      " [0.23878849]\n",
      " [0.23897545]\n",
      " [0.23895949]\n",
      " [0.23878293]\n",
      " [0.2389862 ]\n",
      " [0.23877844]\n",
      " [0.23879434]\n",
      " [0.23901044]\n",
      " [0.23873787]\n",
      " [0.23906726]\n",
      " [0.2388422 ]\n",
      " [0.23869003]\n",
      " [0.23888399]\n",
      " [0.23883867]\n",
      " [0.23909432]\n",
      " [0.23905475]\n",
      " [0.23914295]\n",
      " [0.23847197]\n",
      " [0.23924632]\n",
      " [0.23913194]\n",
      " [0.23892976]\n",
      " [0.23917481]\n",
      " [0.23910718]\n",
      " [0.23893996]\n",
      " [0.23891439]\n",
      " [0.23926753]\n",
      " [0.23929006]\n",
      " [0.23876037]\n",
      " [0.23858275]\n",
      " [0.23877347]\n",
      " [0.23874365]\n",
      " [0.23931015]\n",
      " [0.23936322]\n",
      " [0.23859473]\n",
      " [0.23908514]\n",
      " [0.23915763]\n",
      " [0.2390133 ]\n",
      " [0.23908503]\n",
      " [0.2391975 ]\n",
      " [0.23726943]\n",
      " [0.23910075]\n",
      " [0.23724252]\n",
      " [0.23907764]\n",
      " [0.23779117]\n",
      " [0.23657484]\n",
      " [0.23718704]\n",
      " [0.23885946]\n",
      " [0.23855828]\n",
      " [0.23873481]\n",
      " [0.23833255]\n",
      " [0.23796558]\n",
      " [0.23850428]\n",
      " [0.23843811]\n",
      " [0.23829004]\n",
      " [0.23856966]\n",
      " [0.2387452 ]\n",
      " [0.2382204 ]\n",
      " [0.23851658]\n",
      " [0.23794015]\n",
      " [0.2383709 ]\n",
      " [0.23872648]\n",
      " [0.2386431 ]\n",
      " [0.23857497]\n",
      " [0.23861276]\n",
      " [0.23893593]\n",
      " [0.23889005]\n",
      " [0.23830117]\n",
      " [0.23864283]\n",
      " [0.23850235]\n",
      " [0.23842989]\n",
      " [0.23778988]\n",
      " [0.23917393]\n",
      " [0.2385461 ]\n",
      " [0.23779069]\n",
      " [0.23687854]\n",
      " [0.23948751]\n",
      " [0.23706229]\n",
      " [0.23560704]\n",
      " [0.2379531 ]\n",
      " [0.23515284]\n",
      " [0.23803626]\n",
      " [0.23826595]\n",
      " [0.2388886 ]\n",
      " [0.23681648]\n",
      " [0.23812401]\n",
      " [0.23880722]\n",
      " [0.2380189 ]\n",
      " [0.23879234]\n",
      " [0.23787653]\n",
      " [0.23855418]\n",
      " [0.2393309 ]\n",
      " [0.23867966]\n",
      " [0.23876415]\n",
      " [0.23769131]\n",
      " [0.23821813]\n",
      " [0.23669775]\n",
      " [0.23799765]\n",
      " [0.23906597]\n",
      " [0.23875293]\n",
      " [0.23794423]\n",
      " [0.23843203]\n",
      " [0.23795915]\n",
      " [0.2382338 ]\n",
      " [0.238418  ]\n",
      " [0.23862675]\n",
      " [0.23823749]\n",
      " [0.23863693]\n",
      " [0.23849735]\n",
      " [0.23844443]\n",
      " [0.23863558]\n",
      " [0.23789167]\n",
      " [0.23878615]\n",
      " [0.23857413]\n",
      " [0.23849083]\n",
      " [0.23881847]\n",
      " [0.23862354]\n",
      " [0.23894493]\n",
      " [0.23861983]\n",
      " [0.23881547]\n",
      " [0.23863456]\n",
      " [0.23888701]\n",
      " [0.23876306]\n",
      " [0.23885585]\n",
      " [0.23906395]\n",
      " [0.2391295 ]\n",
      " [0.23868246]\n",
      " [0.23865514]\n",
      " [0.23867118]\n",
      " [0.23886146]\n",
      " [0.2389659 ]\n",
      " [0.2388567 ]\n",
      " [0.23912314]\n",
      " [0.23860198]\n",
      " [0.2389246 ]\n",
      " [0.23889652]\n",
      " [0.23906988]\n",
      " [0.23844376]\n",
      " [0.23865989]\n",
      " [0.23715867]\n",
      " [0.23908271]\n",
      " [0.23780867]\n",
      " [0.23914479]\n",
      " [0.23841791]\n",
      " [0.23883085]\n",
      " [0.23833552]\n",
      " [0.2386594 ]\n",
      " [0.23874679]\n",
      " [0.23901805]\n",
      " [0.23867097]\n",
      " [0.2387636 ]\n",
      " [0.2391224 ]\n",
      " [0.23885167]\n",
      " [0.23871309]\n",
      " [0.23886077]\n",
      " [0.2387021 ]\n",
      " [0.23812251]\n",
      " [0.23863786]\n",
      " [0.23833738]\n",
      " [0.23887163]\n",
      " [0.239234  ]\n",
      " [0.23728615]\n",
      " [0.23834294]\n",
      " [0.23756339]\n",
      " [0.23813802]\n",
      " [0.23861529]\n",
      " [0.23890814]\n",
      " [0.23822379]\n",
      " [0.23877804]\n",
      " [0.23866752]\n",
      " [0.23879923]\n",
      " [0.23863344]\n",
      " [0.23835678]\n",
      " [0.23852612]\n",
      " [0.23896219]\n",
      " [0.23767874]\n",
      " [0.23854637]\n",
      " [0.23840308]\n",
      " [0.23845655]\n",
      " [0.23860283]\n",
      " [0.23886345]\n",
      " [0.2385159 ]\n",
      " [0.23863052]\n",
      " [0.23888013]\n",
      " [0.23869973]\n",
      " [0.23930424]\n",
      " [0.2385526 ]\n",
      " [0.23878576]\n",
      " [0.23908298]\n",
      " [0.23789045]\n",
      " [0.23772325]\n",
      " [0.23832265]\n",
      " [0.2381226 ]\n",
      " [0.23897584]\n",
      " [0.23841701]\n",
      " [0.23848775]\n",
      " [0.238323  ]\n",
      " [0.23691021]\n",
      " [0.23881993]\n",
      " [0.2380579 ]\n",
      " [0.23878175]\n",
      " [0.23755364]\n",
      " [0.23850912]\n",
      " [0.23883551]\n",
      " [0.23825873]\n",
      " [0.2388441 ]\n",
      " [0.23820728]\n",
      " [0.23850869]\n",
      " [0.2378021 ]\n",
      " [0.23807071]]\n",
      "[0.23926085 0.23902944 0.23896585 0.2387518  0.23914094 0.23891784\n",
      " 0.23897251 0.2390586  0.23912852 0.23874508 0.2387021  0.23911148\n",
      " 0.2384469  0.23938629 0.2389681  0.23829742 0.24048017 0.2388648\n",
      " 0.23871686 0.23880877 0.23856688 0.23911741 0.23898529 0.23890683\n",
      " 0.23909855 0.2387526  0.23883852 0.23894073 0.23907758 0.238558\n",
      " 0.23916264 0.23876765 0.23882154 0.23914254 0.23912755 0.23942566\n",
      " 0.23890917 0.23900244 0.23757973 0.23908141 0.23852569 0.23773481\n",
      " 0.23878849 0.23897545 0.23895949 0.23878293 0.2389862  0.23877844\n",
      " 0.23879434 0.23901044 0.23873787 0.23906726 0.2388422  0.23869003\n",
      " 0.23888399 0.23883867 0.23909432 0.23905475 0.23914295 0.23847197\n",
      " 0.23924632 0.23913194 0.23892976 0.23917481 0.23910718 0.23893996\n",
      " 0.23891439 0.23926753 0.23929006 0.23876037 0.23858275 0.23877347\n",
      " 0.23874365 0.23931015 0.23936322 0.23859473 0.23908514 0.23915763\n",
      " 0.2390133  0.23908503 0.2391975  0.23726943 0.23910075 0.23724252\n",
      " 0.23907764 0.23779117 0.23657484 0.23718704 0.23885946 0.23855828\n",
      " 0.23873481 0.23833255 0.23796558 0.23850428 0.23843811 0.23829004\n",
      " 0.23856966 0.2387452  0.2382204  0.23851658 0.23794015 0.2383709\n",
      " 0.23872648 0.2386431  0.23857497 0.23861276 0.23893593 0.23889005\n",
      " 0.23830117 0.23864283 0.23850235 0.23842989 0.23778988 0.23917393\n",
      " 0.2385461  0.23779069 0.23687854 0.23948751 0.23706229 0.23560704\n",
      " 0.2379531  0.23515284 0.23803626 0.23826595 0.2388886  0.23681648\n",
      " 0.23812401 0.23880722 0.2380189  0.23879234 0.23787653 0.23855418\n",
      " 0.2393309  0.23867966 0.23876415 0.23769131 0.23821813 0.23669775\n",
      " 0.23799765 0.23906597 0.23875293 0.23794423 0.23843203 0.23795915\n",
      " 0.2382338  0.238418   0.23862675 0.23823749 0.23863693 0.23849735\n",
      " 0.23844443 0.23863558 0.23789167 0.23878615 0.23857413 0.23849083\n",
      " 0.23881847 0.23862354 0.23894493 0.23861983 0.23881547 0.23863456\n",
      " 0.23888701 0.23876306 0.23885585 0.23906395 0.2391295  0.23868246\n",
      " 0.23865514 0.23867118 0.23886146 0.2389659  0.2388567  0.23912314\n",
      " 0.23860198 0.2389246  0.23889652 0.23906988 0.23844376 0.23865989\n",
      " 0.23715867 0.23908271 0.23780867 0.23914479 0.23841791 0.23883085\n",
      " 0.23833552 0.2386594  0.23874679 0.23901805 0.23867097 0.2387636\n",
      " 0.2391224  0.23885167 0.23871309 0.23886077 0.2387021  0.23812251\n",
      " 0.23863786 0.23833738 0.23887163 0.239234   0.23728615 0.23834294\n",
      " 0.23756339 0.23813802 0.23861529 0.23890814 0.23822379 0.23877804\n",
      " 0.23866752 0.23879923 0.23863344 0.23835678 0.23852612 0.23896219\n",
      " 0.23767874 0.23854637 0.23840308 0.23845655 0.23860283 0.23886345\n",
      " 0.2385159  0.23863052 0.23888013 0.23869973 0.23930424 0.2385526\n",
      " 0.23878576 0.23908298 0.23789045 0.23772325 0.23832265 0.2381226\n",
      " 0.23897584 0.23841701 0.23848775 0.238323   0.23691021 0.23881993\n",
      " 0.2380579  0.23878175 0.23755364 0.23850912 0.23883551 0.23825873\n",
      " 0.2388441  0.23820728 0.23850869 0.2378021  0.23807071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMZN 0.07669444407615947\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     37.558534  0.000000 2013-01-03  29888800  0.000786\n",
      "2     37.558534  0.000000 2013-01-04  52496800  0.032460\n",
      "3     37.558534  0.000000 2013-01-07  61073200 -0.029323\n",
      "4     37.558534  0.000000 2013-01-08  46642400 -0.022170\n",
      "5     37.558534  0.000000 2013-01-09  69502000 -0.022673\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  28.272483  2.321551 2018-12-24  46384000 -0.019404\n",
      "1506  34.058136  2.310726 2018-12-26  69510000  0.046284\n",
      "1507  33.135340  2.274245 2018-12-27  63704400 -0.014607\n",
      "1508  35.551728  2.238228 2018-12-28  62872800  0.018730\n",
      "1509  35.468262  2.157819 2018-12-31  46514000 -0.001123\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 3.75585343e+01  0.00000000e+00  2.98888000e+07  7.85856890e-04]\n",
      " [ 3.75585343e+01  0.00000000e+00  5.24968000e+07  3.24602797e-02]\n",
      " [ 3.75585343e+01  0.00000000e+00  6.10732000e+07 -2.93229324e-02]\n",
      " ...\n",
      " [ 5.02752829e+01  1.31175102e+00  3.29488000e+07 -1.36828670e-03]\n",
      " [ 5.06034940e+01  1.25787615e+00  2.40248000e+07  1.16575641e-03]\n",
      " [ 4.51609199e+01  1.25659940e+00  2.79964000e+07 -1.99546418e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 272)           301376    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 272)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                16380     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318997 (1.22 MB)\n",
      "Trainable params: 318997 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "NVDA None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2613WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 79ms/step - loss: 0.2613\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0420WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.0420\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0820WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 0.0820\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0329WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 78ms/step - loss: 0.0329\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0509WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.0509\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0490WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 78ms/step - loss: 0.0490\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0331WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.0331\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0611WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 0.0611\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0669WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 0.0669\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0415WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.0415\n",
      "6/6 [==============================] - 0s 24ms/step\n",
      "[[0.17375445]\n",
      " [0.17371926]\n",
      " [0.17372757]\n",
      " [0.17373703]\n",
      " [0.17365405]\n",
      " [0.17364186]\n",
      " [0.17360002]\n",
      " [0.17356212]\n",
      " [0.17355183]\n",
      " [0.17357309]\n",
      " [0.17356601]\n",
      " [0.17353806]\n",
      " [0.17355768]\n",
      " [0.17359081]\n",
      " [0.17375663]\n",
      " [0.1736464 ]\n",
      " [0.17361595]\n",
      " [0.17357786]\n",
      " [0.17354402]\n",
      " [0.17355801]\n",
      " [0.17350534]\n",
      " [0.17347196]\n",
      " [0.17350306]\n",
      " [0.17356472]\n",
      " [0.17370132]\n",
      " [0.173902  ]\n",
      " [0.17393976]\n",
      " [0.17400344]\n",
      " [0.17407687]\n",
      " [0.17414045]\n",
      " [0.17413655]\n",
      " [0.17416915]\n",
      " [0.17417039]\n",
      " [0.17418477]\n",
      " [0.17424175]\n",
      " [0.1742641 ]\n",
      " [0.17437503]\n",
      " [0.17458142]\n",
      " [0.17479138]\n",
      " [0.17476141]\n",
      " [0.1748511 ]\n",
      " [0.17494844]\n",
      " [0.17495179]\n",
      " [0.17492932]\n",
      " [0.17494461]\n",
      " [0.17493394]\n",
      " [0.17492175]\n",
      " [0.17501882]\n",
      " [0.17511159]\n",
      " [0.17511526]\n",
      " [0.17506589]\n",
      " [0.175022  ]\n",
      " [0.17505915]\n",
      " [0.17506804]\n",
      " [0.17514503]\n",
      " [0.17520256]\n",
      " [0.17521548]\n",
      " [0.17525849]\n",
      " [0.17537108]\n",
      " [0.17544755]\n",
      " [0.17541414]\n",
      " [0.17544022]\n",
      " [0.17542017]\n",
      " [0.17539482]\n",
      " [0.17538017]\n",
      " [0.17533362]\n",
      " [0.17531186]\n",
      " [0.17529216]\n",
      " [0.17533442]\n",
      " [0.17539358]\n",
      " [0.1753906 ]\n",
      " [0.17537192]\n",
      " [0.17535423]\n",
      " [0.17532027]\n",
      " [0.17531566]\n",
      " [0.17533681]\n",
      " [0.175292  ]\n",
      " [0.17527658]\n",
      " [0.1752902 ]\n",
      " [0.17533517]\n",
      " [0.1753552 ]\n",
      " [0.17538762]\n",
      " [0.17548004]\n",
      " [0.17557377]\n",
      " [0.1756897 ]\n",
      " [0.17565583]\n",
      " [0.17573616]\n",
      " [0.17578015]\n",
      " [0.17567754]\n",
      " [0.17565891]\n",
      " [0.17566015]\n",
      " [0.17561743]\n",
      " [0.17558916]\n",
      " [0.17558186]\n",
      " [0.17555547]\n",
      " [0.17552675]\n",
      " [0.17550424]\n",
      " [0.17547926]\n",
      " [0.1754607 ]\n",
      " [0.1754882 ]\n",
      " [0.1755478 ]\n",
      " [0.17554834]\n",
      " [0.17549625]\n",
      " [0.17548102]\n",
      " [0.17545056]\n",
      " [0.17541787]\n",
      " [0.17539251]\n",
      " [0.17538099]\n",
      " [0.17538157]\n",
      " [0.17536438]\n",
      " [0.17535073]\n",
      " [0.17540064]\n",
      " [0.17543018]\n",
      " [0.17542951]\n",
      " [0.1754644 ]\n",
      " [0.17549385]\n",
      " [0.17550483]\n",
      " [0.17560728]\n",
      " [0.1755861 ]\n",
      " [0.175587  ]\n",
      " [0.175582  ]\n",
      " [0.17550603]\n",
      " [0.17550072]\n",
      " [0.17546636]\n",
      " [0.17544203]\n",
      " [0.17540933]\n",
      " [0.17540729]\n",
      " [0.17536715]\n",
      " [0.17528981]\n",
      " [0.17527649]\n",
      " [0.17522138]\n",
      " [0.1751914 ]\n",
      " [0.1751695 ]\n",
      " [0.17515239]\n",
      " [0.175141  ]\n",
      " [0.1751194 ]\n",
      " [0.17511952]\n",
      " [0.17513166]\n",
      " [0.17512852]\n",
      " [0.17510545]\n",
      " [0.17508174]\n",
      " [0.17506662]\n",
      " [0.17506203]\n",
      " [0.17503947]\n",
      " [0.17499289]\n",
      " [0.17492011]\n",
      " [0.17489216]\n",
      " [0.17482242]\n",
      " [0.1747772 ]\n",
      " [0.17476034]\n",
      " [0.17474005]\n",
      " [0.17471285]\n",
      " [0.17475405]\n",
      " [0.17470911]\n",
      " [0.17467558]\n",
      " [0.17467186]\n",
      " [0.17468919]\n",
      " [0.17465267]\n",
      " [0.17461014]\n",
      " [0.17455974]\n",
      " [0.1745373 ]\n",
      " [0.17453775]\n",
      " [0.17453986]\n",
      " [0.17453697]\n",
      " [0.1744844 ]\n",
      " [0.17445123]\n",
      " [0.17440501]\n",
      " [0.17440113]\n",
      " [0.17441757]\n",
      " [0.17443031]\n",
      " [0.17442521]\n",
      " [0.17439793]\n",
      " [0.17435779]\n",
      " [0.17434056]\n",
      " [0.17434996]\n",
      " [0.17436174]\n",
      " [0.17441525]\n",
      " [0.17438546]\n",
      " [0.17448853]\n",
      " [0.1746217 ]\n",
      " [0.17471875]\n",
      " [0.17474236]\n",
      " [0.1747747 ]\n",
      " [0.17475824]\n",
      " [0.174771  ]\n",
      " [0.174777  ]\n",
      " [0.17477743]\n",
      " [0.1747444 ]\n",
      " [0.17473045]\n",
      " [0.17471397]\n",
      " [0.1747195 ]\n",
      " [0.17470764]\n",
      " [0.17461771]\n",
      " [0.17463346]\n",
      " [0.1746745 ]\n",
      " [0.17467548]\n",
      " [0.17465222]\n",
      " [0.17463623]\n",
      " [0.174665  ]\n",
      " [0.17469902]\n",
      " [0.17472184]\n",
      " [0.17467201]\n",
      " [0.17464335]\n",
      " [0.1747183 ]\n",
      " [0.17481288]\n",
      " [0.17478478]\n",
      " [0.17472555]\n",
      " [0.17469087]\n",
      " [0.1746436 ]\n",
      " [0.17457655]\n",
      " [0.17452589]\n",
      " [0.17451775]\n",
      " [0.17456287]\n",
      " [0.17459863]\n",
      " [0.17459534]\n",
      " [0.17460239]\n",
      " [0.17465073]\n",
      " [0.17474568]\n",
      " [0.17486036]\n",
      " [0.17483415]\n",
      " [0.17477322]\n",
      " [0.17472418]\n",
      " [0.17467028]\n",
      " [0.17455178]\n",
      " [0.17447245]\n",
      " [0.17440058]\n",
      " [0.17432426]\n",
      " [0.17424628]\n",
      " [0.17415416]\n",
      " [0.17418969]\n",
      " [0.17428848]\n",
      " [0.1743891 ]\n",
      " [0.17446199]\n",
      " [0.1745105 ]\n",
      " [0.17459151]\n",
      " [0.1746912 ]\n",
      " [0.17469603]\n",
      " [0.17471883]\n",
      " [0.17475131]\n",
      " [0.17483695]\n",
      " [0.17491399]\n",
      " [0.17504801]\n",
      " [0.17514169]\n",
      " [0.17518163]\n",
      " [0.17521895]\n",
      " [0.17527469]\n",
      " [0.17527504]\n",
      " [0.1753214 ]\n",
      " [0.17528553]\n",
      " [0.17522216]\n",
      " [0.17521477]]\n",
      "[0.17375445 0.17371926 0.17372757 0.17373703 0.17365405 0.17364186\n",
      " 0.17360002 0.17356212 0.17355183 0.17357309 0.17356601 0.17353806\n",
      " 0.17355768 0.17359081 0.17375663 0.1736464  0.17361595 0.17357786\n",
      " 0.17354402 0.17355801 0.17350534 0.17347196 0.17350306 0.17356472\n",
      " 0.17370132 0.173902   0.17393976 0.17400344 0.17407687 0.17414045\n",
      " 0.17413655 0.17416915 0.17417039 0.17418477 0.17424175 0.1742641\n",
      " 0.17437503 0.17458142 0.17479138 0.17476141 0.1748511  0.17494844\n",
      " 0.17495179 0.17492932 0.17494461 0.17493394 0.17492175 0.17501882\n",
      " 0.17511159 0.17511526 0.17506589 0.175022   0.17505915 0.17506804\n",
      " 0.17514503 0.17520256 0.17521548 0.17525849 0.17537108 0.17544755\n",
      " 0.17541414 0.17544022 0.17542017 0.17539482 0.17538017 0.17533362\n",
      " 0.17531186 0.17529216 0.17533442 0.17539358 0.1753906  0.17537192\n",
      " 0.17535423 0.17532027 0.17531566 0.17533681 0.175292   0.17527658\n",
      " 0.1752902  0.17533517 0.1753552  0.17538762 0.17548004 0.17557377\n",
      " 0.1756897  0.17565583 0.17573616 0.17578015 0.17567754 0.17565891\n",
      " 0.17566015 0.17561743 0.17558916 0.17558186 0.17555547 0.17552675\n",
      " 0.17550424 0.17547926 0.1754607  0.1754882  0.1755478  0.17554834\n",
      " 0.17549625 0.17548102 0.17545056 0.17541787 0.17539251 0.17538099\n",
      " 0.17538157 0.17536438 0.17535073 0.17540064 0.17543018 0.17542951\n",
      " 0.1754644  0.17549385 0.17550483 0.17560728 0.1755861  0.175587\n",
      " 0.175582   0.17550603 0.17550072 0.17546636 0.17544203 0.17540933\n",
      " 0.17540729 0.17536715 0.17528981 0.17527649 0.17522138 0.1751914\n",
      " 0.1751695  0.17515239 0.175141   0.1751194  0.17511952 0.17513166\n",
      " 0.17512852 0.17510545 0.17508174 0.17506662 0.17506203 0.17503947\n",
      " 0.17499289 0.17492011 0.17489216 0.17482242 0.1747772  0.17476034\n",
      " 0.17474005 0.17471285 0.17475405 0.17470911 0.17467558 0.17467186\n",
      " 0.17468919 0.17465267 0.17461014 0.17455974 0.1745373  0.17453775\n",
      " 0.17453986 0.17453697 0.1744844  0.17445123 0.17440501 0.17440113\n",
      " 0.17441757 0.17443031 0.17442521 0.17439793 0.17435779 0.17434056\n",
      " 0.17434996 0.17436174 0.17441525 0.17438546 0.17448853 0.1746217\n",
      " 0.17471875 0.17474236 0.1747747  0.17475824 0.174771   0.174777\n",
      " 0.17477743 0.1747444  0.17473045 0.17471397 0.1747195  0.17470764\n",
      " 0.17461771 0.17463346 0.1746745  0.17467548 0.17465222 0.17463623\n",
      " 0.174665   0.17469902 0.17472184 0.17467201 0.17464335 0.1747183\n",
      " 0.17481288 0.17478478 0.17472555 0.17469087 0.1746436  0.17457655\n",
      " 0.17452589 0.17451775 0.17456287 0.17459863 0.17459534 0.17460239\n",
      " 0.17465073 0.17474568 0.17486036 0.17483415 0.17477322 0.17472418\n",
      " 0.17467028 0.17455178 0.17447245 0.17440058 0.17432426 0.17424628\n",
      " 0.17415416 0.17418969 0.17428848 0.1743891  0.17446199 0.1745105\n",
      " 0.17459151 0.1746912  0.17469603 0.17471883 0.17475131 0.17483695\n",
      " 0.17491399 0.17504801 0.17514169 0.17518163 0.17521895 0.17527469\n",
      " 0.17527504 0.1753214  0.17528553 0.17522216 0.17521477]\n",
      "NVDA 0.5881017258664275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     62.480921  0.000000 2013-01-03   11130000 -0.016826\n",
      "2     62.480921  0.000000 2013-01-04   10110000 -0.010699\n",
      "3     62.480921  0.000000 2013-01-07    6630000 -0.001746\n",
      "4     62.480921  0.000000 2013-01-08   19260000 -0.019407\n",
      "5     62.480921  0.000000 2013-01-09   10470000 -0.001188\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  31.334964  0.996541 2018-12-24   83398500 -0.079305\n",
      "1506  45.440901  1.081931 2018-12-26  122446500  0.098877\n",
      "1507  42.397807  1.103078 2018-12-27  128626500 -0.031020\n",
      "1508  48.954772  1.120049 2018-12-28  149085000  0.054598\n",
      "1509  48.595460  1.106474 2018-12-31   94534500 -0.003210\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.24809206e+01  0.00000000e+00  1.11300000e+07 -1.68260932e-02]\n",
      " [ 6.24809206e+01  0.00000000e+00  1.01100000e+07 -1.06985374e-02]\n",
      " [ 6.24809206e+01  0.00000000e+00  6.63000000e+06 -1.74568663e-03]\n",
      " ...\n",
      " [ 4.12403679e+01  5.79377609e-01  7.06815000e+07 -1.79675243e-02]\n",
      " [ 4.43219762e+01  5.67898268e-01  6.47445000e+07  1.18662207e-02]\n",
      " [ 4.17784515e+01  5.57857815e-01  5.66580000e+07 -1.27971933e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 32)            4736      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                1980      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7957 (31.08 KB)\n",
      "Trainable params: 7957 (31.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "TSLA None\n",
      "Epoch 1/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.1710WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 8ms/step - loss: 0.1596\n",
      "Epoch 2/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0133WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0139\n",
      "Epoch 3/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0104WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0107\n",
      "Epoch 4/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0098WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0102\n",
      "Epoch 5/10\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0093WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0113\n",
      "Epoch 6/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0083WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0080\n",
      "Epoch 7/10\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0081WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0085\n",
      "Epoch 8/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0077WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0076\n",
      "Epoch 9/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0067WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0065\n",
      "Epoch 10/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0063WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0061\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "[[0.3966648 ]\n",
      " [0.39552167]\n",
      " [0.3944986 ]\n",
      " [0.3943044 ]\n",
      " [0.39357874]\n",
      " [0.39339814]\n",
      " [0.39358592]\n",
      " [0.39394343]\n",
      " [0.39447695]\n",
      " [0.39477405]\n",
      " [0.3947428 ]\n",
      " [0.39502242]\n",
      " [0.39546633]\n",
      " [0.3960293 ]\n",
      " [0.3964969 ]\n",
      " [0.39750805]\n",
      " [0.39816713]\n",
      " [0.39890146]\n",
      " [0.39930582]\n",
      " [0.39888906]\n",
      " [0.39975873]\n",
      " [0.40159976]\n",
      " [0.40248275]\n",
      " [0.4035095 ]\n",
      " [0.40424088]\n",
      " [0.40510184]\n",
      " [0.40607297]\n",
      " [0.40723693]\n",
      " [0.40769488]\n",
      " [0.40878052]\n",
      " [0.40962428]\n",
      " [0.4105836 ]\n",
      " [0.41168356]\n",
      " [0.41313016]\n",
      " [0.41418156]\n",
      " [0.41549143]\n",
      " [0.41662717]\n",
      " [0.41773313]\n",
      " [0.4189388 ]\n",
      " [0.42046475]\n",
      " [0.4218297 ]\n",
      " [0.4233608 ]\n",
      " [0.42497125]\n",
      " [0.42659843]\n",
      " [0.4275314 ]\n",
      " [0.42865545]\n",
      " [0.42959046]\n",
      " [0.42975646]\n",
      " [0.4298446 ]\n",
      " [0.43045342]\n",
      " [0.43104434]\n",
      " [0.4314734 ]\n",
      " [0.43216085]\n",
      " [0.4330137 ]\n",
      " [0.43388855]\n",
      " [0.43464652]\n",
      " [0.43563   ]\n",
      " [0.43686652]\n",
      " [0.43830386]\n",
      " [0.4393931 ]\n",
      " [0.4405241 ]\n",
      " [0.44130337]\n",
      " [0.44238114]\n",
      " [0.44376907]\n",
      " [0.44541958]\n",
      " [0.44637603]\n",
      " [0.4477905 ]\n",
      " [0.4492051 ]\n",
      " [0.45049793]\n",
      " [0.45201075]\n",
      " [0.45324254]\n",
      " [0.4542991 ]\n",
      " [0.45553815]\n",
      " [0.45649514]\n",
      " [0.45729515]\n",
      " [0.45794812]\n",
      " [0.45894423]\n",
      " [0.46023616]\n",
      " [0.46116257]\n",
      " [0.46174026]\n",
      " [0.4624554 ]\n",
      " [0.46249253]\n",
      " [0.4625397 ]\n",
      " [0.46266988]\n",
      " [0.4628134 ]\n",
      " [0.46207833]\n",
      " [0.46118522]\n",
      " [0.46264458]\n",
      " [0.46342903]\n",
      " [0.4637585 ]\n",
      " [0.4638255 ]\n",
      " [0.46417773]\n",
      " [0.46390313]\n",
      " [0.46382707]\n",
      " [0.463911  ]\n",
      " [0.46404958]\n",
      " [0.46347356]\n",
      " [0.46275276]\n",
      " [0.46161726]\n",
      " [0.46079272]\n",
      " [0.4603016 ]\n",
      " [0.46004993]\n",
      " [0.4593096 ]\n",
      " [0.45891136]\n",
      " [0.45876035]\n",
      " [0.45819855]\n",
      " [0.45787942]\n",
      " [0.457453  ]\n",
      " [0.45596558]\n",
      " [0.45536202]\n",
      " [0.45559987]\n",
      " [0.45545644]\n",
      " [0.45553106]\n",
      " [0.455871  ]\n",
      " [0.45521843]\n",
      " [0.4517974 ]\n",
      " [0.4485304 ]\n",
      " [0.44716918]\n",
      " [0.44691598]\n",
      " [0.449647  ]\n",
      " [0.45292887]\n",
      " [0.4545021 ]\n",
      " [0.45725513]\n",
      " [0.45860648]\n",
      " [0.4592792 ]\n",
      " [0.45925125]\n",
      " [0.45956665]\n",
      " [0.46023715]\n",
      " [0.45985115]\n",
      " [0.46012264]\n",
      " [0.4606451 ]\n",
      " [0.4605721 ]\n",
      " [0.46118778]\n",
      " [0.4616702 ]\n",
      " [0.4616324 ]\n",
      " [0.46124113]\n",
      " [0.4617561 ]\n",
      " [0.46250284]\n",
      " [0.4631452 ]\n",
      " [0.46396908]\n",
      " [0.46432126]\n",
      " [0.46421865]\n",
      " [0.4641605 ]\n",
      " [0.4636359 ]\n",
      " [0.46365115]\n",
      " [0.46492457]\n",
      " [0.4651081 ]\n",
      " [0.46503216]\n",
      " [0.46548057]\n",
      " [0.46589994]\n",
      " [0.46628433]\n",
      " [0.466741  ]\n",
      " [0.46806112]\n",
      " [0.46971613]\n",
      " [0.47088438]\n",
      " [0.47217643]\n",
      " [0.47404605]\n",
      " [0.4752972 ]\n",
      " [0.47723505]\n",
      " [0.4783948 ]\n",
      " [0.4799465 ]\n",
      " [0.48165467]\n",
      " [0.4831472 ]\n",
      " [0.48417944]\n",
      " [0.48565978]\n",
      " [0.4862188 ]\n",
      " [0.48670146]\n",
      " [0.48774448]\n",
      " [0.4874127 ]\n",
      " [0.4875142 ]\n",
      " [0.48740053]\n",
      " [0.48699024]\n",
      " [0.48651773]\n",
      " [0.48575413]\n",
      " [0.48509002]\n",
      " [0.48443076]\n",
      " [0.48311877]\n",
      " [0.4827726 ]\n",
      " [0.48137644]\n",
      " [0.48103803]\n",
      " [0.48097765]\n",
      " [0.48091698]\n",
      " [0.47998106]\n",
      " [0.4782215 ]\n",
      " [0.47563165]\n",
      " [0.47451723]\n",
      " [0.47490084]\n",
      " [0.47611585]\n",
      " [0.47609293]\n",
      " [0.47679937]\n",
      " [0.47713268]\n",
      " [0.47735325]\n",
      " [0.4776969 ]\n",
      " [0.47787088]\n",
      " [0.47809607]\n",
      " [0.47896543]\n",
      " [0.47866255]\n",
      " [0.47834   ]\n",
      " [0.47819752]\n",
      " [0.47854844]\n",
      " [0.4796009 ]\n",
      " [0.4806101 ]\n",
      " [0.48046982]\n",
      " [0.4805581 ]\n",
      " [0.4812402 ]\n",
      " [0.48214704]\n",
      " [0.48208606]\n",
      " [0.48252195]\n",
      " [0.48127735]\n",
      " [0.48043436]\n",
      " [0.48078194]\n",
      " [0.48206294]\n",
      " [0.4831041 ]\n",
      " [0.48378068]\n",
      " [0.4831317 ]\n",
      " [0.48191166]\n",
      " [0.48105782]\n",
      " [0.48105937]\n",
      " [0.48165604]\n",
      " [0.48386592]\n",
      " [0.48500645]\n",
      " [0.4851429 ]\n",
      " [0.48528963]\n",
      " [0.48534727]\n",
      " [0.48521188]\n",
      " [0.48546636]\n",
      " [0.48565167]\n",
      " [0.48558038]\n",
      " [0.48510176]\n",
      " [0.4846064 ]\n",
      " [0.4854741 ]\n",
      " [0.4867807 ]\n",
      " [0.48826408]\n",
      " [0.49083275]\n",
      " [0.49109825]\n",
      " [0.49215603]\n",
      " [0.49219936]\n",
      " [0.49261367]\n",
      " [0.49271166]\n",
      " [0.49334395]\n",
      " [0.4944635 ]\n",
      " [0.49437922]\n",
      " [0.49452418]\n",
      " [0.4942137 ]\n",
      " [0.493001  ]\n",
      " [0.49082363]\n",
      " [0.4883561 ]\n",
      " [0.4873555 ]\n",
      " [0.49119484]\n",
      " [0.48919344]\n",
      " [0.48843926]]\n",
      "[0.3966648  0.39552167 0.3944986  0.3943044  0.39357874 0.39339814\n",
      " 0.39358592 0.39394343 0.39447695 0.39477405 0.3947428  0.39502242\n",
      " 0.39546633 0.3960293  0.3964969  0.39750805 0.39816713 0.39890146\n",
      " 0.39930582 0.39888906 0.39975873 0.40159976 0.40248275 0.4035095\n",
      " 0.40424088 0.40510184 0.40607297 0.40723693 0.40769488 0.40878052\n",
      " 0.40962428 0.4105836  0.41168356 0.41313016 0.41418156 0.41549143\n",
      " 0.41662717 0.41773313 0.4189388  0.42046475 0.4218297  0.4233608\n",
      " 0.42497125 0.42659843 0.4275314  0.42865545 0.42959046 0.42975646\n",
      " 0.4298446  0.43045342 0.43104434 0.4314734  0.43216085 0.4330137\n",
      " 0.43388855 0.43464652 0.43563    0.43686652 0.43830386 0.4393931\n",
      " 0.4405241  0.44130337 0.44238114 0.44376907 0.44541958 0.44637603\n",
      " 0.4477905  0.4492051  0.45049793 0.45201075 0.45324254 0.4542991\n",
      " 0.45553815 0.45649514 0.45729515 0.45794812 0.45894423 0.46023616\n",
      " 0.46116257 0.46174026 0.4624554  0.46249253 0.4625397  0.46266988\n",
      " 0.4628134  0.46207833 0.46118522 0.46264458 0.46342903 0.4637585\n",
      " 0.4638255  0.46417773 0.46390313 0.46382707 0.463911   0.46404958\n",
      " 0.46347356 0.46275276 0.46161726 0.46079272 0.4603016  0.46004993\n",
      " 0.4593096  0.45891136 0.45876035 0.45819855 0.45787942 0.457453\n",
      " 0.45596558 0.45536202 0.45559987 0.45545644 0.45553106 0.455871\n",
      " 0.45521843 0.4517974  0.4485304  0.44716918 0.44691598 0.449647\n",
      " 0.45292887 0.4545021  0.45725513 0.45860648 0.4592792  0.45925125\n",
      " 0.45956665 0.46023715 0.45985115 0.46012264 0.4606451  0.4605721\n",
      " 0.46118778 0.4616702  0.4616324  0.46124113 0.4617561  0.46250284\n",
      " 0.4631452  0.46396908 0.46432126 0.46421865 0.4641605  0.4636359\n",
      " 0.46365115 0.46492457 0.4651081  0.46503216 0.46548057 0.46589994\n",
      " 0.46628433 0.466741   0.46806112 0.46971613 0.47088438 0.47217643\n",
      " 0.47404605 0.4752972  0.47723505 0.4783948  0.4799465  0.48165467\n",
      " 0.4831472  0.48417944 0.48565978 0.4862188  0.48670146 0.48774448\n",
      " 0.4874127  0.4875142  0.48740053 0.48699024 0.48651773 0.48575413\n",
      " 0.48509002 0.48443076 0.48311877 0.4827726  0.48137644 0.48103803\n",
      " 0.48097765 0.48091698 0.47998106 0.4782215  0.47563165 0.47451723\n",
      " 0.47490084 0.47611585 0.47609293 0.47679937 0.47713268 0.47735325\n",
      " 0.4776969  0.47787088 0.47809607 0.47896543 0.47866255 0.47834\n",
      " 0.47819752 0.47854844 0.4796009  0.4806101  0.48046982 0.4805581\n",
      " 0.4812402  0.48214704 0.48208606 0.48252195 0.48127735 0.48043436\n",
      " 0.48078194 0.48206294 0.4831041  0.48378068 0.4831317  0.48191166\n",
      " 0.48105782 0.48105937 0.48165604 0.48386592 0.48500645 0.4851429\n",
      " 0.48528963 0.48534727 0.48521188 0.48546636 0.48565167 0.48558038\n",
      " 0.48510176 0.4846064  0.4854741  0.4867807  0.48826408 0.49083275\n",
      " 0.49109825 0.49215603 0.49219936 0.49261367 0.49271166 0.49334395\n",
      " 0.4944635  0.49437922 0.49452418 0.4942137  0.493001   0.49082363\n",
      " 0.4883561  0.4873555  0.49119484 0.48919344 0.48843926]\n",
      "TSLA 0.028063748183617992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     67.187505  0.000000 2013-01-03   63140600 -0.008248\n",
      "2     67.187505  0.000000 2013-01-04   72715400  0.035029\n",
      "3     67.187505  0.000000 2013-01-07   83781800  0.022689\n",
      "4     67.187505  0.000000 2013-01-08   45871300 -0.012312\n",
      "5     67.187505  0.000000 2013-01-09  104787700  0.051311\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  31.108940  5.769428 2018-12-24   22066000 -0.007148\n",
      "1506  45.302163  6.084470 2018-12-26   39723400  0.078417\n",
      "1507  45.706889  6.029865 2018-12-27   31202500  0.002531\n",
      "1508  44.335349  5.864875 2018-12-28   22627600 -0.009861\n",
      "1509  42.157669  5.780956 2018-12-31   24625300 -0.015968\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.71875050e+01  0.00000000e+00  6.31406000e+07 -8.24819237e-03]\n",
      " [ 6.71875050e+01  0.00000000e+00  7.27154000e+07  3.50292230e-02]\n",
      " [ 6.71875050e+01  0.00000000e+00  8.37818000e+07  2.26891770e-02]\n",
      " ...\n",
      " [ 4.93130625e+01  2.63885149e+00  9.49610000e+06  9.21920289e-03]\n",
      " [ 5.01211163e+01  2.54464834e+00  1.22208000e+07  1.68759143e-03]\n",
      " [ 4.62563365e+01  2.53360198e+00  1.02615000e+07 -8.23974093e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "META None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.1387WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 160ms/step - loss: 1.1387\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0179WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0179\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0190WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 167ms/step - loss: 0.0190\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0109WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 167ms/step - loss: 0.0109\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0107WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 164ms/step - loss: 0.0107\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0104WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 184ms/step - loss: 0.0104\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 165ms/step - loss: 0.0096\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0655WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 163ms/step - loss: 0.0655\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0261WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0261\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0074WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 169ms/step - loss: 0.0074\n",
      "6/6 [==============================] - 1s 57ms/step\n",
      "[[0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]\n",
      " [0.2168965]]\n",
      "[0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965\n",
      " 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965 0.2168965]\n",
      "META 0.10910858279680762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date   Volume   Returns\n",
      "1     61.971897  0.000000 2013-01-03  1339800 -0.017466\n",
      "2     61.971897  0.000000 2013-01-04  1457200  0.006167\n",
      "3     61.971897  0.000000 2013-01-07  1432900  0.001489\n",
      "4     61.971897  0.000000 2013-01-08  1892900 -0.010847\n",
      "5     61.971897  0.000000 2013-01-09  1459500  0.009731\n",
      "...         ...       ...        ...      ...       ...\n",
      "1505  42.509944  0.791440 2018-12-24  1771100 -0.004695\n",
      "1506  49.653444  0.822052 2018-12-26  2421200  0.021079\n",
      "1507  45.870627  0.824048 2018-12-27  3242700 -0.012938\n",
      "1508  48.803024  0.811616 2018-12-28  2190500  0.009050\n",
      "1509  49.123845  0.793644 2018-12-31  1719900  0.000974\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.19718974e+01  0.00000000e+00  1.33980000e+06 -1.74659142e-02]\n",
      " [ 6.19718974e+01  0.00000000e+00  1.45720000e+06  6.16651659e-03]\n",
      " [ 6.19718974e+01  0.00000000e+00  1.43290000e+06  1.48901530e-03]\n",
      " ...\n",
      " [ 6.44897739e+01  4.69127944e-01  1.45950000e+06 -7.76912042e-04]\n",
      " [ 6.48089786e+01  4.49190136e-01  1.20730000e+06  7.76912042e-04]\n",
      " [ 6.58019793e+01  4.41390851e-01  2.01340000e+06  2.32633701e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "HSBC None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.7595WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 169ms/step - loss: 1.7595\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0520WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 165ms/step - loss: 0.0520\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0330WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 164ms/step - loss: 0.0330\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0246WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 160ms/step - loss: 0.0246\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0340WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 161ms/step - loss: 0.0340\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0301WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 161ms/step - loss: 0.0301\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0167WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 168ms/step - loss: 0.0167\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0223WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 164ms/step - loss: 0.0223\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0177WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 162ms/step - loss: 0.0177\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0296WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 163ms/step - loss: 0.0296\n",
      "6/6 [==============================] - 1s 51ms/step\n",
      "[[0.3339317 ]\n",
      " [0.334055  ]\n",
      " [0.33410662]\n",
      " [0.33355737]\n",
      " [0.3338745 ]\n",
      " [0.33378097]\n",
      " [0.3334465 ]\n",
      " [0.33369792]\n",
      " [0.33346385]\n",
      " [0.33337268]\n",
      " [0.33309355]\n",
      " [0.33303607]\n",
      " [0.33364955]\n",
      " [0.3335617 ]\n",
      " [0.33314615]\n",
      " [0.33356786]\n",
      " [0.33349466]\n",
      " [0.3329375 ]\n",
      " [0.33326587]\n",
      " [0.33287236]\n",
      " [0.33293042]\n",
      " [0.33339405]\n",
      " [0.33326146]\n",
      " [0.33251506]\n",
      " [0.33300307]\n",
      " [0.33314723]\n",
      " [0.33334488]\n",
      " [0.33321324]\n",
      " [0.33331478]\n",
      " [0.3334579 ]\n",
      " [0.33349425]\n",
      " [0.33353084]\n",
      " [0.33344245]\n",
      " [0.33328214]\n",
      " [0.33367613]\n",
      " [0.33423448]\n",
      " [0.3337512 ]\n",
      " [0.33340627]\n",
      " [0.3335095 ]\n",
      " [0.3332404 ]\n",
      " [0.33337703]\n",
      " [0.3336642 ]\n",
      " [0.33350915]\n",
      " [0.333212  ]\n",
      " [0.33324036]\n",
      " [0.3333947 ]\n",
      " [0.33331266]\n",
      " [0.33394018]\n",
      " [0.33325163]\n",
      " [0.33402437]\n",
      " [0.33388782]\n",
      " [0.33357447]\n",
      " [0.33358115]\n",
      " [0.32945824]\n",
      " [0.3326069 ]\n",
      " [0.33337498]\n",
      " [0.33384192]\n",
      " [0.33375114]\n",
      " [0.33382824]\n",
      " [0.33324793]\n",
      " [0.33323285]\n",
      " [0.33316517]\n",
      " [0.3327774 ]\n",
      " [0.33351058]\n",
      " [0.33383265]\n",
      " [0.33358204]\n",
      " [0.33288595]\n",
      " [0.33409217]\n",
      " [0.33348447]\n",
      " [0.33289725]\n",
      " [0.33320913]\n",
      " [0.33320305]\n",
      " [0.33300316]\n",
      " [0.3338963 ]\n",
      " [0.33375466]\n",
      " [0.3337366 ]\n",
      " [0.33407152]\n",
      " [0.33366472]\n",
      " [0.33378205]\n",
      " [0.33345205]\n",
      " [0.33337462]\n",
      " [0.33344853]\n",
      " [0.3330696 ]\n",
      " [0.3311028 ]\n",
      " [0.33118472]\n",
      " [0.3326406 ]\n",
      " [0.33232108]\n",
      " [0.33209574]\n",
      " [0.33332694]\n",
      " [0.33367777]\n",
      " [0.33338162]\n",
      " [0.33368224]\n",
      " [0.33408105]\n",
      " [0.33436075]\n",
      " [0.3338679 ]\n",
      " [0.3341617 ]\n",
      " [0.33416772]\n",
      " [0.3339865 ]\n",
      " [0.33380324]\n",
      " [0.3334737 ]\n",
      " [0.33258513]\n",
      " [0.33296227]\n",
      " [0.33355665]\n",
      " [0.33330598]\n",
      " [0.33366933]\n",
      " [0.333464  ]\n",
      " [0.33333188]\n",
      " [0.33334726]\n",
      " [0.33325058]\n",
      " [0.33289862]\n",
      " [0.3332072 ]\n",
      " [0.33246717]\n",
      " [0.33358723]\n",
      " [0.33339366]\n",
      " [0.3332579 ]\n",
      " [0.33170944]\n",
      " [0.33108494]\n",
      " [0.331863  ]\n",
      " [0.3326799 ]\n",
      " [0.33279696]\n",
      " [0.33236   ]\n",
      " [0.3327319 ]\n",
      " [0.33273137]\n",
      " [0.33269277]\n",
      " [0.3326271 ]\n",
      " [0.33301377]\n",
      " [0.33336252]\n",
      " [0.33364147]\n",
      " [0.3337481 ]\n",
      " [0.33371177]\n",
      " [0.33388445]\n",
      " [0.3341549 ]\n",
      " [0.3339864 ]\n",
      " [0.3339191 ]\n",
      " [0.3333702 ]\n",
      " [0.3339225 ]\n",
      " [0.3338784 ]\n",
      " [0.3338027 ]\n",
      " [0.33404133]\n",
      " [0.3343847 ]\n",
      " [0.33414063]\n",
      " [0.3340944 ]\n",
      " [0.33421466]\n",
      " [0.33395192]\n",
      " [0.33331275]\n",
      " [0.3333574 ]\n",
      " [0.33321625]\n",
      " [0.33317074]\n",
      " [0.33321333]\n",
      " [0.3336523 ]\n",
      " [0.33393785]\n",
      " [0.33414078]\n",
      " [0.33375838]\n",
      " [0.33378008]\n",
      " [0.33384588]\n",
      " [0.33388346]\n",
      " [0.3340726 ]\n",
      " [0.3337531 ]\n",
      " [0.3338273 ]\n",
      " [0.3336429 ]\n",
      " [0.33356956]\n",
      " [0.3326851 ]\n",
      " [0.3329426 ]\n",
      " [0.33307695]\n",
      " [0.33321953]\n",
      " [0.33363912]\n",
      " [0.33367166]\n",
      " [0.33282295]\n",
      " [0.33385497]\n",
      " [0.33383158]\n",
      " [0.3330316 ]\n",
      " [0.33364385]\n",
      " [0.33353108]\n",
      " [0.3331557 ]\n",
      " [0.3331712 ]\n",
      " [0.3333295 ]\n",
      " [0.3326729 ]\n",
      " [0.3330446 ]\n",
      " [0.33234724]\n",
      " [0.33231375]\n",
      " [0.3327785 ]\n",
      " [0.33297527]\n",
      " [0.33279145]\n",
      " [0.33297837]\n",
      " [0.33283696]\n",
      " [0.33301604]\n",
      " [0.33292785]\n",
      " [0.33320206]\n",
      " [0.33340457]\n",
      " [0.3333051 ]\n",
      " [0.3336134 ]\n",
      " [0.33353725]\n",
      " [0.33330426]\n",
      " [0.33393434]\n",
      " [0.33363605]\n",
      " [0.333516  ]\n",
      " [0.33310285]\n",
      " [0.3335907 ]\n",
      " [0.33359727]\n",
      " [0.33363807]\n",
      " [0.3334924 ]\n",
      " [0.3339604 ]\n",
      " [0.33404383]\n",
      " [0.33374   ]\n",
      " [0.33370292]\n",
      " [0.33349812]\n",
      " [0.33374864]\n",
      " [0.3333779 ]\n",
      " [0.33322853]\n",
      " [0.33349478]\n",
      " [0.33348355]\n",
      " [0.333521  ]\n",
      " [0.33370012]\n",
      " [0.3331973 ]\n",
      " [0.33336696]\n",
      " [0.3331123 ]\n",
      " [0.33279508]\n",
      " [0.33310562]\n",
      " [0.33324683]\n",
      " [0.33321634]\n",
      " [0.33304507]\n",
      " [0.33298078]\n",
      " [0.33262992]\n",
      " [0.33296114]\n",
      " [0.3333088 ]\n",
      " [0.33344978]\n",
      " [0.33322966]\n",
      " [0.33306286]\n",
      " [0.3329352 ]\n",
      " [0.33296925]\n",
      " [0.3326646 ]\n",
      " [0.33241516]\n",
      " [0.33205214]\n",
      " [0.33232874]\n",
      " [0.33221295]\n",
      " [0.33222273]\n",
      " [0.33245057]\n",
      " [0.33285847]\n",
      " [0.3332299 ]\n",
      " [0.33288032]\n",
      " [0.3331636 ]\n",
      " [0.3333484 ]\n",
      " [0.33373088]\n",
      " [0.33290026]\n",
      " [0.3334455 ]\n",
      " [0.33318046]\n",
      " [0.3334071 ]\n",
      " [0.33326226]\n",
      " [0.33335966]\n",
      " [0.33303505]\n",
      " [0.33313444]]\n",
      "[0.3339317  0.334055   0.33410662 0.33355737 0.3338745  0.33378097\n",
      " 0.3334465  0.33369792 0.33346385 0.33337268 0.33309355 0.33303607\n",
      " 0.33364955 0.3335617  0.33314615 0.33356786 0.33349466 0.3329375\n",
      " 0.33326587 0.33287236 0.33293042 0.33339405 0.33326146 0.33251506\n",
      " 0.33300307 0.33314723 0.33334488 0.33321324 0.33331478 0.3334579\n",
      " 0.33349425 0.33353084 0.33344245 0.33328214 0.33367613 0.33423448\n",
      " 0.3337512  0.33340627 0.3335095  0.3332404  0.33337703 0.3336642\n",
      " 0.33350915 0.333212   0.33324036 0.3333947  0.33331266 0.33394018\n",
      " 0.33325163 0.33402437 0.33388782 0.33357447 0.33358115 0.32945824\n",
      " 0.3326069  0.33337498 0.33384192 0.33375114 0.33382824 0.33324793\n",
      " 0.33323285 0.33316517 0.3327774  0.33351058 0.33383265 0.33358204\n",
      " 0.33288595 0.33409217 0.33348447 0.33289725 0.33320913 0.33320305\n",
      " 0.33300316 0.3338963  0.33375466 0.3337366  0.33407152 0.33366472\n",
      " 0.33378205 0.33345205 0.33337462 0.33344853 0.3330696  0.3311028\n",
      " 0.33118472 0.3326406  0.33232108 0.33209574 0.33332694 0.33367777\n",
      " 0.33338162 0.33368224 0.33408105 0.33436075 0.3338679  0.3341617\n",
      " 0.33416772 0.3339865  0.33380324 0.3334737  0.33258513 0.33296227\n",
      " 0.33355665 0.33330598 0.33366933 0.333464   0.33333188 0.33334726\n",
      " 0.33325058 0.33289862 0.3332072  0.33246717 0.33358723 0.33339366\n",
      " 0.3332579  0.33170944 0.33108494 0.331863   0.3326799  0.33279696\n",
      " 0.33236    0.3327319  0.33273137 0.33269277 0.3326271  0.33301377\n",
      " 0.33336252 0.33364147 0.3337481  0.33371177 0.33388445 0.3341549\n",
      " 0.3339864  0.3339191  0.3333702  0.3339225  0.3338784  0.3338027\n",
      " 0.33404133 0.3343847  0.33414063 0.3340944  0.33421466 0.33395192\n",
      " 0.33331275 0.3333574  0.33321625 0.33317074 0.33321333 0.3336523\n",
      " 0.33393785 0.33414078 0.33375838 0.33378008 0.33384588 0.33388346\n",
      " 0.3340726  0.3337531  0.3338273  0.3336429  0.33356956 0.3326851\n",
      " 0.3329426  0.33307695 0.33321953 0.33363912 0.33367166 0.33282295\n",
      " 0.33385497 0.33383158 0.3330316  0.33364385 0.33353108 0.3331557\n",
      " 0.3331712  0.3333295  0.3326729  0.3330446  0.33234724 0.33231375\n",
      " 0.3327785  0.33297527 0.33279145 0.33297837 0.33283696 0.33301604\n",
      " 0.33292785 0.33320206 0.33340457 0.3333051  0.3336134  0.33353725\n",
      " 0.33330426 0.33393434 0.33363605 0.333516   0.33310285 0.3335907\n",
      " 0.33359727 0.33363807 0.3334924  0.3339604  0.33404383 0.33374\n",
      " 0.33370292 0.33349812 0.33374864 0.3333779  0.33322853 0.33349478\n",
      " 0.33348355 0.333521   0.33370012 0.3331973  0.33336696 0.3331123\n",
      " 0.33279508 0.33310562 0.33324683 0.33321634 0.33304507 0.33298078\n",
      " 0.33262992 0.33296114 0.3333088  0.33344978 0.33322966 0.33306286\n",
      " 0.3329352  0.33296925 0.3326646  0.33241516 0.33205214 0.33232874\n",
      " 0.33221295 0.33222273 0.33245057 0.33285847 0.3332299  0.33288032\n",
      " 0.3331636  0.3333484  0.33373088 0.33290026 0.3334455  0.33318046\n",
      " 0.3334071  0.33326226 0.33335966 0.33303505 0.33313444]\n",
      "HSBC 0.011289994063047037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     72.392423  0.000000 2013-01-03   7448100  0.004232\n",
      "2     72.392423  0.000000 2013-01-04  13587200  0.036339\n",
      "3     72.392423  0.000000 2013-01-07   6989100 -0.001359\n",
      "4     72.392423  0.000000 2013-01-08   5246100  0.007546\n",
      "5     72.392423  0.000000 2013-01-09   7267200  0.020792\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  37.523468  2.825029 2018-12-24   3284000 -0.024704\n",
      "1506  48.992228  2.956098 2018-12-26   3792400  0.039851\n",
      "1507  52.857558  3.042805 2018-12-27   4101600  0.016073\n",
      "1508  55.500453  3.004033 2018-12-28   3984000  0.011537\n",
      "1509  58.413700  2.907317 2018-12-31   2731000  0.013222\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 7.23924235e+01  0.00000000e+00  7.44810000e+06  4.23244275e-03]\n",
      " [ 7.23924235e+01  0.00000000e+00  1.35872000e+07  3.63391519e-02]\n",
      " [ 7.23924235e+01  0.00000000e+00  6.98910000e+06 -1.35864120e-03]\n",
      " ...\n",
      " [ 4.81842682e+01  1.13312494e+00  1.33280000e+06  1.76324212e-03]\n",
      " [ 4.74813572e+01  1.09147338e+00  1.73170000e+06 -1.05757214e-03]\n",
      " [ 4.30476431e+01  1.08351123e+00  2.56380000e+06 -6.96127817e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "LLY None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.0002WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 162ms/step - loss: 2.0002\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0377WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 160ms/step - loss: 0.0377\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0184WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 161ms/step - loss: 0.0184\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0123WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 161ms/step - loss: 0.0123\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0166WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 165ms/step - loss: 0.0166\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0215WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 161ms/step - loss: 0.0215\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0212WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 161ms/step - loss: 0.0212\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0166WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 159ms/step - loss: 0.0166\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0139WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 160ms/step - loss: 0.0139\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0164WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 160ms/step - loss: 0.0164\n",
      "6/6 [==============================] - 1s 54ms/step\n",
      "[[0.265287  ]\n",
      " [0.2650045 ]\n",
      " [0.26486987]\n",
      " [0.26373848]\n",
      " [0.26368338]\n",
      " [0.26397872]\n",
      " [0.26394135]\n",
      " [0.2631517 ]\n",
      " [0.26316953]\n",
      " [0.26273832]\n",
      " [0.26338145]\n",
      " [0.26401356]\n",
      " [0.26374838]\n",
      " [0.26103362]\n",
      " [0.26092392]\n",
      " [0.2599041 ]\n",
      " [0.25929466]\n",
      " [0.2567401 ]\n",
      " [0.2561714 ]\n",
      " [0.2591033 ]\n",
      " [0.25976238]\n",
      " [0.2604625 ]\n",
      " [0.2596727 ]\n",
      " [0.26024038]\n",
      " [0.26139402]\n",
      " [0.26130542]\n",
      " [0.26021156]\n",
      " [0.25955683]\n",
      " [0.25980857]\n",
      " [0.25973603]\n",
      " [0.26167038]\n",
      " [0.26088467]\n",
      " [0.26144788]\n",
      " [0.26179948]\n",
      " [0.26216576]\n",
      " [0.26270607]\n",
      " [0.26327896]\n",
      " [0.2635167 ]\n",
      " [0.26338613]\n",
      " [0.26277435]\n",
      " [0.2635591 ]\n",
      " [0.26374277]\n",
      " [0.26408437]\n",
      " [0.26329204]\n",
      " [0.26359627]\n",
      " [0.26406598]\n",
      " [0.2642484 ]\n",
      " [0.26425192]\n",
      " [0.264738  ]\n",
      " [0.26344514]\n",
      " [0.2630747 ]\n",
      " [0.26287395]\n",
      " [0.26325375]\n",
      " [0.2622326 ]\n",
      " [0.26199144]\n",
      " [0.26125357]\n",
      " [0.26114002]\n",
      " [0.2612798 ]\n",
      " [0.26096666]\n",
      " [0.25988346]\n",
      " [0.2603467 ]\n",
      " [0.2613312 ]\n",
      " [0.26208642]\n",
      " [0.26318198]\n",
      " [0.2626594 ]\n",
      " [0.2625754 ]\n",
      " [0.2626742 ]\n",
      " [0.262951  ]\n",
      " [0.26341575]\n",
      " [0.26091167]\n",
      " [0.26202556]\n",
      " [0.2614835 ]\n",
      " [0.26085344]\n",
      " [0.26070988]\n",
      " [0.25912982]\n",
      " [0.25908905]\n",
      " [0.2608187 ]\n",
      " [0.2629998 ]\n",
      " [0.2635063 ]\n",
      " [0.26170358]\n",
      " [0.25569648]\n",
      " [0.25813022]\n",
      " [0.25687543]\n",
      " [0.25096583]\n",
      " [0.25357044]\n",
      " [0.25180268]\n",
      " [0.24968149]\n",
      " [0.25607455]\n",
      " [0.25654906]\n",
      " [0.25725588]\n",
      " [0.2593561 ]\n",
      " [0.2603607 ]\n",
      " [0.26111072]\n",
      " [0.26045653]\n",
      " [0.2601342 ]\n",
      " [0.2595539 ]\n",
      " [0.26132077]\n",
      " [0.2621525 ]\n",
      " [0.26142058]\n",
      " [0.2600604 ]\n",
      " [0.2596793 ]\n",
      " [0.26071268]\n",
      " [0.26101887]\n",
      " [0.26078746]\n",
      " [0.26110888]\n",
      " [0.26209193]\n",
      " [0.26246503]\n",
      " [0.26207685]\n",
      " [0.26205182]\n",
      " [0.26176056]\n",
      " [0.26188293]\n",
      " [0.2625305 ]\n",
      " [0.26182103]\n",
      " [0.26147416]\n",
      " [0.26115802]\n",
      " [0.25953287]\n",
      " [0.25800055]\n",
      " [0.259991  ]\n",
      " [0.25972402]\n",
      " [0.2610413 ]\n",
      " [0.26136112]\n",
      " [0.26006624]\n",
      " [0.2617784 ]\n",
      " [0.26259106]\n",
      " [0.26261982]\n",
      " [0.26141122]\n",
      " [0.26157984]\n",
      " [0.26283377]\n",
      " [0.26278323]\n",
      " [0.26302332]\n",
      " [0.26320508]\n",
      " [0.26336098]\n",
      " [0.26313385]\n",
      " [0.26286837]\n",
      " [0.26273322]\n",
      " [0.26217443]\n",
      " [0.2629585 ]\n",
      " [0.26289582]\n",
      " [0.26362264]\n",
      " [0.26433352]\n",
      " [0.26453912]\n",
      " [0.26312816]\n",
      " [0.26143646]\n",
      " [0.26107338]\n",
      " [0.26050264]\n",
      " [0.26097557]\n",
      " [0.2611669 ]\n",
      " [0.2613308 ]\n",
      " [0.26193607]\n",
      " [0.2631181 ]\n",
      " [0.2639098 ]\n",
      " [0.26416487]\n",
      " [0.2636932 ]\n",
      " [0.2632697 ]\n",
      " [0.26349306]\n",
      " [0.263589  ]\n",
      " [0.26354635]\n",
      " [0.2637146 ]\n",
      " [0.26397493]\n",
      " [0.26400056]\n",
      " [0.26399326]\n",
      " [0.26361573]\n",
      " [0.26429403]\n",
      " [0.26441064]\n",
      " [0.26474842]\n",
      " [0.26476902]\n",
      " [0.2646327 ]\n",
      " [0.26502314]\n",
      " [0.26504436]\n",
      " [0.26506874]\n",
      " [0.26480952]\n",
      " [0.26439953]\n",
      " [0.2644781 ]\n",
      " [0.26477638]\n",
      " [0.2648358 ]\n",
      " [0.26426142]\n",
      " [0.26447576]\n",
      " [0.2638293 ]\n",
      " [0.26366067]\n",
      " [0.26334837]\n",
      " [0.2631368 ]\n",
      " [0.26221004]\n",
      " [0.2615914 ]\n",
      " [0.2625936 ]\n",
      " [0.26244292]\n",
      " [0.26307198]\n",
      " [0.26366088]\n",
      " [0.2637447 ]\n",
      " [0.2643167 ]\n",
      " [0.26459065]\n",
      " [0.26468277]\n",
      " [0.26411733]\n",
      " [0.26475388]\n",
      " [0.26506025]\n",
      " [0.26452312]\n",
      " [0.26472571]\n",
      " [0.26484308]\n",
      " [0.2641989 ]\n",
      " [0.26310346]\n",
      " [0.26342416]\n",
      " [0.26470318]\n",
      " [0.26557428]\n",
      " [0.26557648]\n",
      " [0.26565385]\n",
      " [0.26580498]\n",
      " [0.26598346]\n",
      " [0.26599026]\n",
      " [0.2661209 ]\n",
      " [0.26624185]\n",
      " [0.26637337]\n",
      " [0.26650378]\n",
      " [0.26653898]\n",
      " [0.26651433]\n",
      " [0.26650935]\n",
      " [0.26646915]\n",
      " [0.26649797]\n",
      " [0.2665791 ]\n",
      " [0.2666336 ]\n",
      " [0.26678333]\n",
      " [0.26686484]\n",
      " [0.26644   ]\n",
      " [0.26638198]\n",
      " [0.26620403]\n",
      " [0.26620173]\n",
      " [0.26585448]\n",
      " [0.2657795 ]\n",
      " [0.2657772 ]\n",
      " [0.2657866 ]\n",
      " [0.26576015]\n",
      " [0.26484925]\n",
      " [0.26510364]\n",
      " [0.26497734]\n",
      " [0.26539087]\n",
      " [0.26505974]\n",
      " [0.26432055]\n",
      " [0.26433718]\n",
      " [0.26444784]\n",
      " [0.26393324]\n",
      " [0.26373753]\n",
      " [0.26437643]\n",
      " [0.26355076]\n",
      " [0.26362965]\n",
      " [0.26363403]\n",
      " [0.26312426]\n",
      " [0.26310647]\n",
      " [0.26358148]\n",
      " [0.26397854]\n",
      " [0.26428622]\n",
      " [0.2644369 ]\n",
      " [0.26453206]\n",
      " [0.2648136 ]]\n",
      "[0.265287   0.2650045  0.26486987 0.26373848 0.26368338 0.26397872\n",
      " 0.26394135 0.2631517  0.26316953 0.26273832 0.26338145 0.26401356\n",
      " 0.26374838 0.26103362 0.26092392 0.2599041  0.25929466 0.2567401\n",
      " 0.2561714  0.2591033  0.25976238 0.2604625  0.2596727  0.26024038\n",
      " 0.26139402 0.26130542 0.26021156 0.25955683 0.25980857 0.25973603\n",
      " 0.26167038 0.26088467 0.26144788 0.26179948 0.26216576 0.26270607\n",
      " 0.26327896 0.2635167  0.26338613 0.26277435 0.2635591  0.26374277\n",
      " 0.26408437 0.26329204 0.26359627 0.26406598 0.2642484  0.26425192\n",
      " 0.264738   0.26344514 0.2630747  0.26287395 0.26325375 0.2622326\n",
      " 0.26199144 0.26125357 0.26114002 0.2612798  0.26096666 0.25988346\n",
      " 0.2603467  0.2613312  0.26208642 0.26318198 0.2626594  0.2625754\n",
      " 0.2626742  0.262951   0.26341575 0.26091167 0.26202556 0.2614835\n",
      " 0.26085344 0.26070988 0.25912982 0.25908905 0.2608187  0.2629998\n",
      " 0.2635063  0.26170358 0.25569648 0.25813022 0.25687543 0.25096583\n",
      " 0.25357044 0.25180268 0.24968149 0.25607455 0.25654906 0.25725588\n",
      " 0.2593561  0.2603607  0.26111072 0.26045653 0.2601342  0.2595539\n",
      " 0.26132077 0.2621525  0.26142058 0.2600604  0.2596793  0.26071268\n",
      " 0.26101887 0.26078746 0.26110888 0.26209193 0.26246503 0.26207685\n",
      " 0.26205182 0.26176056 0.26188293 0.2625305  0.26182103 0.26147416\n",
      " 0.26115802 0.25953287 0.25800055 0.259991   0.25972402 0.2610413\n",
      " 0.26136112 0.26006624 0.2617784  0.26259106 0.26261982 0.26141122\n",
      " 0.26157984 0.26283377 0.26278323 0.26302332 0.26320508 0.26336098\n",
      " 0.26313385 0.26286837 0.26273322 0.26217443 0.2629585  0.26289582\n",
      " 0.26362264 0.26433352 0.26453912 0.26312816 0.26143646 0.26107338\n",
      " 0.26050264 0.26097557 0.2611669  0.2613308  0.26193607 0.2631181\n",
      " 0.2639098  0.26416487 0.2636932  0.2632697  0.26349306 0.263589\n",
      " 0.26354635 0.2637146  0.26397493 0.26400056 0.26399326 0.26361573\n",
      " 0.26429403 0.26441064 0.26474842 0.26476902 0.2646327  0.26502314\n",
      " 0.26504436 0.26506874 0.26480952 0.26439953 0.2644781  0.26477638\n",
      " 0.2648358  0.26426142 0.26447576 0.2638293  0.26366067 0.26334837\n",
      " 0.2631368  0.26221004 0.2615914  0.2625936  0.26244292 0.26307198\n",
      " 0.26366088 0.2637447  0.2643167  0.26459065 0.26468277 0.26411733\n",
      " 0.26475388 0.26506025 0.26452312 0.26472571 0.26484308 0.2641989\n",
      " 0.26310346 0.26342416 0.26470318 0.26557428 0.26557648 0.26565385\n",
      " 0.26580498 0.26598346 0.26599026 0.2661209  0.26624185 0.26637337\n",
      " 0.26650378 0.26653898 0.26651433 0.26650935 0.26646915 0.26649797\n",
      " 0.2665791  0.2666336  0.26678333 0.26686484 0.26644    0.26638198\n",
      " 0.26620403 0.26620173 0.26585448 0.2657795  0.2657772  0.2657866\n",
      " 0.26576015 0.26484925 0.26510364 0.26497734 0.26539087 0.26505974\n",
      " 0.26432055 0.26433718 0.26444784 0.26393324 0.26373753 0.26437643\n",
      " 0.26355076 0.26362965 0.26363403 0.26312426 0.26310647 0.26358148\n",
      " 0.26397854 0.26428622 0.2644369  0.26453206 0.2648136 ]\n",
      "LLY 0.01234452057320567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     44.727164  0.000000 2013-01-03  13148600 -0.000552\n",
      "2     44.727164  0.000000 2013-01-04   7464200 -0.007212\n",
      "3     44.727164  0.000000 2013-01-07   9429900 -0.014583\n",
      "4     44.727164  0.000000 2013-01-08   8112900 -0.009080\n",
      "5     44.727164  0.000000 2013-01-09  12977400  0.000000\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  37.537064  0.908531 2018-12-24   6288500 -0.011831\n",
      "1506  49.031619  0.949350 2018-12-26  10063400  0.040810\n",
      "1507  49.168646  0.941540 2018-12-27   8688600  0.000544\n",
      "1508  51.686998  0.938573 2018-12-28   7369000  0.009740\n",
      "1509  49.983193  0.930103 2018-12-31   4600600 -0.006212\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 4.47271642e+01  0.00000000e+00  1.31486000e+07 -5.52437490e-04]\n",
      " [ 4.47271642e+01  0.00000000e+00  7.46420000e+06 -7.21227695e-03]\n",
      " [ 4.47271642e+01  0.00000000e+00  9.42990000e+06 -1.45825059e-02]\n",
      " ...\n",
      " [ 3.99763433e+01  4.79369105e-01  3.09290000e+06  3.33203236e-03]\n",
      " [ 4.92677399e+01  5.02985423e-01  4.27950000e+06  1.67470987e-02]\n",
      " [ 4.81727046e+01  5.07772157e-01  3.87090000e+06 -2.26694442e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 332)           447536    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 332)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                19980     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 468757 (1.79 MB)\n",
      "Trainable params: 468757 (1.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "TSM None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3167WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 107ms/step - loss: 0.3167\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0072WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 112ms/step - loss: 0.0072\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0042WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 0.0042\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0048WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 106ms/step - loss: 0.0048\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0046WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 106ms/step - loss: 0.0046\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0043WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 106ms/step - loss: 0.0043\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0049WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 107ms/step - loss: 0.0049\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0046WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 107ms/step - loss: 0.0046\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0051WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 108ms/step - loss: 0.0051\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0052WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 107ms/step - loss: 0.0052\n",
      "6/6 [==============================] - 0s 38ms/step\n",
      "[[0.24997988]\n",
      " [0.25000453]\n",
      " [0.25009295]\n",
      " [0.24998406]\n",
      " [0.24996677]\n",
      " [0.2500221 ]\n",
      " [0.25010338]\n",
      " [0.25010937]\n",
      " [0.24998489]\n",
      " [0.25002927]\n",
      " [0.24995697]\n",
      " [0.2500149 ]\n",
      " [0.25002822]\n",
      " [0.25007275]\n",
      " [0.25005645]\n",
      " [0.25003636]\n",
      " [0.2500901 ]\n",
      " [0.25002605]\n",
      " [0.25010672]\n",
      " [0.25003654]\n",
      " [0.25008824]\n",
      " [0.24999136]\n",
      " [0.25003633]\n",
      " [0.25012618]\n",
      " [0.2501561 ]\n",
      " [0.24982852]\n",
      " [0.24990243]\n",
      " [0.25007012]\n",
      " [0.25000188]\n",
      " [0.24984735]\n",
      " [0.25002617]\n",
      " [0.24997136]\n",
      " [0.2499718 ]\n",
      " [0.25006896]\n",
      " [0.25000626]\n",
      " [0.25020382]\n",
      " [0.24910535]\n",
      " [0.24957728]\n",
      " [0.24847081]\n",
      " [0.24913631]\n",
      " [0.249273  ]\n",
      " [0.24912211]\n",
      " [0.24921067]\n",
      " [0.24877638]\n",
      " [0.24907047]\n",
      " [0.24924406]\n",
      " [0.24949044]\n",
      " [0.24940574]\n",
      " [0.24945778]\n",
      " [0.24951291]\n",
      " [0.24948215]\n",
      " [0.24947871]\n",
      " [0.24950404]\n",
      " [0.24953568]\n",
      " [0.2494818 ]\n",
      " [0.24984504]\n",
      " [0.24940455]\n",
      " [0.24959464]\n",
      " [0.24988447]\n",
      " [0.24975061]\n",
      " [0.25003925]\n",
      " [0.2500413 ]\n",
      " [0.24998501]\n",
      " [0.25011334]\n",
      " [0.25013325]\n",
      " [0.25004005]\n",
      " [0.24976787]\n",
      " [0.24983557]\n",
      " [0.24988075]\n",
      " [0.24989858]\n",
      " [0.24997413]\n",
      " [0.24991184]\n",
      " [0.24996237]\n",
      " [0.25004277]\n",
      " [0.25006536]\n",
      " [0.24985926]\n",
      " [0.24988452]\n",
      " [0.2500013 ]\n",
      " [0.24976018]\n",
      " [0.24971513]\n",
      " [0.24995631]\n",
      " [0.24994992]\n",
      " [0.24953151]\n",
      " [0.24906975]\n",
      " [0.24950229]\n",
      " [0.24878463]\n",
      " [0.24867   ]\n",
      " [0.2493982 ]\n",
      " [0.2497069 ]\n",
      " [0.24957529]\n",
      " [0.24969718]\n",
      " [0.2497654 ]\n",
      " [0.24973625]\n",
      " [0.24967593]\n",
      " [0.24952641]\n",
      " [0.24938211]\n",
      " [0.24978507]\n",
      " [0.24991557]\n",
      " [0.2496197 ]\n",
      " [0.2496545 ]\n",
      " [0.24945205]\n",
      " [0.24969175]\n",
      " [0.24972868]\n",
      " [0.24958646]\n",
      " [0.2498666 ]\n",
      " [0.24958988]\n",
      " [0.24973544]\n",
      " [0.24989748]\n",
      " [0.24977954]\n",
      " [0.24987468]\n",
      " [0.24990436]\n",
      " [0.24969044]\n",
      " [0.2497182 ]\n",
      " [0.24976635]\n",
      " [0.24972452]\n",
      " [0.24946779]\n",
      " [0.24910662]\n",
      " [0.24979801]\n",
      " [0.2493732 ]\n",
      " [0.24921817]\n",
      " [0.24966007]\n",
      " [0.24918807]\n",
      " [0.2496332 ]\n",
      " [0.24969463]\n",
      " [0.24950135]\n",
      " [0.24931043]\n",
      " [0.2495481 ]\n",
      " [0.24965085]\n",
      " [0.24955407]\n",
      " [0.24963796]\n",
      " [0.24928433]\n",
      " [0.249558  ]\n",
      " [0.24918434]\n",
      " [0.24917306]\n",
      " [0.24722156]\n",
      " [0.24847555]\n",
      " [0.24870583]\n",
      " [0.24870503]\n",
      " [0.24849069]\n",
      " [0.24882916]\n",
      " [0.24879535]\n",
      " [0.24896392]\n",
      " [0.24932265]\n",
      " [0.24843314]\n",
      " [0.24912256]\n",
      " [0.2492376 ]\n",
      " [0.24925418]\n",
      " [0.24946013]\n",
      " [0.24951176]\n",
      " [0.24956438]\n",
      " [0.24970046]\n",
      " [0.24969661]\n",
      " [0.24939862]\n",
      " [0.24962963]\n",
      " [0.24927518]\n",
      " [0.24915135]\n",
      " [0.24962395]\n",
      " [0.2496737 ]\n",
      " [0.24970132]\n",
      " [0.24945274]\n",
      " [0.24964577]\n",
      " [0.24905363]\n",
      " [0.24933234]\n",
      " [0.24946448]\n",
      " [0.24964862]\n",
      " [0.24972591]\n",
      " [0.24962375]\n",
      " [0.2495594 ]\n",
      " [0.24963176]\n",
      " [0.24943271]\n",
      " [0.24956484]\n",
      " [0.24968478]\n",
      " [0.24949355]\n",
      " [0.24938826]\n",
      " [0.2493814 ]\n",
      " [0.24929279]\n",
      " [0.2489332 ]\n",
      " [0.2495746 ]\n",
      " [0.24910805]\n",
      " [0.2494595 ]\n",
      " [0.24928364]\n",
      " [0.2494191 ]\n",
      " [0.24891469]\n",
      " [0.24943607]\n",
      " [0.24961711]\n",
      " [0.24958445]\n",
      " [0.24955013]\n",
      " [0.24953285]\n",
      " [0.24979365]\n",
      " [0.24988306]\n",
      " [0.24990058]\n",
      " [0.24951929]\n",
      " [0.24983557]\n",
      " [0.2499375 ]\n",
      " [0.24988344]\n",
      " [0.24988914]\n",
      " [0.2497116 ]\n",
      " [0.24961382]\n",
      " [0.24978329]\n",
      " [0.24999674]\n",
      " [0.24989629]\n",
      " [0.2499737 ]\n",
      " [0.24987493]\n",
      " [0.2499435 ]\n",
      " [0.24996509]\n",
      " [0.2500047 ]\n",
      " [0.25002807]\n",
      " [0.25000682]\n",
      " [0.25006354]\n",
      " [0.24980736]\n",
      " [0.24987291]\n",
      " [0.2500461 ]\n",
      " [0.25002724]\n",
      " [0.24973902]\n",
      " [0.24974218]\n",
      " [0.24988942]\n",
      " [0.2497018 ]\n",
      " [0.2497662 ]\n",
      " [0.24959123]\n",
      " [0.24971355]\n",
      " [0.24988079]\n",
      " [0.2498681 ]\n",
      " [0.24981822]\n",
      " [0.24996158]\n",
      " [0.24989489]\n",
      " [0.24976656]\n",
      " [0.24987572]\n",
      " [0.24979386]\n",
      " [0.24983132]\n",
      " [0.24990214]\n",
      " [0.24991201]\n",
      " [0.2497234 ]\n",
      " [0.249734  ]\n",
      " [0.24987444]\n",
      " [0.24971348]\n",
      " [0.2496647 ]\n",
      " [0.2495828 ]\n",
      " [0.24996896]\n",
      " [0.2497225 ]\n",
      " [0.24963282]\n",
      " [0.24983157]\n",
      " [0.24988693]\n",
      " [0.24974889]\n",
      " [0.24987252]\n",
      " [0.24968866]\n",
      " [0.249834  ]\n",
      " [0.24986336]\n",
      " [0.24961577]\n",
      " [0.24990603]\n",
      " [0.24952686]\n",
      " [0.24977925]]\n",
      "[0.24997988 0.25000453 0.25009295 0.24998406 0.24996677 0.2500221\n",
      " 0.25010338 0.25010937 0.24998489 0.25002927 0.24995697 0.2500149\n",
      " 0.25002822 0.25007275 0.25005645 0.25003636 0.2500901  0.25002605\n",
      " 0.25010672 0.25003654 0.25008824 0.24999136 0.25003633 0.25012618\n",
      " 0.2501561  0.24982852 0.24990243 0.25007012 0.25000188 0.24984735\n",
      " 0.25002617 0.24997136 0.2499718  0.25006896 0.25000626 0.25020382\n",
      " 0.24910535 0.24957728 0.24847081 0.24913631 0.249273   0.24912211\n",
      " 0.24921067 0.24877638 0.24907047 0.24924406 0.24949044 0.24940574\n",
      " 0.24945778 0.24951291 0.24948215 0.24947871 0.24950404 0.24953568\n",
      " 0.2494818  0.24984504 0.24940455 0.24959464 0.24988447 0.24975061\n",
      " 0.25003925 0.2500413  0.24998501 0.25011334 0.25013325 0.25004005\n",
      " 0.24976787 0.24983557 0.24988075 0.24989858 0.24997413 0.24991184\n",
      " 0.24996237 0.25004277 0.25006536 0.24985926 0.24988452 0.2500013\n",
      " 0.24976018 0.24971513 0.24995631 0.24994992 0.24953151 0.24906975\n",
      " 0.24950229 0.24878463 0.24867    0.2493982  0.2497069  0.24957529\n",
      " 0.24969718 0.2497654  0.24973625 0.24967593 0.24952641 0.24938211\n",
      " 0.24978507 0.24991557 0.2496197  0.2496545  0.24945205 0.24969175\n",
      " 0.24972868 0.24958646 0.2498666  0.24958988 0.24973544 0.24989748\n",
      " 0.24977954 0.24987468 0.24990436 0.24969044 0.2497182  0.24976635\n",
      " 0.24972452 0.24946779 0.24910662 0.24979801 0.2493732  0.24921817\n",
      " 0.24966007 0.24918807 0.2496332  0.24969463 0.24950135 0.24931043\n",
      " 0.2495481  0.24965085 0.24955407 0.24963796 0.24928433 0.249558\n",
      " 0.24918434 0.24917306 0.24722156 0.24847555 0.24870583 0.24870503\n",
      " 0.24849069 0.24882916 0.24879535 0.24896392 0.24932265 0.24843314\n",
      " 0.24912256 0.2492376  0.24925418 0.24946013 0.24951176 0.24956438\n",
      " 0.24970046 0.24969661 0.24939862 0.24962963 0.24927518 0.24915135\n",
      " 0.24962395 0.2496737  0.24970132 0.24945274 0.24964577 0.24905363\n",
      " 0.24933234 0.24946448 0.24964862 0.24972591 0.24962375 0.2495594\n",
      " 0.24963176 0.24943271 0.24956484 0.24968478 0.24949355 0.24938826\n",
      " 0.2493814  0.24929279 0.2489332  0.2495746  0.24910805 0.2494595\n",
      " 0.24928364 0.2494191  0.24891469 0.24943607 0.24961711 0.24958445\n",
      " 0.24955013 0.24953285 0.24979365 0.24988306 0.24990058 0.24951929\n",
      " 0.24983557 0.2499375  0.24988344 0.24988914 0.2497116  0.24961382\n",
      " 0.24978329 0.24999674 0.24989629 0.2499737  0.24987493 0.2499435\n",
      " 0.24996509 0.2500047  0.25002807 0.25000682 0.25006354 0.24980736\n",
      " 0.24987291 0.2500461  0.25002724 0.24973902 0.24974218 0.24988942\n",
      " 0.2497018  0.2497662  0.24959123 0.24971355 0.24988079 0.2498681\n",
      " 0.24981822 0.24996158 0.24989489 0.24976656 0.24987572 0.24979386\n",
      " 0.24983132 0.24990214 0.24991201 0.2497234  0.249734   0.24987444\n",
      " 0.24971348 0.2496647  0.2495828  0.24996896 0.2497225  0.24963282\n",
      " 0.24983157 0.24988693 0.24974889 0.24987252 0.24968866 0.249834\n",
      " 0.24986336 0.24961577 0.24990603 0.24952686 0.24977925]\n",
      "TSM 0.06932742258639095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     66.693394  0.000000 2013-01-03  14937200  0.000772\n",
      "2     66.693394  0.000000 2013-01-04  10376000  0.008134\n",
      "3     66.693394  0.000000 2013-01-07  10242400  0.007119\n",
      "4     66.693394  0.000000 2013-01-08  11800400  0.009267\n",
      "5     66.693394  0.000000 2013-01-09  17552800  0.015134\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  29.106970  4.241238 2018-12-24   8617700 -0.020571\n",
      "1506  44.326920  4.546150 2018-12-26  13499500  0.067497\n",
      "1507  46.897970  4.613568 2018-12-27  10883000  0.013576\n",
      "1508  45.536616  4.566884 2018-12-28   7381300 -0.008139\n",
      "1509  47.082678  4.398535 2018-12-31   7976000  0.007608\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.66933935e+01  0.00000000e+00  1.49372000e+07  7.71898938e-04]\n",
      " [ 6.66933935e+01  0.00000000e+00  1.03760000e+07  8.13441023e-03]\n",
      " [ 6.66933935e+01  0.00000000e+00  1.02424000e+07  7.11856960e-03]\n",
      " ...\n",
      " [ 5.96391507e+01  1.53336575e+00  7.07310000e+06  9.07461299e-03]\n",
      " [ 6.08387162e+01  1.49026821e+00  6.93710000e+06  2.89023704e-03]\n",
      " [ 5.89518303e+01  1.44596354e+00  5.24900000e+06 -2.89023704e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "V None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6222WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 157ms/step - loss: 0.6222\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0107WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 160ms/step - loss: 0.0107\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0042WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 158ms/step - loss: 0.0042\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0043WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 155ms/step - loss: 0.0043\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0044WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 153ms/step - loss: 0.0044\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0065WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 157ms/step - loss: 0.0065\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0035WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 156ms/step - loss: 0.0035\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0088WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 158ms/step - loss: 0.0088\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0051WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 156ms/step - loss: 0.0051\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0046WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 162ms/step - loss: 0.0046\n",
      "6/6 [==============================] - 1s 52ms/step\n",
      "[[0.18075334]\n",
      " [0.18073186]\n",
      " [0.18072301]\n",
      " [0.18071336]\n",
      " [0.18078308]\n",
      " [0.18067577]\n",
      " [0.18075374]\n",
      " [0.18068534]\n",
      " [0.18061408]\n",
      " [0.18070573]\n",
      " [0.18055129]\n",
      " [0.18070692]\n",
      " [0.1806317 ]\n",
      " [0.18068981]\n",
      " [0.18071398]\n",
      " [0.1806851 ]\n",
      " [0.18065561]\n",
      " [0.18068586]\n",
      " [0.18062954]\n",
      " [0.18072543]\n",
      " [0.18065357]\n",
      " [0.1806916 ]\n",
      " [0.18070602]\n",
      " [0.18068013]\n",
      " [0.1807025 ]\n",
      " [0.18066972]\n",
      " [0.18068436]\n",
      " [0.18064092]\n",
      " [0.18077216]\n",
      " [0.18043989]\n",
      " [0.18072864]\n",
      " [0.18044862]\n",
      " [0.1805526 ]\n",
      " [0.1807751 ]\n",
      " [0.18057188]\n",
      " [0.1807381 ]\n",
      " [0.1806831 ]\n",
      " [0.18074116]\n",
      " [0.18000625]\n",
      " [0.18068135]\n",
      " [0.18017992]\n",
      " [0.17979008]\n",
      " [0.18035434]\n",
      " [0.18042155]\n",
      " [0.18052682]\n",
      " [0.18054026]\n",
      " [0.18042234]\n",
      " [0.18055496]\n",
      " [0.18046513]\n",
      " [0.18040106]\n",
      " [0.18053696]\n",
      " [0.18048647]\n",
      " [0.18034536]\n",
      " [0.18045448]\n",
      " [0.18050909]\n",
      " [0.18054685]\n",
      " [0.18054691]\n",
      " [0.18063311]\n",
      " [0.18061604]\n",
      " [0.18055713]\n",
      " [0.18061942]\n",
      " [0.18068019]\n",
      " [0.18060523]\n",
      " [0.18074861]\n",
      " [0.18060352]\n",
      " [0.180561  ]\n",
      " [0.18056394]\n",
      " [0.1806396 ]\n",
      " [0.18059957]\n",
      " [0.18057543]\n",
      " [0.1806423 ]\n",
      " [0.18061793]\n",
      " [0.1805204 ]\n",
      " [0.18063645]\n",
      " [0.18057594]\n",
      " [0.18054417]\n",
      " [0.18059224]\n",
      " [0.18063915]\n",
      " [0.18050638]\n",
      " [0.18048182]\n",
      " [0.18059719]\n",
      " [0.18066701]\n",
      " [0.17990756]\n",
      " [0.17954099]\n",
      " [0.18049654]\n",
      " [0.1801263 ]\n",
      " [0.17917901]\n",
      " [0.18017747]\n",
      " [0.18025813]\n",
      " [0.18008617]\n",
      " [0.18035382]\n",
      " [0.18034571]\n",
      " [0.18020713]\n",
      " [0.18026337]\n",
      " [0.18009378]\n",
      " [0.18022376]\n",
      " [0.1804679 ]\n",
      " [0.18044579]\n",
      " [0.18022287]\n",
      " [0.18026502]\n",
      " [0.17997521]\n",
      " [0.18022344]\n",
      " [0.18031406]\n",
      " [0.18013354]\n",
      " [0.18028945]\n",
      " [0.18027988]\n",
      " [0.1804668 ]\n",
      " [0.18029344]\n",
      " [0.18021427]\n",
      " [0.18023613]\n",
      " [0.18036531]\n",
      " [0.18039475]\n",
      " [0.1801795 ]\n",
      " [0.18049133]\n",
      " [0.18016985]\n",
      " [0.17981881]\n",
      " [0.17968953]\n",
      " [0.1804492 ]\n",
      " [0.17969017]\n",
      " [0.17994222]\n",
      " [0.18031807]\n",
      " [0.1799686 ]\n",
      " [0.1801737 ]\n",
      " [0.18017933]\n",
      " [0.18029241]\n",
      " [0.1797392 ]\n",
      " [0.18017867]\n",
      " [0.1803301 ]\n",
      " [0.18009505]\n",
      " [0.18030456]\n",
      " [0.18018433]\n",
      " [0.18033856]\n",
      " [0.18044437]\n",
      " [0.18038514]\n",
      " [0.18029511]\n",
      " [0.1803665 ]\n",
      " [0.18039241]\n",
      " [0.17996153]\n",
      " [0.18017262]\n",
      " [0.18067929]\n",
      " [0.18022108]\n",
      " [0.18037803]\n",
      " [0.18038222]\n",
      " [0.18024021]\n",
      " [0.18039651]\n",
      " [0.18042633]\n",
      " [0.1804621 ]\n",
      " [0.18044637]\n",
      " [0.18048075]\n",
      " [0.18045154]\n",
      " [0.18051313]\n",
      " [0.18042514]\n",
      " [0.18046877]\n",
      " [0.18048921]\n",
      " [0.18044075]\n",
      " [0.18049115]\n",
      " [0.18055378]\n",
      " [0.18050781]\n",
      " [0.1805901 ]\n",
      " [0.1805312 ]\n",
      " [0.1804872 ]\n",
      " [0.18031795]\n",
      " [0.18053028]\n",
      " [0.18048486]\n",
      " [0.18047994]\n",
      " [0.1806444 ]\n",
      " [0.18053827]\n",
      " [0.18067479]\n",
      " [0.18031675]\n",
      " [0.18055063]\n",
      " [0.18042384]\n",
      " [0.18057159]\n",
      " [0.1804719 ]\n",
      " [0.18053488]\n",
      " [0.18049228]\n",
      " [0.18058926]\n",
      " [0.18042965]\n",
      " [0.18053706]\n",
      " [0.18040867]\n",
      " [0.18056498]\n",
      " [0.17985386]\n",
      " [0.1804271 ]\n",
      " [0.18010212]\n",
      " [0.18043219]\n",
      " [0.18028432]\n",
      " [0.18033914]\n",
      " [0.18018965]\n",
      " [0.18048783]\n",
      " [0.18043095]\n",
      " [0.18050665]\n",
      " [0.18049671]\n",
      " [0.18050632]\n",
      " [0.18054923]\n",
      " [0.1804451 ]\n",
      " [0.18040565]\n",
      " [0.18052687]\n",
      " [0.18054596]\n",
      " [0.1804258 ]\n",
      " [0.18054731]\n",
      " [0.18042095]\n",
      " [0.18045911]\n",
      " [0.18060938]\n",
      " [0.18045083]\n",
      " [0.18028176]\n",
      " [0.17986377]\n",
      " [0.18022293]\n",
      " [0.18035853]\n",
      " [0.18034786]\n",
      " [0.18041804]\n",
      " [0.18034565]\n",
      " [0.18042865]\n",
      " [0.18046513]\n",
      " [0.1803807 ]\n",
      " [0.18040073]\n",
      " [0.18047214]\n",
      " [0.18051778]\n",
      " [0.18037957]\n",
      " [0.18051971]\n",
      " [0.18055093]\n",
      " [0.18049428]\n",
      " [0.18040472]\n",
      " [0.18057257]\n",
      " [0.18057027]\n",
      " [0.18064258]\n",
      " [0.1806126 ]\n",
      " [0.18053779]\n",
      " [0.1806405 ]\n",
      " [0.18053094]\n",
      " [0.18056032]\n",
      " [0.18063094]\n",
      " [0.18001485]\n",
      " [0.18052849]\n",
      " [0.18030521]\n",
      " [0.18047562]\n",
      " [0.18053555]\n",
      " [0.18054844]\n",
      " [0.18057638]\n",
      " [0.18055162]\n",
      " [0.18037593]\n",
      " [0.18058354]\n",
      " [0.18047093]\n",
      " [0.18057317]\n",
      " [0.1805458 ]\n",
      " [0.18046638]\n",
      " [0.1805706 ]\n",
      " [0.18051489]\n",
      " [0.18059045]\n",
      " [0.18054743]\n",
      " [0.18060093]\n",
      " [0.18047853]\n",
      " [0.18052958]]\n",
      "[0.18075334 0.18073186 0.18072301 0.18071336 0.18078308 0.18067577\n",
      " 0.18075374 0.18068534 0.18061408 0.18070573 0.18055129 0.18070692\n",
      " 0.1806317  0.18068981 0.18071398 0.1806851  0.18065561 0.18068586\n",
      " 0.18062954 0.18072543 0.18065357 0.1806916  0.18070602 0.18068013\n",
      " 0.1807025  0.18066972 0.18068436 0.18064092 0.18077216 0.18043989\n",
      " 0.18072864 0.18044862 0.1805526  0.1807751  0.18057188 0.1807381\n",
      " 0.1806831  0.18074116 0.18000625 0.18068135 0.18017992 0.17979008\n",
      " 0.18035434 0.18042155 0.18052682 0.18054026 0.18042234 0.18055496\n",
      " 0.18046513 0.18040106 0.18053696 0.18048647 0.18034536 0.18045448\n",
      " 0.18050909 0.18054685 0.18054691 0.18063311 0.18061604 0.18055713\n",
      " 0.18061942 0.18068019 0.18060523 0.18074861 0.18060352 0.180561\n",
      " 0.18056394 0.1806396  0.18059957 0.18057543 0.1806423  0.18061793\n",
      " 0.1805204  0.18063645 0.18057594 0.18054417 0.18059224 0.18063915\n",
      " 0.18050638 0.18048182 0.18059719 0.18066701 0.17990756 0.17954099\n",
      " 0.18049654 0.1801263  0.17917901 0.18017747 0.18025813 0.18008617\n",
      " 0.18035382 0.18034571 0.18020713 0.18026337 0.18009378 0.18022376\n",
      " 0.1804679  0.18044579 0.18022287 0.18026502 0.17997521 0.18022344\n",
      " 0.18031406 0.18013354 0.18028945 0.18027988 0.1804668  0.18029344\n",
      " 0.18021427 0.18023613 0.18036531 0.18039475 0.1801795  0.18049133\n",
      " 0.18016985 0.17981881 0.17968953 0.1804492  0.17969017 0.17994222\n",
      " 0.18031807 0.1799686  0.1801737  0.18017933 0.18029241 0.1797392\n",
      " 0.18017867 0.1803301  0.18009505 0.18030456 0.18018433 0.18033856\n",
      " 0.18044437 0.18038514 0.18029511 0.1803665  0.18039241 0.17996153\n",
      " 0.18017262 0.18067929 0.18022108 0.18037803 0.18038222 0.18024021\n",
      " 0.18039651 0.18042633 0.1804621  0.18044637 0.18048075 0.18045154\n",
      " 0.18051313 0.18042514 0.18046877 0.18048921 0.18044075 0.18049115\n",
      " 0.18055378 0.18050781 0.1805901  0.1805312  0.1804872  0.18031795\n",
      " 0.18053028 0.18048486 0.18047994 0.1806444  0.18053827 0.18067479\n",
      " 0.18031675 0.18055063 0.18042384 0.18057159 0.1804719  0.18053488\n",
      " 0.18049228 0.18058926 0.18042965 0.18053706 0.18040867 0.18056498\n",
      " 0.17985386 0.1804271  0.18010212 0.18043219 0.18028432 0.18033914\n",
      " 0.18018965 0.18048783 0.18043095 0.18050665 0.18049671 0.18050632\n",
      " 0.18054923 0.1804451  0.18040565 0.18052687 0.18054596 0.1804258\n",
      " 0.18054731 0.18042095 0.18045911 0.18060938 0.18045083 0.18028176\n",
      " 0.17986377 0.18022293 0.18035853 0.18034786 0.18041804 0.18034565\n",
      " 0.18042865 0.18046513 0.1803807  0.18040073 0.18047214 0.18051778\n",
      " 0.18037957 0.18051971 0.18055093 0.18049428 0.18040472 0.18057257\n",
      " 0.18057027 0.18064258 0.1806126  0.18053779 0.1806405  0.18053094\n",
      " 0.18056032 0.18063094 0.18001485 0.18052849 0.18030521 0.18047562\n",
      " 0.18053555 0.18054844 0.18057638 0.18055162 0.18037593 0.18058354\n",
      " 0.18047093 0.18057317 0.1805458  0.18046638 0.1805706  0.18051489\n",
      " 0.18059045 0.18054743 0.18060093 0.18047853 0.18052958]\n",
      "V 0.039342345191253326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     59.375105  0.000000 2013-01-03  14413200 -0.047883\n",
      "2     59.375105  0.000000 2013-01-04  10458000  0.001922\n",
      "3     59.375105  0.000000 2013-01-07   7804500  0.000000\n",
      "4     59.375105  0.000000 2013-01-08   8196200 -0.013335\n",
      "5     59.375105  0.000000 2013-01-09   5154300  0.018696\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  24.998042  7.370625 2018-12-24   3091000 -0.022917\n",
      "1506  36.801896  7.669866 2018-12-26   4159300  0.043720\n",
      "1507  39.387429  7.888447 2018-12-27   4295800  0.010709\n",
      "1508  39.661376  7.680700 2018-12-28   3312700  0.001097\n",
      "1509  42.639537  7.420650 2018-12-31   3123200  0.011628\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.93751052e+01  0.00000000e+00  1.44132000e+07 -4.78830457e-02]\n",
      " [ 5.93751052e+01  0.00000000e+00  1.04580000e+07  1.92162726e-03]\n",
      " [ 5.93751052e+01  0.00000000e+00  7.80450000e+06  0.00000000e+00]\n",
      " ...\n",
      " [ 5.41169669e+01  3.38720948e+00  1.86190000e+06  3.72736936e-03]\n",
      " [ 5.92085036e+01  3.33383733e+00  1.45260000e+06  1.06048675e-02]\n",
      " [ 5.29843816e+01  3.34499111e+00  2.35020000e+06 -1.04235263e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 32)            4736      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                1980      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7957 (31.08 KB)\n",
      "Trainable params: 7957 (31.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "UNH None\n",
      "Epoch 1/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0911WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 8ms/step - loss: 0.0777\n",
      "Epoch 2/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0035WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0039\n",
      "Epoch 3/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0089WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0086\n",
      "Epoch 4/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0032  WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0033\n",
      "Epoch 5/10\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0053WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0051\n",
      "Epoch 6/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0066WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0056\n",
      "Epoch 7/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0053WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0052\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0041WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0041\n",
      "Epoch 9/10\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0046WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0046\n",
      "Epoch 10/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0033WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "[[0.15482998]\n",
      " [0.15483187]\n",
      " [0.15483864]\n",
      " [0.15486294]\n",
      " [0.1549131 ]\n",
      " [0.15499271]\n",
      " [0.1551411 ]\n",
      " [0.15538636]\n",
      " [0.15560687]\n",
      " [0.15569931]\n",
      " [0.15564159]\n",
      " [0.15557754]\n",
      " [0.15553603]\n",
      " [0.15547515]\n",
      " [0.15545648]\n",
      " [0.15544638]\n",
      " [0.15553716]\n",
      " [0.15563318]\n",
      " [0.1555886 ]\n",
      " [0.15568715]\n",
      " [0.15542233]\n",
      " [0.15534386]\n",
      " [0.15523678]\n",
      " [0.15515535]\n",
      " [0.15508711]\n",
      " [0.15499477]\n",
      " [0.15496828]\n",
      " [0.15498593]\n",
      " [0.15498331]\n",
      " [0.15499285]\n",
      " [0.15505917]\n",
      " [0.1551608 ]\n",
      " [0.15523499]\n",
      " [0.15536503]\n",
      " [0.1555087 ]\n",
      " [0.15564516]\n",
      " [0.15580742]\n",
      " [0.15602529]\n",
      " [0.15626402]\n",
      " [0.1565116 ]\n",
      " [0.15671766]\n",
      " [0.1568296 ]\n",
      " [0.15688206]\n",
      " [0.15691167]\n",
      " [0.1569507 ]\n",
      " [0.15699066]\n",
      " [0.15701835]\n",
      " [0.15698424]\n",
      " [0.15701102]\n",
      " [0.15701985]\n",
      " [0.15701878]\n",
      " [0.15709013]\n",
      " [0.15706822]\n",
      " [0.15706216]\n",
      " [0.15704113]\n",
      " [0.15703662]\n",
      " [0.15699214]\n",
      " [0.15696073]\n",
      " [0.15695342]\n",
      " [0.1569308 ]\n",
      " [0.15695438]\n",
      " [0.15700322]\n",
      " [0.15698846]\n",
      " [0.15699118]\n",
      " [0.15697235]\n",
      " [0.15699217]\n",
      " [0.15705068]\n",
      " [0.15713644]\n",
      " [0.15725774]\n",
      " [0.15740785]\n",
      " [0.1575661 ]\n",
      " [0.15768583]\n",
      " [0.15774181]\n",
      " [0.15772972]\n",
      " [0.15778834]\n",
      " [0.15783885]\n",
      " [0.1578697 ]\n",
      " [0.15790619]\n",
      " [0.1577633 ]\n",
      " [0.15721014]\n",
      " [0.15754329]\n",
      " [0.15754572]\n",
      " [0.15755749]\n",
      " [0.15772213]\n",
      " [0.15815908]\n",
      " [0.15848768]\n",
      " [0.15856987]\n",
      " [0.1586082 ]\n",
      " [0.15869081]\n",
      " [0.15869598]\n",
      " [0.1586622 ]\n",
      " [0.15859768]\n",
      " [0.15853569]\n",
      " [0.15850408]\n",
      " [0.15844631]\n",
      " [0.15842155]\n",
      " [0.15840009]\n",
      " [0.15835178]\n",
      " [0.15831992]\n",
      " [0.15824458]\n",
      " [0.15820305]\n",
      " [0.15818863]\n",
      " [0.15817393]\n",
      " [0.15813518]\n",
      " [0.15806636]\n",
      " [0.15802076]\n",
      " [0.15798682]\n",
      " [0.15801488]\n",
      " [0.1580019 ]\n",
      " [0.15799448]\n",
      " [0.15794998]\n",
      " [0.15788615]\n",
      " [0.15786502]\n",
      " [0.15779534]\n",
      " [0.15771282]\n",
      " [0.1575873 ]\n",
      " [0.15769866]\n",
      " [0.15789157]\n",
      " [0.15792245]\n",
      " [0.15793791]\n",
      " [0.15795627]\n",
      " [0.15808013]\n",
      " [0.15818684]\n",
      " [0.15821567]\n",
      " [0.15818644]\n",
      " [0.1581244 ]\n",
      " [0.15800057]\n",
      " [0.15794072]\n",
      " [0.15789291]\n",
      " [0.15780926]\n",
      " [0.15776756]\n",
      " [0.15773812]\n",
      " [0.15778849]\n",
      " [0.15780807]\n",
      " [0.15771215]\n",
      " [0.15761375]\n",
      " [0.15745753]\n",
      " [0.15735273]\n",
      " [0.1572778 ]\n",
      " [0.15721962]\n",
      " [0.15718889]\n",
      " [0.1571647 ]\n",
      " [0.15707646]\n",
      " [0.15703085]\n",
      " [0.15699963]\n",
      " [0.15698346]\n",
      " [0.15694414]\n",
      " [0.15686998]\n",
      " [0.15681869]\n",
      " [0.15681568]\n",
      " [0.15683252]\n",
      " [0.15684417]\n",
      " [0.15684262]\n",
      " [0.15673798]\n",
      " [0.15669873]\n",
      " [0.15666279]\n",
      " [0.15664098]\n",
      " [0.15664373]\n",
      " [0.15657306]\n",
      " [0.15652497]\n",
      " [0.15647711]\n",
      " [0.15645185]\n",
      " [0.1564004 ]\n",
      " [0.15637463]\n",
      " [0.15630099]\n",
      " [0.15625691]\n",
      " [0.1562148 ]\n",
      " [0.15615836]\n",
      " [0.15613443]\n",
      " [0.15609434]\n",
      " [0.15609543]\n",
      " [0.15609533]\n",
      " [0.15604857]\n",
      " [0.15599793]\n",
      " [0.155951  ]\n",
      " [0.15591913]\n",
      " [0.15584017]\n",
      " [0.15576766]\n",
      " [0.1557481 ]\n",
      " [0.15574875]\n",
      " [0.15573516]\n",
      " [0.15567411]\n",
      " [0.15562917]\n",
      " [0.15552686]\n",
      " [0.1555095 ]\n",
      " [0.1554926 ]\n",
      " [0.15545955]\n",
      " [0.1554179 ]\n",
      " [0.15538839]\n",
      " [0.15533262]\n",
      " [0.15530789]\n",
      " [0.15526412]\n",
      " [0.1552297 ]\n",
      " [0.15523468]\n",
      " [0.1552513 ]\n",
      " [0.15523738]\n",
      " [0.1553199 ]\n",
      " [0.1552994 ]\n",
      " [0.15519938]\n",
      " [0.15513812]\n",
      " [0.15503846]\n",
      " [0.15502876]\n",
      " [0.15504861]\n",
      " [0.15505002]\n",
      " [0.15504232]\n",
      " [0.15500417]\n",
      " [0.15500653]\n",
      " [0.15503417]\n",
      " [0.15504989]\n",
      " [0.15505126]\n",
      " [0.15504935]\n",
      " [0.15507479]\n",
      " [0.15515432]\n",
      " [0.15522757]\n",
      " [0.15526658]\n",
      " [0.15534276]\n",
      " [0.15535286]\n",
      " [0.1553742 ]\n",
      " [0.15543295]\n",
      " [0.15546623]\n",
      " [0.15551537]\n",
      " [0.15555257]\n",
      " [0.15562512]\n",
      " [0.15569606]\n",
      " [0.15579066]\n",
      " [0.15587509]\n",
      " [0.15595528]\n",
      " [0.15606421]\n",
      " [0.15612172]\n",
      " [0.15619321]\n",
      " [0.15622078]\n",
      " [0.15629673]\n",
      " [0.15633956]\n",
      " [0.15626876]\n",
      " [0.15643132]\n",
      " [0.15650731]\n",
      " [0.15661466]\n",
      " [0.1566818 ]\n",
      " [0.15672065]\n",
      " [0.15677074]\n",
      " [0.1569163 ]\n",
      " [0.15701142]\n",
      " [0.15703571]\n",
      " [0.15713033]\n",
      " [0.15711805]\n",
      " [0.15718842]\n",
      " [0.1571809 ]\n",
      " [0.15731315]\n",
      " [0.15740284]\n",
      " [0.15747526]\n",
      " [0.15752402]]\n",
      "[0.15482998 0.15483187 0.15483864 0.15486294 0.1549131  0.15499271\n",
      " 0.1551411  0.15538636 0.15560687 0.15569931 0.15564159 0.15557754\n",
      " 0.15553603 0.15547515 0.15545648 0.15544638 0.15553716 0.15563318\n",
      " 0.1555886  0.15568715 0.15542233 0.15534386 0.15523678 0.15515535\n",
      " 0.15508711 0.15499477 0.15496828 0.15498593 0.15498331 0.15499285\n",
      " 0.15505917 0.1551608  0.15523499 0.15536503 0.1555087  0.15564516\n",
      " 0.15580742 0.15602529 0.15626402 0.1565116  0.15671766 0.1568296\n",
      " 0.15688206 0.15691167 0.1569507  0.15699066 0.15701835 0.15698424\n",
      " 0.15701102 0.15701985 0.15701878 0.15709013 0.15706822 0.15706216\n",
      " 0.15704113 0.15703662 0.15699214 0.15696073 0.15695342 0.1569308\n",
      " 0.15695438 0.15700322 0.15698846 0.15699118 0.15697235 0.15699217\n",
      " 0.15705068 0.15713644 0.15725774 0.15740785 0.1575661  0.15768583\n",
      " 0.15774181 0.15772972 0.15778834 0.15783885 0.1578697  0.15790619\n",
      " 0.1577633  0.15721014 0.15754329 0.15754572 0.15755749 0.15772213\n",
      " 0.15815908 0.15848768 0.15856987 0.1586082  0.15869081 0.15869598\n",
      " 0.1586622  0.15859768 0.15853569 0.15850408 0.15844631 0.15842155\n",
      " 0.15840009 0.15835178 0.15831992 0.15824458 0.15820305 0.15818863\n",
      " 0.15817393 0.15813518 0.15806636 0.15802076 0.15798682 0.15801488\n",
      " 0.1580019  0.15799448 0.15794998 0.15788615 0.15786502 0.15779534\n",
      " 0.15771282 0.1575873  0.15769866 0.15789157 0.15792245 0.15793791\n",
      " 0.15795627 0.15808013 0.15818684 0.15821567 0.15818644 0.1581244\n",
      " 0.15800057 0.15794072 0.15789291 0.15780926 0.15776756 0.15773812\n",
      " 0.15778849 0.15780807 0.15771215 0.15761375 0.15745753 0.15735273\n",
      " 0.1572778  0.15721962 0.15718889 0.1571647  0.15707646 0.15703085\n",
      " 0.15699963 0.15698346 0.15694414 0.15686998 0.15681869 0.15681568\n",
      " 0.15683252 0.15684417 0.15684262 0.15673798 0.15669873 0.15666279\n",
      " 0.15664098 0.15664373 0.15657306 0.15652497 0.15647711 0.15645185\n",
      " 0.1564004  0.15637463 0.15630099 0.15625691 0.1562148  0.15615836\n",
      " 0.15613443 0.15609434 0.15609543 0.15609533 0.15604857 0.15599793\n",
      " 0.155951   0.15591913 0.15584017 0.15576766 0.1557481  0.15574875\n",
      " 0.15573516 0.15567411 0.15562917 0.15552686 0.1555095  0.1554926\n",
      " 0.15545955 0.1554179  0.15538839 0.15533262 0.15530789 0.15526412\n",
      " 0.1552297  0.15523468 0.1552513  0.15523738 0.1553199  0.1552994\n",
      " 0.15519938 0.15513812 0.15503846 0.15502876 0.15504861 0.15505002\n",
      " 0.15504232 0.15500417 0.15500653 0.15503417 0.15504989 0.15505126\n",
      " 0.15504935 0.15507479 0.15515432 0.15522757 0.15526658 0.15534276\n",
      " 0.15535286 0.1553742  0.15543295 0.15546623 0.15551537 0.15555257\n",
      " 0.15562512 0.15569606 0.15579066 0.15587509 0.15595528 0.15606421\n",
      " 0.15612172 0.15619321 0.15622078 0.15629673 0.15633956 0.15626876\n",
      " 0.15643132 0.15650731 0.15661466 0.1566818  0.15672065 0.15677074\n",
      " 0.1569163  0.15701142 0.15703571 0.15713033 0.15711805 0.15718842\n",
      " 0.1571809  0.15731315 0.15740284 0.15747526 0.15752402]\n",
      "UNH 0.03305677414288582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     70.560664  0.000000 2013-01-03  13268200 -0.001805\n",
      "2     70.560664  0.000000 2013-01-04  11427900  0.004619\n",
      "3     70.560664  0.000000 2013-01-07  11799800 -0.011646\n",
      "4     70.560664  0.000000 2013-01-08  14226400  0.006236\n",
      "5     70.560664  0.000000 2013-01-09  10892200 -0.003850\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  20.097153  2.067170 2018-12-24  14262800 -0.039068\n",
      "1506  33.197873  2.205944 2018-12-26  24887700  0.046673\n",
      "1507  34.309618  2.252662 2018-12-27  22077000  0.004361\n",
      "1508  32.800758  2.217472 2018-12-28  19710600 -0.011232\n",
      "1509  32.883365  2.165510 2018-12-31  15807000  0.000293\n",
      "\n",
      "[1509 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198\n",
      "[[ 7.05606642e+01  0.00000000e+00  1.32682000e+07 -1.80528488e-03]\n",
      " [ 7.05606642e+01  0.00000000e+00  1.14279000e+07  4.61943771e-03]\n",
      " [ 7.05606642e+01  0.00000000e+00  1.17998000e+07 -1.16460577e-02]\n",
      " ...\n",
      " [ 6.21825398e+01  7.14947103e-01  7.00060000e+06 -9.52692033e-04]\n",
      " [ 6.32127116e+01  6.81736596e-01  7.49530000e+06  1.42909123e-03]\n",
      " [ 5.78391954e+01  6.73040950e-01  8.52340000e+06 -4.53282410e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 32)            4736      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                1980      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7957 (31.08 KB)\n",
      "Trainable params: 7957 (31.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "XOM None\n",
      "Epoch 1/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 8ms/step - loss: 0.0885\n",
      "Epoch 2/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0499WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0433\n",
      "Epoch 3/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0339WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0305\n",
      "Epoch 4/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0496WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0455\n",
      "Epoch 5/10\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0727WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0779\n",
      "Epoch 6/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0384WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0413\n",
      "Epoch 7/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0268WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0322\n",
      "Epoch 8/10\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 0.0301WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0325\n",
      "Epoch 9/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0253WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0295\n",
      "Epoch 10/10\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0224WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0211\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "[[0.2666095 ]\n",
      " [0.26795864]\n",
      " [0.2689247 ]\n",
      " [0.2702956 ]\n",
      " [0.27155828]\n",
      " [0.27273864]\n",
      " [0.27358782]\n",
      " [0.27502042]\n",
      " [0.27645287]\n",
      " [0.2772799 ]\n",
      " [0.27789015]\n",
      " [0.2791459 ]\n",
      " [0.28075045]\n",
      " [0.28240848]\n",
      " [0.28331748]\n",
      " [0.28379303]\n",
      " [0.28430495]\n",
      " [0.28524894]\n",
      " [0.28539905]\n",
      " [0.2857871 ]\n",
      " [0.28656375]\n",
      " [0.28714192]\n",
      " [0.2877644 ]\n",
      " [0.28905925]\n",
      " [0.29084188]\n",
      " [0.2933009 ]\n",
      " [0.29617837]\n",
      " [0.29880163]\n",
      " [0.30133867]\n",
      " [0.30372798]\n",
      " [0.30570346]\n",
      " [0.3077689 ]\n",
      " [0.3098175 ]\n",
      " [0.31205645]\n",
      " [0.3144989 ]\n",
      " [0.31700802]\n",
      " [0.3192289 ]\n",
      " [0.32143176]\n",
      " [0.32381195]\n",
      " [0.32604972]\n",
      " [0.32925642]\n",
      " [0.33198714]\n",
      " [0.33459488]\n",
      " [0.3364346 ]\n",
      " [0.33832583]\n",
      " [0.34074238]\n",
      " [0.3432813 ]\n",
      " [0.34565407]\n",
      " [0.3477929 ]\n",
      " [0.34984738]\n",
      " [0.35153702]\n",
      " [0.35482708]\n",
      " [0.3572696 ]\n",
      " [0.35919887]\n",
      " [0.36144587]\n",
      " [0.36432177]\n",
      " [0.36692145]\n",
      " [0.36909062]\n",
      " [0.37133175]\n",
      " [0.373587  ]\n",
      " [0.3755973 ]\n",
      " [0.3783774 ]\n",
      " [0.38150513]\n",
      " [0.38387856]\n",
      " [0.38630974]\n",
      " [0.3889313 ]\n",
      " [0.3907425 ]\n",
      " [0.3924387 ]\n",
      " [0.39534104]\n",
      " [0.3975825 ]\n",
      " [0.3992916 ]\n",
      " [0.40151435]\n",
      " [0.4027483 ]\n",
      " [0.40424857]\n",
      " [0.40618062]\n",
      " [0.40762576]\n",
      " [0.40903893]\n",
      " [0.41035372]\n",
      " [0.41179484]\n",
      " [0.4122577 ]\n",
      " [0.41205227]\n",
      " [0.41142705]\n",
      " [0.4114536 ]\n",
      " [0.41124362]\n",
      " [0.4098476 ]\n",
      " [0.4088914 ]\n",
      " [0.40742493]\n",
      " [0.40616634]\n",
      " [0.40543586]\n",
      " [0.40482634]\n",
      " [0.40377113]\n",
      " [0.40292835]\n",
      " [0.40186858]\n",
      " [0.40103456]\n",
      " [0.39965302]\n",
      " [0.3978504 ]\n",
      " [0.39654905]\n",
      " [0.39552528]\n",
      " [0.3947527 ]\n",
      " [0.39321923]\n",
      " [0.3912915 ]\n",
      " [0.3897105 ]\n",
      " [0.38859308]\n",
      " [0.3878589 ]\n",
      " [0.38643363]\n",
      " [0.3848389 ]\n",
      " [0.3835976 ]\n",
      " [0.3829022 ]\n",
      " [0.3824034 ]\n",
      " [0.38136154]\n",
      " [0.37996697]\n",
      " [0.3790134 ]\n",
      " [0.37885094]\n",
      " [0.37783125]\n",
      " [0.37710258]\n",
      " [0.3771114 ]\n",
      " [0.37597743]\n",
      " [0.37501755]\n",
      " [0.37478027]\n",
      " [0.3739944 ]\n",
      " [0.3730501 ]\n",
      " [0.37304828]\n",
      " [0.3717248 ]\n",
      " [0.37186116]\n",
      " [0.37106064]\n",
      " [0.37115794]\n",
      " [0.36984345]\n",
      " [0.3688011 ]\n",
      " [0.36888516]\n",
      " [0.36836976]\n",
      " [0.36760873]\n",
      " [0.36725664]\n",
      " [0.36699116]\n",
      " [0.36641824]\n",
      " [0.36647904]\n",
      " [0.36622697]\n",
      " [0.36571133]\n",
      " [0.3657003 ]\n",
      " [0.36459023]\n",
      " [0.36435652]\n",
      " [0.36461642]\n",
      " [0.3627124 ]\n",
      " [0.36187837]\n",
      " [0.3607523 ]\n",
      " [0.35992512]\n",
      " [0.35908505]\n",
      " [0.3582495 ]\n",
      " [0.35751978]\n",
      " [0.35658884]\n",
      " [0.35636476]\n",
      " [0.3564909 ]\n",
      " [0.35630095]\n",
      " [0.3560708 ]\n",
      " [0.35590956]\n",
      " [0.35589823]\n",
      " [0.35586253]\n",
      " [0.35562012]\n",
      " [0.35619676]\n",
      " [0.3561027 ]\n",
      " [0.3566436 ]\n",
      " [0.3557347 ]\n",
      " [0.35445887]\n",
      " [0.35342324]\n",
      " [0.35358334]\n",
      " [0.35303017]\n",
      " [0.35268795]\n",
      " [0.35141098]\n",
      " [0.35021132]\n",
      " [0.34958807]\n",
      " [0.3490614 ]\n",
      " [0.34886006]\n",
      " [0.3482111 ]\n",
      " [0.34733143]\n",
      " [0.34639555]\n",
      " [0.34569418]\n",
      " [0.3450514 ]\n",
      " [0.34450966]\n",
      " [0.34378722]\n",
      " [0.34300715]\n",
      " [0.34174493]\n",
      " [0.3417485 ]\n",
      " [0.33992594]\n",
      " [0.33896077]\n",
      " [0.33844346]\n",
      " [0.3374367 ]\n",
      " [0.33712286]\n",
      " [0.33584565]\n",
      " [0.33498132]\n",
      " [0.33419034]\n",
      " [0.33341894]\n",
      " [0.33286628]\n",
      " [0.33254093]\n",
      " [0.33142924]\n",
      " [0.3308825 ]\n",
      " [0.3307892 ]\n",
      " [0.33004406]\n",
      " [0.32959896]\n",
      " [0.3296945 ]\n",
      " [0.3300884 ]\n",
      " [0.3302608 ]\n",
      " [0.33057892]\n",
      " [0.331656  ]\n",
      " [0.33240712]\n",
      " [0.3333868 ]\n",
      " [0.33321363]\n",
      " [0.33367974]\n",
      " [0.33456102]\n",
      " [0.33507416]\n",
      " [0.3357361 ]\n",
      " [0.3367404 ]\n",
      " [0.33784324]\n",
      " [0.33948815]\n",
      " [0.34026882]\n",
      " [0.3411243 ]\n",
      " [0.34228092]\n",
      " [0.34275255]\n",
      " [0.34310144]\n",
      " [0.34311694]\n",
      " [0.34410018]\n",
      " [0.34499484]\n",
      " [0.34619778]\n",
      " [0.34752494]\n",
      " [0.34973985]\n",
      " [0.35085312]\n",
      " [0.352333  ]\n",
      " [0.35415167]\n",
      " [0.35586306]\n",
      " [0.35785514]\n",
      " [0.35946152]\n",
      " [0.36104608]\n",
      " [0.36246973]\n",
      " [0.3643435 ]\n",
      " [0.3651116 ]\n",
      " [0.36700907]\n",
      " [0.3681161 ]\n",
      " [0.37004396]\n",
      " [0.37186202]\n",
      " [0.37318346]\n",
      " [0.37516046]\n",
      " [0.37714648]\n",
      " [0.37899736]\n",
      " [0.3811127 ]\n",
      " [0.38278708]\n",
      " [0.38565248]\n",
      " [0.38922864]\n",
      " [0.39244416]\n",
      " [0.3956105 ]\n",
      " [0.39849243]\n",
      " [0.4010564 ]\n",
      " [0.4043424 ]\n",
      " [0.4075395 ]]\n",
      "[0.2666095  0.26795864 0.2689247  0.2702956  0.27155828 0.27273864\n",
      " 0.27358782 0.27502042 0.27645287 0.2772799  0.27789015 0.2791459\n",
      " 0.28075045 0.28240848 0.28331748 0.28379303 0.28430495 0.28524894\n",
      " 0.28539905 0.2857871  0.28656375 0.28714192 0.2877644  0.28905925\n",
      " 0.29084188 0.2933009  0.29617837 0.29880163 0.30133867 0.30372798\n",
      " 0.30570346 0.3077689  0.3098175  0.31205645 0.3144989  0.31700802\n",
      " 0.3192289  0.32143176 0.32381195 0.32604972 0.32925642 0.33198714\n",
      " 0.33459488 0.3364346  0.33832583 0.34074238 0.3432813  0.34565407\n",
      " 0.3477929  0.34984738 0.35153702 0.35482708 0.3572696  0.35919887\n",
      " 0.36144587 0.36432177 0.36692145 0.36909062 0.37133175 0.373587\n",
      " 0.3755973  0.3783774  0.38150513 0.38387856 0.38630974 0.3889313\n",
      " 0.3907425  0.3924387  0.39534104 0.3975825  0.3992916  0.40151435\n",
      " 0.4027483  0.40424857 0.40618062 0.40762576 0.40903893 0.41035372\n",
      " 0.41179484 0.4122577  0.41205227 0.41142705 0.4114536  0.41124362\n",
      " 0.4098476  0.4088914  0.40742493 0.40616634 0.40543586 0.40482634\n",
      " 0.40377113 0.40292835 0.40186858 0.40103456 0.39965302 0.3978504\n",
      " 0.39654905 0.39552528 0.3947527  0.39321923 0.3912915  0.3897105\n",
      " 0.38859308 0.3878589  0.38643363 0.3848389  0.3835976  0.3829022\n",
      " 0.3824034  0.38136154 0.37996697 0.3790134  0.37885094 0.37783125\n",
      " 0.37710258 0.3771114  0.37597743 0.37501755 0.37478027 0.3739944\n",
      " 0.3730501  0.37304828 0.3717248  0.37186116 0.37106064 0.37115794\n",
      " 0.36984345 0.3688011  0.36888516 0.36836976 0.36760873 0.36725664\n",
      " 0.36699116 0.36641824 0.36647904 0.36622697 0.36571133 0.3657003\n",
      " 0.36459023 0.36435652 0.36461642 0.3627124  0.36187837 0.3607523\n",
      " 0.35992512 0.35908505 0.3582495  0.35751978 0.35658884 0.35636476\n",
      " 0.3564909  0.35630095 0.3560708  0.35590956 0.35589823 0.35586253\n",
      " 0.35562012 0.35619676 0.3561027  0.3566436  0.3557347  0.35445887\n",
      " 0.35342324 0.35358334 0.35303017 0.35268795 0.35141098 0.35021132\n",
      " 0.34958807 0.3490614  0.34886006 0.3482111  0.34733143 0.34639555\n",
      " 0.34569418 0.3450514  0.34450966 0.34378722 0.34300715 0.34174493\n",
      " 0.3417485  0.33992594 0.33896077 0.33844346 0.3374367  0.33712286\n",
      " 0.33584565 0.33498132 0.33419034 0.33341894 0.33286628 0.33254093\n",
      " 0.33142924 0.3308825  0.3307892  0.33004406 0.32959896 0.3296945\n",
      " 0.3300884  0.3302608  0.33057892 0.331656   0.33240712 0.3333868\n",
      " 0.33321363 0.33367974 0.33456102 0.33507416 0.3357361  0.3367404\n",
      " 0.33784324 0.33948815 0.34026882 0.3411243  0.34228092 0.34275255\n",
      " 0.34310144 0.34311694 0.34410018 0.34499484 0.34619778 0.34752494\n",
      " 0.34973985 0.35085312 0.352333   0.35415167 0.35586306 0.35785514\n",
      " 0.35946152 0.36104608 0.36246973 0.3643435  0.3651116  0.36700907\n",
      " 0.3681161  0.37004396 0.37186202 0.37318346 0.37516046 0.37714648\n",
      " 0.37899736 0.3811127  0.38278708 0.38565248 0.38922864 0.39244416\n",
      " 0.3956105  0.39849243 0.4010564  0.4043424  0.4075395 ]\n",
      "XOM 0.06657430340899109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     72.092950  0.000000 2013-01-03  24227700 -0.002018\n",
      "2     72.092950  0.000000 2013-01-04  24487700  0.017570\n",
      "3     72.092950  0.000000 2013-01-07  24456900  0.001101\n",
      "4     72.092950  0.000000 2013-01-08  19624200  0.001980\n",
      "5     72.092950  0.000000 2013-01-09  25920600 -0.000660\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  20.814920  2.744690 2018-12-24  17009300 -0.021792\n",
      "1506  34.730676  2.895069 2018-12-26  22542900  0.040622\n",
      "1507  38.045705  2.945421 2018-12-27  20304700  0.011192\n",
      "1508  37.645335  2.869320 2018-12-28  17963300 -0.002166\n",
      "1509  40.195012  2.810082 2018-12-31  13237200  0.008126\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 7.20929495e+01  0.00000000e+00  2.42277000e+07 -2.01794426e-03]\n",
      " [ 7.20929495e+01  0.00000000e+00  2.44877000e+07  1.75700690e-02]\n",
      " [ 7.20929495e+01  0.00000000e+00  2.44569000e+07  1.10149659e-03]\n",
      " ...\n",
      " [ 6.26735594e+01  1.62084199e+00  9.49650000e+06  1.86720247e-03]\n",
      " [ 6.45914734e+01  1.55363846e+00  7.44060000e+06  5.30203979e-03]\n",
      " [ 5.96678076e+01  1.53837831e+00  8.92570000e+06 -7.91693193e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "JPM None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.0609WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 164ms/step - loss: 1.0609\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0205WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 163ms/step - loss: 0.0205\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0147WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 168ms/step - loss: 0.0147\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0134WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 164ms/step - loss: 0.0134\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0092WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 167ms/step - loss: 0.0092\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0081WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 163ms/step - loss: 0.0081\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0075WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 166ms/step - loss: 0.0075\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0083WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 160ms/step - loss: 0.0083\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0098WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 161ms/step - loss: 0.0098\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0120WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 160ms/step - loss: 0.0120\n",
      "6/6 [==============================] - 1s 50ms/step\n",
      "[[0.28780586]\n",
      " [0.28762168]\n",
      " [0.28755715]\n",
      " [0.28772333]\n",
      " [0.2876559 ]\n",
      " [0.28754815]\n",
      " [0.2875735 ]\n",
      " [0.2877856 ]\n",
      " [0.28758913]\n",
      " [0.28765222]\n",
      " [0.28764647]\n",
      " [0.2878188 ]\n",
      " [0.28764445]\n",
      " [0.28783968]\n",
      " [0.28775   ]\n",
      " [0.28778046]\n",
      " [0.28768522]\n",
      " [0.28760803]\n",
      " [0.28754205]\n",
      " [0.28764814]\n",
      " [0.28767896]\n",
      " [0.28758025]\n",
      " [0.28750542]\n",
      " [0.28719077]\n",
      " [0.2871704 ]\n",
      " [0.28728208]\n",
      " [0.2872439 ]\n",
      " [0.28733325]\n",
      " [0.28719857]\n",
      " [0.28748477]\n",
      " [0.28740805]\n",
      " [0.28733248]\n",
      " [0.28753638]\n",
      " [0.2874682 ]\n",
      " [0.28735137]\n",
      " [0.28729877]\n",
      " [0.28736842]\n",
      " [0.2878876 ]\n",
      " [0.2878815 ]\n",
      " [0.28777343]\n",
      " [0.28767234]\n",
      " [0.28778726]\n",
      " [0.2873997 ]\n",
      " [0.28740114]\n",
      " [0.28742743]\n",
      " [0.2875642 ]\n",
      " [0.28744602]\n",
      " [0.2876094 ]\n",
      " [0.28734213]\n",
      " [0.28737003]\n",
      " [0.2876969 ]\n",
      " [0.28753066]\n",
      " [0.28742376]\n",
      " [0.28743726]\n",
      " [0.2876106 ]\n",
      " [0.28745037]\n",
      " [0.2873631 ]\n",
      " [0.2874661 ]\n",
      " [0.2874915 ]\n",
      " [0.28738493]\n",
      " [0.2876122 ]\n",
      " [0.2875521 ]\n",
      " [0.28767776]\n",
      " [0.28749308]\n",
      " [0.28756315]\n",
      " [0.28763133]\n",
      " [0.2877195 ]\n",
      " [0.28769025]\n",
      " [0.28783888]\n",
      " [0.28768593]\n",
      " [0.28767604]\n",
      " [0.28765044]\n",
      " [0.28764835]\n",
      " [0.2876805 ]\n",
      " [0.28759906]\n",
      " [0.2877503 ]\n",
      " [0.28764272]\n",
      " [0.28768814]\n",
      " [0.28759205]\n",
      " [0.2875245 ]\n",
      " [0.2876406 ]\n",
      " [0.28766057]\n",
      " [0.2871331 ]\n",
      " [0.28599566]\n",
      " [0.28737384]\n",
      " [0.2870834 ]\n",
      " [0.2860067 ]\n",
      " [0.28708005]\n",
      " [0.28706795]\n",
      " [0.28702915]\n",
      " [0.28719944]\n",
      " [0.2870542 ]\n",
      " [0.28697866]\n",
      " [0.2870825 ]\n",
      " [0.28713644]\n",
      " [0.28713453]\n",
      " [0.2873175 ]\n",
      " [0.2873299 ]\n",
      " [0.28708485]\n",
      " [0.2869323 ]\n",
      " [0.2867935 ]\n",
      " [0.28700003]\n",
      " [0.28714103]\n",
      " [0.2870175 ]\n",
      " [0.28698868]\n",
      " [0.2870636 ]\n",
      " [0.28736395]\n",
      " [0.28708893]\n",
      " [0.28695124]\n",
      " [0.28691566]\n",
      " [0.28709334]\n",
      " [0.2871702 ]\n",
      " [0.28698468]\n",
      " [0.28709316]\n",
      " [0.28705734]\n",
      " [0.28572744]\n",
      " [0.28594387]\n",
      " [0.28712118]\n",
      " [0.2863304 ]\n",
      " [0.28663373]\n",
      " [0.28695965]\n",
      " [0.28637987]\n",
      " [0.28692335]\n",
      " [0.28700182]\n",
      " [0.28702796]\n",
      " [0.28648996]\n",
      " [0.28696594]\n",
      " [0.28706115]\n",
      " [0.28671986]\n",
      " [0.28717154]\n",
      " [0.28639036]\n",
      " [0.28680536]\n",
      " [0.28682643]\n",
      " [0.28676468]\n",
      " [0.2871464 ]\n",
      " [0.28691798]\n",
      " [0.2868788 ]\n",
      " [0.28687316]\n",
      " [0.28686428]\n",
      " [0.2869159 ]\n",
      " [0.28683543]\n",
      " [0.28685012]\n",
      " [0.28691047]\n",
      " [0.28677082]\n",
      " [0.28677124]\n",
      " [0.28707477]\n",
      " [0.28713745]\n",
      " [0.2873022 ]\n",
      " [0.28738722]\n",
      " [0.28728515]\n",
      " [0.2872197 ]\n",
      " [0.28726226]\n",
      " [0.28720516]\n",
      " [0.2872849 ]\n",
      " [0.2872227 ]\n",
      " [0.2870243 ]\n",
      " [0.2873412 ]\n",
      " [0.2873652 ]\n",
      " [0.28722027]\n",
      " [0.28706372]\n",
      " [0.2870503 ]\n",
      " [0.2857546 ]\n",
      " [0.2872765 ]\n",
      " [0.28679323]\n",
      " [0.2872008 ]\n",
      " [0.28705686]\n",
      " [0.28701466]\n",
      " [0.2874514 ]\n",
      " [0.28729758]\n",
      " [0.28729206]\n",
      " [0.2872583 ]\n",
      " [0.28719854]\n",
      " [0.28720117]\n",
      " [0.28684235]\n",
      " [0.28705698]\n",
      " [0.28708896]\n",
      " [0.28695434]\n",
      " [0.28702018]\n",
      " [0.28701696]\n",
      " [0.28651154]\n",
      " [0.2865472 ]\n",
      " [0.2866967 ]\n",
      " [0.28624958]\n",
      " [0.2871065 ]\n",
      " [0.28672725]\n",
      " [0.28702122]\n",
      " [0.28658456]\n",
      " [0.2870226 ]\n",
      " [0.2870843 ]\n",
      " [0.2874996 ]\n",
      " [0.28715205]\n",
      " [0.28718182]\n",
      " [0.28729737]\n",
      " [0.2872562 ]\n",
      " [0.28769994]\n",
      " [0.2873636 ]\n",
      " [0.2874411 ]\n",
      " [0.28722513]\n",
      " [0.28747398]\n",
      " [0.28755784]\n",
      " [0.28745854]\n",
      " [0.28746754]\n",
      " [0.28738672]\n",
      " [0.28752285]\n",
      " [0.28749406]\n",
      " [0.28729856]\n",
      " [0.2874654 ]\n",
      " [0.28743502]\n",
      " [0.28748807]\n",
      " [0.28743184]\n",
      " [0.2874866 ]\n",
      " [0.28746322]\n",
      " [0.28737468]\n",
      " [0.28731725]\n",
      " [0.28713894]\n",
      " [0.28739607]\n",
      " [0.287216  ]\n",
      " [0.28743425]\n",
      " [0.2873503 ]\n",
      " [0.28735742]\n",
      " [0.28746343]\n",
      " [0.28735226]\n",
      " [0.2873686 ]\n",
      " [0.28742233]\n",
      " [0.28765893]\n",
      " [0.28738895]\n",
      " [0.2873749 ]\n",
      " [0.2873457 ]\n",
      " [0.28733724]\n",
      " [0.28744587]\n",
      " [0.28730047]\n",
      " [0.28726107]\n",
      " [0.2873435 ]\n",
      " [0.28720063]\n",
      " [0.2873847 ]\n",
      " [0.2870515 ]\n",
      " [0.2873044 ]\n",
      " [0.28725237]\n",
      " [0.28732842]\n",
      " [0.28742486]\n",
      " [0.28778055]\n",
      " [0.28766057]\n",
      " [0.287538  ]\n",
      " [0.2873252 ]\n",
      " [0.28732622]\n",
      " [0.28715062]\n",
      " [0.28716764]\n",
      " [0.28687906]\n",
      " [0.28721207]\n",
      " [0.28728855]\n",
      " [0.2874449 ]]\n",
      "[0.28780586 0.28762168 0.28755715 0.28772333 0.2876559  0.28754815\n",
      " 0.2875735  0.2877856  0.28758913 0.28765222 0.28764647 0.2878188\n",
      " 0.28764445 0.28783968 0.28775    0.28778046 0.28768522 0.28760803\n",
      " 0.28754205 0.28764814 0.28767896 0.28758025 0.28750542 0.28719077\n",
      " 0.2871704  0.28728208 0.2872439  0.28733325 0.28719857 0.28748477\n",
      " 0.28740805 0.28733248 0.28753638 0.2874682  0.28735137 0.28729877\n",
      " 0.28736842 0.2878876  0.2878815  0.28777343 0.28767234 0.28778726\n",
      " 0.2873997  0.28740114 0.28742743 0.2875642  0.28744602 0.2876094\n",
      " 0.28734213 0.28737003 0.2876969  0.28753066 0.28742376 0.28743726\n",
      " 0.2876106  0.28745037 0.2873631  0.2874661  0.2874915  0.28738493\n",
      " 0.2876122  0.2875521  0.28767776 0.28749308 0.28756315 0.28763133\n",
      " 0.2877195  0.28769025 0.28783888 0.28768593 0.28767604 0.28765044\n",
      " 0.28764835 0.2876805  0.28759906 0.2877503  0.28764272 0.28768814\n",
      " 0.28759205 0.2875245  0.2876406  0.28766057 0.2871331  0.28599566\n",
      " 0.28737384 0.2870834  0.2860067  0.28708005 0.28706795 0.28702915\n",
      " 0.28719944 0.2870542  0.28697866 0.2870825  0.28713644 0.28713453\n",
      " 0.2873175  0.2873299  0.28708485 0.2869323  0.2867935  0.28700003\n",
      " 0.28714103 0.2870175  0.28698868 0.2870636  0.28736395 0.28708893\n",
      " 0.28695124 0.28691566 0.28709334 0.2871702  0.28698468 0.28709316\n",
      " 0.28705734 0.28572744 0.28594387 0.28712118 0.2863304  0.28663373\n",
      " 0.28695965 0.28637987 0.28692335 0.28700182 0.28702796 0.28648996\n",
      " 0.28696594 0.28706115 0.28671986 0.28717154 0.28639036 0.28680536\n",
      " 0.28682643 0.28676468 0.2871464  0.28691798 0.2868788  0.28687316\n",
      " 0.28686428 0.2869159  0.28683543 0.28685012 0.28691047 0.28677082\n",
      " 0.28677124 0.28707477 0.28713745 0.2873022  0.28738722 0.28728515\n",
      " 0.2872197  0.28726226 0.28720516 0.2872849  0.2872227  0.2870243\n",
      " 0.2873412  0.2873652  0.28722027 0.28706372 0.2870503  0.2857546\n",
      " 0.2872765  0.28679323 0.2872008  0.28705686 0.28701466 0.2874514\n",
      " 0.28729758 0.28729206 0.2872583  0.28719854 0.28720117 0.28684235\n",
      " 0.28705698 0.28708896 0.28695434 0.28702018 0.28701696 0.28651154\n",
      " 0.2865472  0.2866967  0.28624958 0.2871065  0.28672725 0.28702122\n",
      " 0.28658456 0.2870226  0.2870843  0.2874996  0.28715205 0.28718182\n",
      " 0.28729737 0.2872562  0.28769994 0.2873636  0.2874411  0.28722513\n",
      " 0.28747398 0.28755784 0.28745854 0.28746754 0.28738672 0.28752285\n",
      " 0.28749406 0.28729856 0.2874654  0.28743502 0.28748807 0.28743184\n",
      " 0.2874866  0.28746322 0.28737468 0.28731725 0.28713894 0.28739607\n",
      " 0.287216   0.28743425 0.2873503  0.28735742 0.28746343 0.28735226\n",
      " 0.2873686  0.28742233 0.28765893 0.28738895 0.2873749  0.2873457\n",
      " 0.28733724 0.28744587 0.28730047 0.28726107 0.2873435  0.28720063\n",
      " 0.2873847  0.2870515  0.2873044  0.28725237 0.28732842 0.28742486\n",
      " 0.28778055 0.28766057 0.287538   0.2873252  0.28732622 0.28715062\n",
      " 0.28716764 0.28687906 0.28721207 0.28728855 0.2874449 ]\n",
      "JPM 0.0470925586413233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     55.765179  0.000000 2013-01-03   8910100 -0.006375\n",
      "2     55.765179  0.000000 2013-01-04   6438000  0.003772\n",
      "3     55.765179  0.000000 2013-01-07   6201400 -0.009603\n",
      "4     55.765179  0.000000 2013-01-08   5866900  0.002774\n",
      "5     55.765179  0.000000 2013-01-09   5055200 -0.000292\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  22.049033  1.980552 2018-12-24   6110300 -0.015149\n",
      "1506  42.246704  2.167655 2018-12-26  10028300  0.052103\n",
      "1507  46.112297  2.239251 2018-12-27   9881500  0.012967\n",
      "1508  47.833130  2.213590 2018-12-28   9874000  0.005879\n",
      "1509  51.015143  2.145477 2018-12-31   7005800  0.011011\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.57651790e+01  0.00000000e+00  8.91010000e+06 -6.37516438e-03]\n",
      " [ 5.57651790e+01  0.00000000e+00  6.43800000e+06  3.77182159e-03]\n",
      " [ 5.57651790e+01  0.00000000e+00  6.20140000e+06 -9.60296427e-03]\n",
      " ...\n",
      " [ 6.94340298e+01  1.28891770e+00  5.14080000e+06  1.00805554e-03]\n",
      " [ 7.00006105e+01  1.23256643e+00  9.76390000e+06  1.40950470e-03]\n",
      " [ 6.40631205e+01  1.21166901e+00  7.14430000e+06 -6.56084255e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 272)           301376    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 272)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                16380     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318997 (1.22 MB)\n",
      "Trainable params: 318997 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "WMT None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3203WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 89ms/step - loss: 0.3203\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0352WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.0352\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0245WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.0245\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0199WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.0199\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 88ms/step - loss: 0.0175\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0177WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.0177\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.0175\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0164WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 92ms/step - loss: 0.0164\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0151WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.0151\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0135WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.0135\n",
      "6/6 [==============================] - 0s 23ms/step\n",
      "[[0.35682887]\n",
      " [0.35657826]\n",
      " [0.35703152]\n",
      " [0.35765743]\n",
      " [0.35821193]\n",
      " [0.35842392]\n",
      " [0.35845345]\n",
      " [0.35824054]\n",
      " [0.3581888 ]\n",
      " [0.35820168]\n",
      " [0.35818154]\n",
      " [0.35824093]\n",
      " [0.3583339 ]\n",
      " [0.3583315 ]\n",
      " [0.3583233 ]\n",
      " [0.35832596]\n",
      " [0.35813513]\n",
      " [0.35777265]\n",
      " [0.3578483 ]\n",
      " [0.3580606 ]\n",
      " [0.3582641 ]\n",
      " [0.35836834]\n",
      " [0.3581723 ]\n",
      " [0.35817048]\n",
      " [0.35829103]\n",
      " [0.35837436]\n",
      " [0.3584016 ]\n",
      " [0.35839266]\n",
      " [0.35783297]\n",
      " [0.3570062 ]\n",
      " [0.35707897]\n",
      " [0.35771686]\n",
      " [0.35765418]\n",
      " [0.3575991 ]\n",
      " [0.35766596]\n",
      " [0.3576454 ]\n",
      " [0.35753945]\n",
      " [0.35750344]\n",
      " [0.35749352]\n",
      " [0.3574412 ]\n",
      " [0.3573928 ]\n",
      " [0.35728896]\n",
      " [0.35729986]\n",
      " [0.35719734]\n",
      " [0.35712802]\n",
      " [0.35704952]\n",
      " [0.3570184 ]\n",
      " [0.3569633 ]\n",
      " [0.35692707]\n",
      " [0.3566961 ]\n",
      " [0.35662425]\n",
      " [0.35678518]\n",
      " [0.35685015]\n",
      " [0.35685527]\n",
      " [0.35663754]\n",
      " [0.35667187]\n",
      " [0.35677066]\n",
      " [0.3567577 ]\n",
      " [0.35664254]\n",
      " [0.35637838]\n",
      " [0.3562926 ]\n",
      " [0.35647827]\n",
      " [0.35661125]\n",
      " [0.35667107]\n",
      " [0.3567028 ]\n",
      " [0.35628003]\n",
      " [0.35596925]\n",
      " [0.35600045]\n",
      " [0.3560936 ]\n",
      " [0.35604036]\n",
      " [0.35623768]\n",
      " [0.35641533]\n",
      " [0.35652062]\n",
      " [0.3565359 ]\n",
      " [0.35653162]\n",
      " [0.35643882]\n",
      " [0.35640156]\n",
      " [0.3563378 ]\n",
      " [0.3562029 ]\n",
      " [0.35558784]\n",
      " [0.3550862 ]\n",
      " [0.35473394]\n",
      " [0.3541256 ]\n",
      " [0.35333824]\n",
      " [0.3533839 ]\n",
      " [0.35362184]\n",
      " [0.35331237]\n",
      " [0.35321727]\n",
      " [0.3532297 ]\n",
      " [0.35332656]\n",
      " [0.35325933]\n",
      " [0.35326743]\n",
      " [0.35302767]\n",
      " [0.3518127 ]\n",
      " [0.35197842]\n",
      " [0.35224712]\n",
      " [0.35236305]\n",
      " [0.35234022]\n",
      " [0.35213092]\n",
      " [0.3519107 ]\n",
      " [0.35180482]\n",
      " [0.35187334]\n",
      " [0.3520944 ]\n",
      " [0.3520115 ]\n",
      " [0.35188067]\n",
      " [0.35198843]\n",
      " [0.3521307 ]\n",
      " [0.35211462]\n",
      " [0.35202435]\n",
      " [0.35181192]\n",
      " [0.35189328]\n",
      " [0.352197  ]\n",
      " [0.35225892]\n",
      " [0.35226452]\n",
      " [0.35232365]\n",
      " [0.35215676]\n",
      " [0.35200447]\n",
      " [0.35222363]\n",
      " [0.35214457]\n",
      " [0.35235155]\n",
      " [0.35248983]\n",
      " [0.35225505]\n",
      " [0.35232857]\n",
      " [0.3525265 ]\n",
      " [0.3525992 ]\n",
      " [0.3524701 ]\n",
      " [0.35235095]\n",
      " [0.35234487]\n",
      " [0.3522588 ]\n",
      " [0.352148  ]\n",
      " [0.35223764]\n",
      " [0.3524882 ]\n",
      " [0.3527386 ]\n",
      " [0.35278437]\n",
      " [0.3527863 ]\n",
      " [0.35263726]\n",
      " [0.3524807 ]\n",
      " [0.35258493]\n",
      " [0.35282546]\n",
      " [0.35301876]\n",
      " [0.35298038]\n",
      " [0.35311708]\n",
      " [0.35298538]\n",
      " [0.35274392]\n",
      " [0.35273716]\n",
      " [0.3528848 ]\n",
      " [0.35259956]\n",
      " [0.35239065]\n",
      " [0.35212886]\n",
      " [0.35216182]\n",
      " [0.35244277]\n",
      " [0.35267818]\n",
      " [0.35278898]\n",
      " [0.3529443 ]\n",
      " [0.35292822]\n",
      " [0.35289848]\n",
      " [0.3530473 ]\n",
      " [0.35299313]\n",
      " [0.35294145]\n",
      " [0.35304576]\n",
      " [0.3530804 ]\n",
      " [0.35322064]\n",
      " [0.35366398]\n",
      " [0.3536077 ]\n",
      " [0.35376135]\n",
      " [0.35425302]\n",
      " [0.35439107]\n",
      " [0.3544316 ]\n",
      " [0.3545478 ]\n",
      " [0.35454035]\n",
      " [0.354546  ]\n",
      " [0.3545686 ]\n",
      " [0.35459608]\n",
      " [0.35452974]\n",
      " [0.35453218]\n",
      " [0.354474  ]\n",
      " [0.3547241 ]\n",
      " [0.35482913]\n",
      " [0.35502714]\n",
      " [0.35530245]\n",
      " [0.355667  ]\n",
      " [0.3557331 ]\n",
      " [0.35575476]\n",
      " [0.35552925]\n",
      " [0.3552829 ]\n",
      " [0.35485014]\n",
      " [0.3548814 ]\n",
      " [0.3549102 ]\n",
      " [0.3549374 ]\n",
      " [0.35531723]\n",
      " [0.3555981 ]\n",
      " [0.35548007]\n",
      " [0.3554442 ]\n",
      " [0.35560018]\n",
      " [0.35567644]\n",
      " [0.35572153]\n",
      " [0.35570657]\n",
      " [0.355582  ]\n",
      " [0.3555748 ]\n",
      " [0.3554564 ]\n",
      " [0.35548633]\n",
      " [0.3555232 ]\n",
      " [0.35560256]\n",
      " [0.35567126]\n",
      " [0.35580927]\n",
      " [0.3558752 ]\n",
      " [0.35556304]\n",
      " [0.3557043 ]\n",
      " [0.35595384]\n",
      " [0.3561113 ]\n",
      " [0.35617584]\n",
      " [0.3562352 ]\n",
      " [0.35608888]\n",
      " [0.35624593]\n",
      " [0.3561729 ]\n",
      " [0.35585183]\n",
      " [0.35547298]\n",
      " [0.3557428 ]\n",
      " [0.35649684]\n",
      " [0.35614854]\n",
      " [0.35610324]\n",
      " [0.35602388]\n",
      " [0.35588092]\n",
      " [0.3557041 ]\n",
      " [0.3555833 ]\n",
      " [0.35565168]\n",
      " [0.35564524]\n",
      " [0.35560146]\n",
      " [0.35545117]\n",
      " [0.35529342]\n",
      " [0.35540697]\n",
      " [0.35542417]\n",
      " [0.3552765 ]\n",
      " [0.35536975]\n",
      " [0.35534292]\n",
      " [0.35508043]\n",
      " [0.35480976]\n",
      " [0.35467488]\n",
      " [0.35473892]\n",
      " [0.35487625]\n",
      " [0.35488665]\n",
      " [0.35497624]\n",
      " [0.35496902]\n",
      " [0.35466194]\n",
      " [0.35457492]\n",
      " [0.35437566]\n",
      " [0.35416842]\n",
      " [0.354138  ]\n",
      " [0.35436416]\n",
      " [0.35457653]\n",
      " [0.35432667]]\n",
      "[0.35682887 0.35657826 0.35703152 0.35765743 0.35821193 0.35842392\n",
      " 0.35845345 0.35824054 0.3581888  0.35820168 0.35818154 0.35824093\n",
      " 0.3583339  0.3583315  0.3583233  0.35832596 0.35813513 0.35777265\n",
      " 0.3578483  0.3580606  0.3582641  0.35836834 0.3581723  0.35817048\n",
      " 0.35829103 0.35837436 0.3584016  0.35839266 0.35783297 0.3570062\n",
      " 0.35707897 0.35771686 0.35765418 0.3575991  0.35766596 0.3576454\n",
      " 0.35753945 0.35750344 0.35749352 0.3574412  0.3573928  0.35728896\n",
      " 0.35729986 0.35719734 0.35712802 0.35704952 0.3570184  0.3569633\n",
      " 0.35692707 0.3566961  0.35662425 0.35678518 0.35685015 0.35685527\n",
      " 0.35663754 0.35667187 0.35677066 0.3567577  0.35664254 0.35637838\n",
      " 0.3562926  0.35647827 0.35661125 0.35667107 0.3567028  0.35628003\n",
      " 0.35596925 0.35600045 0.3560936  0.35604036 0.35623768 0.35641533\n",
      " 0.35652062 0.3565359  0.35653162 0.35643882 0.35640156 0.3563378\n",
      " 0.3562029  0.35558784 0.3550862  0.35473394 0.3541256  0.35333824\n",
      " 0.3533839  0.35362184 0.35331237 0.35321727 0.3532297  0.35332656\n",
      " 0.35325933 0.35326743 0.35302767 0.3518127  0.35197842 0.35224712\n",
      " 0.35236305 0.35234022 0.35213092 0.3519107  0.35180482 0.35187334\n",
      " 0.3520944  0.3520115  0.35188067 0.35198843 0.3521307  0.35211462\n",
      " 0.35202435 0.35181192 0.35189328 0.352197   0.35225892 0.35226452\n",
      " 0.35232365 0.35215676 0.35200447 0.35222363 0.35214457 0.35235155\n",
      " 0.35248983 0.35225505 0.35232857 0.3525265  0.3525992  0.3524701\n",
      " 0.35235095 0.35234487 0.3522588  0.352148   0.35223764 0.3524882\n",
      " 0.3527386  0.35278437 0.3527863  0.35263726 0.3524807  0.35258493\n",
      " 0.35282546 0.35301876 0.35298038 0.35311708 0.35298538 0.35274392\n",
      " 0.35273716 0.3528848  0.35259956 0.35239065 0.35212886 0.35216182\n",
      " 0.35244277 0.35267818 0.35278898 0.3529443  0.35292822 0.35289848\n",
      " 0.3530473  0.35299313 0.35294145 0.35304576 0.3530804  0.35322064\n",
      " 0.35366398 0.3536077  0.35376135 0.35425302 0.35439107 0.3544316\n",
      " 0.3545478  0.35454035 0.354546   0.3545686  0.35459608 0.35452974\n",
      " 0.35453218 0.354474   0.3547241  0.35482913 0.35502714 0.35530245\n",
      " 0.355667   0.3557331  0.35575476 0.35552925 0.3552829  0.35485014\n",
      " 0.3548814  0.3549102  0.3549374  0.35531723 0.3555981  0.35548007\n",
      " 0.3554442  0.35560018 0.35567644 0.35572153 0.35570657 0.355582\n",
      " 0.3555748  0.3554564  0.35548633 0.3555232  0.35560256 0.35567126\n",
      " 0.35580927 0.3558752  0.35556304 0.3557043  0.35595384 0.3561113\n",
      " 0.35617584 0.3562352  0.35608888 0.35624593 0.3561729  0.35585183\n",
      " 0.35547298 0.3557428  0.35649684 0.35614854 0.35610324 0.35602388\n",
      " 0.35588092 0.3557041  0.3555833  0.35565168 0.35564524 0.35560146\n",
      " 0.35545117 0.35529342 0.35540697 0.35542417 0.3552765  0.35536975\n",
      " 0.35534292 0.35508043 0.35480976 0.35467488 0.35473892 0.35487625\n",
      " 0.35488665 0.35497624 0.35496902 0.35466194 0.35457492 0.35437566\n",
      " 0.35416842 0.354138   0.35436416 0.35457653 0.35432667]\n",
      "WMT 0.13994276803597389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date   Volume   Returns\n",
      "1     85.021113  0.000000 2013-01-03  2462000 -0.002898\n",
      "2     85.021113  0.000000 2013-01-04  1611000  0.005907\n",
      "3     85.021113  0.000000 2013-01-07  2802000  0.003420\n",
      "4     85.021113  0.000000 2013-01-08  4150000  0.009775\n",
      "5     85.021113  0.000000 2013-01-09  2535000  0.002133\n",
      "...         ...       ...        ...      ...       ...\n",
      "1505  41.594247  0.458917 2018-12-24  1744400 -0.011077\n",
      "1506  49.161167  0.478637 2018-12-26  2354600  0.024254\n",
      "1507  52.361346  0.479448 2018-12-27  2486000  0.011472\n",
      "1508  53.806659  0.470202 2018-12-28  2713600  0.005250\n",
      "1509  55.268175  0.450545 2018-12-31  1450400  0.005223\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 8.50211128e+01  0.00000000e+00  2.46200000e+06 -2.89752866e-03]\n",
      " [ 8.50211128e+01  0.00000000e+00  1.61100000e+06  5.90724238e-03]\n",
      " [ 8.50211128e+01  0.00000000e+00  2.80200000e+06  3.42024300e-03]\n",
      " ...\n",
      " [ 6.35601808e+01  3.29168476e-01  1.31580000e+06 -1.86557945e-04]\n",
      " [ 6.47029836e+01  3.15656399e-01  8.70800000e+05  2.98232995e-03]\n",
      " [ 6.38937427e+01  3.09180969e-01  1.12360000e+06 -1.11735354e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 392)           622496    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 392)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                23580     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 647317 (2.47 MB)\n",
      "Trainable params: 647317 (2.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "NVO None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9935WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 134ms/step - loss: 0.9935\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0546WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 133ms/step - loss: 0.0546\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0666WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 140ms/step - loss: 0.0666\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0511WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 143ms/step - loss: 0.0511\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0381WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 137ms/step - loss: 0.0381\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0667WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 135ms/step - loss: 0.0667\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0744WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 142ms/step - loss: 0.0744\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1093WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 140ms/step - loss: 0.1093\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0770WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 137ms/step - loss: 0.0770\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0504WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 137ms/step - loss: 0.0504\n",
      "6/6 [==============================] - 0s 42ms/step\n",
      "[[0.6011954 ]\n",
      " [0.6011973 ]\n",
      " [0.6012346 ]\n",
      " [0.60093606]\n",
      " [0.6011977 ]\n",
      " [0.6010232 ]\n",
      " [0.6002477 ]\n",
      " [0.60086226]\n",
      " [0.6009017 ]\n",
      " [0.6010343 ]\n",
      " [0.6011956 ]\n",
      " [0.6012196 ]\n",
      " [0.6011208 ]\n",
      " [0.6011396 ]\n",
      " [0.60112107]\n",
      " [0.6006004 ]\n",
      " [0.60076237]\n",
      " [0.60089296]\n",
      " [0.6009108 ]\n",
      " [0.6008745 ]\n",
      " [0.60083675]\n",
      " [0.6009592 ]\n",
      " [0.6008968 ]\n",
      " [0.6008712 ]\n",
      " [0.60104966]\n",
      " [0.6005645 ]\n",
      " [0.6008312 ]\n",
      " [0.6008161 ]\n",
      " [0.60085934]\n",
      " [0.60079426]\n",
      " [0.60100305]\n",
      " [0.60113215]\n",
      " [0.6012562 ]\n",
      " [0.601356  ]\n",
      " [0.60137516]\n",
      " [0.60146576]\n",
      " [0.60120344]\n",
      " [0.6012565 ]\n",
      " [0.60108286]\n",
      " [0.6011159 ]\n",
      " [0.6012259 ]\n",
      " [0.6008687 ]\n",
      " [0.600608  ]\n",
      " [0.60095084]\n",
      " [0.60081136]\n",
      " [0.60090786]\n",
      " [0.6009801 ]\n",
      " [0.6011433 ]\n",
      " [0.6012537 ]\n",
      " [0.6010925 ]\n",
      " [0.6009539 ]\n",
      " [0.6010122 ]\n",
      " [0.6010114 ]\n",
      " [0.6009455 ]\n",
      " [0.60106856]\n",
      " [0.6011331 ]\n",
      " [0.60118383]\n",
      " [0.6012072 ]\n",
      " [0.6012544 ]\n",
      " [0.60122854]\n",
      " [0.601302  ]\n",
      " [0.60144806]\n",
      " [0.601413  ]\n",
      " [0.60145736]\n",
      " [0.60145813]\n",
      " [0.60149735]\n",
      " [0.60146976]\n",
      " [0.6015005 ]\n",
      " [0.60154355]\n",
      " [0.60153544]\n",
      " [0.60164505]\n",
      " [0.60170865]\n",
      " [0.601729  ]\n",
      " [0.6018469 ]\n",
      " [0.6018743 ]\n",
      " [0.60178447]\n",
      " [0.6015969 ]\n",
      " [0.6016736 ]\n",
      " [0.60149235]\n",
      " [0.60106695]\n",
      " [0.60046315]\n",
      " [0.5985844 ]\n",
      " [0.5984875 ]\n",
      " [0.59818864]\n",
      " [0.5985616 ]\n",
      " [0.598809  ]\n",
      " [0.5982895 ]\n",
      " [0.59933805]\n",
      " [0.5994437 ]\n",
      " [0.59933865]\n",
      " [0.5996721 ]\n",
      " [0.5999515 ]\n",
      " [0.60005516]\n",
      " [0.59973323]\n",
      " [0.5999305 ]\n",
      " [0.60028505]\n",
      " [0.6004695 ]\n",
      " [0.60065407]\n",
      " [0.60013026]\n",
      " [0.5998776 ]\n",
      " [0.59954596]\n",
      " [0.59987277]\n",
      " [0.5999137 ]\n",
      " [0.59987307]\n",
      " [0.59993535]\n",
      " [0.6001213 ]\n",
      " [0.6000822 ]\n",
      " [0.6001339 ]\n",
      " [0.5999484 ]\n",
      " [0.5999575 ]\n",
      " [0.6000304 ]\n",
      " [0.60008955]\n",
      " [0.60009223]\n",
      " [0.6001564 ]\n",
      " [0.600037  ]\n",
      " [0.599549  ]\n",
      " [0.60001725]\n",
      " [0.6004629 ]\n",
      " [0.6000522 ]\n",
      " [0.6003122 ]\n",
      " [0.6003595 ]\n",
      " [0.59999084]\n",
      " [0.60033995]\n",
      " [0.60024565]\n",
      " [0.6002542 ]\n",
      " [0.60009927]\n",
      " [0.6004995 ]\n",
      " [0.6007365 ]\n",
      " [0.6006052 ]\n",
      " [0.60065407]\n",
      " [0.6003777 ]\n",
      " [0.60036975]\n",
      " [0.6004579 ]\n",
      " [0.6002561 ]\n",
      " [0.59983397]\n",
      " [0.59969944]\n",
      " [0.5997101 ]\n",
      " [0.5996967 ]\n",
      " [0.5996513 ]\n",
      " [0.5998882 ]\n",
      " [0.6000864 ]\n",
      " [0.5998784 ]\n",
      " [0.6002764 ]\n",
      " [0.6007117 ]\n",
      " [0.60044646]\n",
      " [0.6003453 ]\n",
      " [0.6003676 ]\n",
      " [0.6005938 ]\n",
      " [0.6004784 ]\n",
      " [0.6004947 ]\n",
      " [0.600758  ]\n",
      " [0.6010004 ]\n",
      " [0.6004029 ]\n",
      " [0.6006073 ]\n",
      " [0.6006658 ]\n",
      " [0.60062057]\n",
      " [0.6006213 ]\n",
      " [0.60063803]\n",
      " [0.60052806]\n",
      " [0.6004613 ]\n",
      " [0.6003059 ]\n",
      " [0.59990466]\n",
      " [0.6003541 ]\n",
      " [0.6003354 ]\n",
      " [0.6004704 ]\n",
      " [0.6004015 ]\n",
      " [0.6003125 ]\n",
      " [0.6000917 ]\n",
      " [0.5997245 ]\n",
      " [0.59917665]\n",
      " [0.59932053]\n",
      " [0.59908915]\n",
      " [0.5993873 ]\n",
      " [0.5995475 ]\n",
      " [0.5997412 ]\n",
      " [0.5996869 ]\n",
      " [0.5996997 ]\n",
      " [0.60035545]\n",
      " [0.6006571 ]\n",
      " [0.6008249 ]\n",
      " [0.6009088 ]\n",
      " [0.6005823 ]\n",
      " [0.6002885 ]\n",
      " [0.60029894]\n",
      " [0.60056746]\n",
      " [0.6008112 ]\n",
      " [0.60085785]\n",
      " [0.60104674]\n",
      " [0.60120285]\n",
      " [0.60125446]\n",
      " [0.60140353]\n",
      " [0.6012308 ]\n",
      " [0.60129136]\n",
      " [0.6013973 ]\n",
      " [0.60128   ]\n",
      " [0.60132885]\n",
      " [0.60140765]\n",
      " [0.6012749 ]\n",
      " [0.6013231 ]\n",
      " [0.60132986]\n",
      " [0.60137707]\n",
      " [0.6014468 ]\n",
      " [0.6014842 ]\n",
      " [0.60144997]\n",
      " [0.60134524]\n",
      " [0.6009721 ]\n",
      " [0.6008727 ]\n",
      " [0.60085696]\n",
      " [0.6009711 ]\n",
      " [0.60083145]\n",
      " [0.6009959 ]\n",
      " [0.59952205]\n",
      " [0.5996583 ]\n",
      " [0.5996726 ]\n",
      " [0.5999339 ]\n",
      " [0.60039955]\n",
      " [0.6001471 ]\n",
      " [0.6002247 ]\n",
      " [0.60045964]\n",
      " [0.60065484]\n",
      " [0.6009115 ]\n",
      " [0.6009082 ]\n",
      " [0.60092354]\n",
      " [0.6009619 ]\n",
      " [0.6010872 ]\n",
      " [0.60109395]\n",
      " [0.60115814]\n",
      " [0.60108507]\n",
      " [0.6009318 ]\n",
      " [0.6008076 ]\n",
      " [0.60052586]\n",
      " [0.6004283 ]\n",
      " [0.60042506]\n",
      " [0.6006341 ]\n",
      " [0.6006881 ]\n",
      " [0.600875  ]\n",
      " [0.600933  ]\n",
      " [0.60049117]\n",
      " [0.6003368 ]\n",
      " [0.6002493 ]\n",
      " [0.60005796]\n",
      " [0.60037035]\n",
      " [0.60045546]\n",
      " [0.6006071 ]\n",
      " [0.6007366 ]\n",
      " [0.60047454]\n",
      " [0.6004961 ]\n",
      " [0.6002688 ]\n",
      " [0.5999978 ]\n",
      " [0.6003839 ]\n",
      " [0.6001387 ]]\n",
      "[0.6011954  0.6011973  0.6012346  0.60093606 0.6011977  0.6010232\n",
      " 0.6002477  0.60086226 0.6009017  0.6010343  0.6011956  0.6012196\n",
      " 0.6011208  0.6011396  0.60112107 0.6006004  0.60076237 0.60089296\n",
      " 0.6009108  0.6008745  0.60083675 0.6009592  0.6008968  0.6008712\n",
      " 0.60104966 0.6005645  0.6008312  0.6008161  0.60085934 0.60079426\n",
      " 0.60100305 0.60113215 0.6012562  0.601356   0.60137516 0.60146576\n",
      " 0.60120344 0.6012565  0.60108286 0.6011159  0.6012259  0.6008687\n",
      " 0.600608   0.60095084 0.60081136 0.60090786 0.6009801  0.6011433\n",
      " 0.6012537  0.6010925  0.6009539  0.6010122  0.6010114  0.6009455\n",
      " 0.60106856 0.6011331  0.60118383 0.6012072  0.6012544  0.60122854\n",
      " 0.601302   0.60144806 0.601413   0.60145736 0.60145813 0.60149735\n",
      " 0.60146976 0.6015005  0.60154355 0.60153544 0.60164505 0.60170865\n",
      " 0.601729   0.6018469  0.6018743  0.60178447 0.6015969  0.6016736\n",
      " 0.60149235 0.60106695 0.60046315 0.5985844  0.5984875  0.59818864\n",
      " 0.5985616  0.598809   0.5982895  0.59933805 0.5994437  0.59933865\n",
      " 0.5996721  0.5999515  0.60005516 0.59973323 0.5999305  0.60028505\n",
      " 0.6004695  0.60065407 0.60013026 0.5998776  0.59954596 0.59987277\n",
      " 0.5999137  0.59987307 0.59993535 0.6001213  0.6000822  0.6001339\n",
      " 0.5999484  0.5999575  0.6000304  0.60008955 0.60009223 0.6001564\n",
      " 0.600037   0.599549   0.60001725 0.6004629  0.6000522  0.6003122\n",
      " 0.6003595  0.59999084 0.60033995 0.60024565 0.6002542  0.60009927\n",
      " 0.6004995  0.6007365  0.6006052  0.60065407 0.6003777  0.60036975\n",
      " 0.6004579  0.6002561  0.59983397 0.59969944 0.5997101  0.5996967\n",
      " 0.5996513  0.5998882  0.6000864  0.5998784  0.6002764  0.6007117\n",
      " 0.60044646 0.6003453  0.6003676  0.6005938  0.6004784  0.6004947\n",
      " 0.600758   0.6010004  0.6004029  0.6006073  0.6006658  0.60062057\n",
      " 0.6006213  0.60063803 0.60052806 0.6004613  0.6003059  0.59990466\n",
      " 0.6003541  0.6003354  0.6004704  0.6004015  0.6003125  0.6000917\n",
      " 0.5997245  0.59917665 0.59932053 0.59908915 0.5993873  0.5995475\n",
      " 0.5997412  0.5996869  0.5996997  0.60035545 0.6006571  0.6008249\n",
      " 0.6009088  0.6005823  0.6002885  0.60029894 0.60056746 0.6008112\n",
      " 0.60085785 0.60104674 0.60120285 0.60125446 0.60140353 0.6012308\n",
      " 0.60129136 0.6013973  0.60128    0.60132885 0.60140765 0.6012749\n",
      " 0.6013231  0.60132986 0.60137707 0.6014468  0.6014842  0.60144997\n",
      " 0.60134524 0.6009721  0.6008727  0.60085696 0.6009711  0.60083145\n",
      " 0.6009959  0.59952205 0.5996583  0.5996726  0.5999339  0.60039955\n",
      " 0.6001471  0.6002247  0.60045964 0.60065484 0.6009115  0.6009082\n",
      " 0.60092354 0.6009619  0.6010872  0.60109395 0.60115814 0.60108507\n",
      " 0.6009318  0.6008076  0.60052586 0.6004283  0.60042506 0.6006341\n",
      " 0.6006881  0.600875   0.600933   0.60049117 0.6003368  0.6002493\n",
      " 0.60005796 0.60037035 0.60045546 0.6006071  0.6007366  0.60047454\n",
      " 0.6004961  0.6002688  0.5999978  0.6003839  0.6001387 ]\n",
      "NVO 0.012312148266318323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     76.776775  0.000000 2013-01-03   9598300 -0.001413\n",
      "2     76.776775  0.000000 2013-01-04  11631800  0.011386\n",
      "3     76.776775  0.000000 2013-01-07   7548800 -0.002098\n",
      "4     76.776775  0.000000 2013-01-08   9825300  0.000140\n",
      "5     76.776775  0.000000 2013-01-09   7672800  0.004471\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  22.357813  3.818837 2018-12-24   7531900 -0.041851\n",
      "1506  32.109627  3.885349 2018-12-26   9253000  0.031018\n",
      "1507  33.730988  3.917824 2018-12-27   9918700  0.005509\n",
      "1508  33.558369  3.780837 2018-12-28   6537200 -0.001100\n",
      "1509  37.909004  3.673635 2018-12-31   7409900  0.013889\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 7.67767746e+01  0.00000000e+00  9.59830000e+06 -1.41273000e-03]\n",
      " [ 7.67767746e+01  0.00000000e+00  1.16318000e+07  1.13855158e-02]\n",
      " [ 7.67767746e+01  0.00000000e+00  7.54880000e+06 -2.09842788e-03]\n",
      " ...\n",
      " [ 5.17152804e+01  1.51739114e+00  3.75360000e+06  3.42056884e-03]\n",
      " [ 5.16531572e+01  1.45686307e+00  2.48500000e+06 -7.09196910e-05]\n",
      " [ 4.65923954e+01  1.46923035e+00  4.45290000e+06 -5.99432914e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\2841010685.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 332)           447536    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 332)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                19980     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 468757 (1.79 MB)\n",
      "Trainable params: 468757 (1.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "JNJ None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1828WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 109ms/step - loss: 0.1828\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0109WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 116ms/step - loss: 0.0109\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0099WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 114ms/step - loss: 0.0099\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0079WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 0.0079\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0068WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 0.0068\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0060WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 109ms/step - loss: 0.0060\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0058WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 0.0058\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0055WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 112ms/step - loss: 0.0055\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0053WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 112ms/step - loss: 0.0053\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0052WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 0.0052\n",
      "6/6 [==============================] - 0s 38ms/step\n",
      "[[0.25521505]\n",
      " [0.2551991 ]\n",
      " [0.2551933 ]\n",
      " [0.25519162]\n",
      " [0.2552653 ]\n",
      " [0.25518563]\n",
      " [0.25515214]\n",
      " [0.25513345]\n",
      " [0.25527173]\n",
      " [0.25514823]\n",
      " [0.25519705]\n",
      " [0.255191  ]\n",
      " [0.25522003]\n",
      " [0.25506213]\n",
      " [0.25522256]\n",
      " [0.25517088]\n",
      " [0.25520822]\n",
      " [0.2550941 ]\n",
      " [0.2551849 ]\n",
      " [0.25524965]\n",
      " [0.25522587]\n",
      " [0.255234  ]\n",
      " [0.25518963]\n",
      " [0.25520626]\n",
      " [0.2552848 ]\n",
      " [0.25515252]\n",
      " [0.25515386]\n",
      " [0.25521502]\n",
      " [0.25518256]\n",
      " [0.25516266]\n",
      " [0.25517178]\n",
      " [0.25510982]\n",
      " [0.25515914]\n",
      " [0.25518432]\n",
      " [0.25508764]\n",
      " [0.25522327]\n",
      " [0.25522283]\n",
      " [0.25526184]\n",
      " [0.2551363 ]\n",
      " [0.2550861 ]\n",
      " [0.25520027]\n",
      " [0.25510293]\n",
      " [0.25520092]\n",
      " [0.2552549 ]\n",
      " [0.25512916]\n",
      " [0.25521833]\n",
      " [0.25521037]\n",
      " [0.25521728]\n",
      " [0.25517958]\n",
      " [0.2550764 ]\n",
      " [0.25511095]\n",
      " [0.25513792]\n",
      " [0.25516403]\n",
      " [0.2551295 ]\n",
      " [0.25516462]\n",
      " [0.25512138]\n",
      " [0.25518817]\n",
      " [0.2552152 ]\n",
      " [0.25518227]\n",
      " [0.25509238]\n",
      " [0.25508845]\n",
      " [0.25523177]\n",
      " [0.25515497]\n",
      " [0.25520793]\n",
      " [0.25515455]\n",
      " [0.25522208]\n",
      " [0.25513896]\n",
      " [0.25519747]\n",
      " [0.25518334]\n",
      " [0.25514355]\n",
      " [0.25508755]\n",
      " [0.2551371 ]\n",
      " [0.25511268]\n",
      " [0.2550931 ]\n",
      " [0.25420544]\n",
      " [0.25506675]\n",
      " [0.25526813]\n",
      " [0.25519508]\n",
      " [0.2550102 ]\n",
      " [0.25497004]\n",
      " [0.2545662 ]\n",
      " [0.2551921 ]\n",
      " [0.25457957]\n",
      " [0.25382626]\n",
      " [0.2550272 ]\n",
      " [0.25497723]\n",
      " [0.25450173]\n",
      " [0.25525853]\n",
      " [0.25522673]\n",
      " [0.2551895 ]\n",
      " [0.25519705]\n",
      " [0.25531426]\n",
      " [0.25531286]\n",
      " [0.2550356 ]\n",
      " [0.25515392]\n",
      " [0.2552329 ]\n",
      " [0.25535017]\n",
      " [0.25527647]\n",
      " [0.2551751 ]\n",
      " [0.25502232]\n",
      " [0.25495416]\n",
      " [0.25531092]\n",
      " [0.255288  ]\n",
      " [0.25510928]\n",
      " [0.25527608]\n",
      " [0.25537384]\n",
      " [0.2553142 ]\n",
      " [0.2551647 ]\n",
      " [0.25528374]\n",
      " [0.2550865 ]\n",
      " [0.25524932]\n",
      " [0.25513303]\n",
      " [0.25495517]\n",
      " [0.25529438]\n",
      " [0.25521392]\n",
      " [0.2547801 ]\n",
      " [0.25498667]\n",
      " [0.25532624]\n",
      " [0.25519916]\n",
      " [0.2552362 ]\n",
      " [0.2552429 ]\n",
      " [0.25484928]\n",
      " [0.25539482]\n",
      " [0.25534534]\n",
      " [0.25526553]\n",
      " [0.25505614]\n",
      " [0.2553343 ]\n",
      " [0.25530782]\n",
      " [0.25523466]\n",
      " [0.25531086]\n",
      " [0.25526488]\n",
      " [0.25527805]\n",
      " [0.2550625 ]\n",
      " [0.25495887]\n",
      " [0.25521415]\n",
      " [0.25513977]\n",
      " [0.2552172 ]\n",
      " [0.25516903]\n",
      " [0.25525588]\n",
      " [0.25528115]\n",
      " [0.25524443]\n",
      " [0.25504103]\n",
      " [0.25516057]\n",
      " [0.25491673]\n",
      " [0.25513923]\n",
      " [0.2552749 ]\n",
      " [0.25513747]\n",
      " [0.25510603]\n",
      " [0.25525483]\n",
      " [0.25532827]\n",
      " [0.25532222]\n",
      " [0.25512776]\n",
      " [0.25514784]\n",
      " [0.2552318 ]\n",
      " [0.2550664 ]\n",
      " [0.2552243 ]\n",
      " [0.25514442]\n",
      " [0.2550997 ]\n",
      " [0.25521976]\n",
      " [0.2550477 ]\n",
      " [0.2551588 ]\n",
      " [0.25486666]\n",
      " [0.25524965]\n",
      " [0.25497505]\n",
      " [0.25528777]\n",
      " [0.25523984]\n",
      " [0.25514758]\n",
      " [0.25524843]\n",
      " [0.25519776]\n",
      " [0.25522298]\n",
      " [0.25506732]\n",
      " [0.2551701 ]\n",
      " [0.25517213]\n",
      " [0.25512183]\n",
      " [0.25503954]\n",
      " [0.25502104]\n",
      " [0.2552123 ]\n",
      " [0.25511533]\n",
      " [0.25510672]\n",
      " [0.25519362]\n",
      " [0.2551172 ]\n",
      " [0.25514808]\n",
      " [0.25509855]\n",
      " [0.25518316]\n",
      " [0.25508144]\n",
      " [0.2551798 ]\n",
      " [0.2552452 ]\n",
      " [0.25527146]\n",
      " [0.25519624]\n",
      " [0.25517926]\n",
      " [0.25524575]\n",
      " [0.2550932 ]\n",
      " [0.25523058]\n",
      " [0.25497484]\n",
      " [0.25503173]\n",
      " [0.25527757]\n",
      " [0.2550848 ]\n",
      " [0.25505382]\n",
      " [0.25519097]\n",
      " [0.25524455]\n",
      " [0.2553063 ]\n",
      " [0.25513327]\n",
      " [0.25525445]\n",
      " [0.255234  ]\n",
      " [0.25514644]\n",
      " [0.25518328]\n",
      " [0.25518483]\n",
      " [0.2550837 ]\n",
      " [0.25521797]\n",
      " [0.25519317]\n",
      " [0.2551636 ]\n",
      " [0.25518256]\n",
      " [0.25519502]\n",
      " [0.25515974]\n",
      " [0.2551497 ]\n",
      " [0.25511822]\n",
      " [0.2552126 ]\n",
      " [0.25524133]\n",
      " [0.2552496 ]\n",
      " [0.25525635]\n",
      " [0.2550851 ]\n",
      " [0.255149  ]\n",
      " [0.255215  ]\n",
      " [0.25522894]\n",
      " [0.2551779 ]\n",
      " [0.25513223]\n",
      " [0.25520656]\n",
      " [0.25517392]\n",
      " [0.2551398 ]\n",
      " [0.25510392]\n",
      " [0.25525633]\n",
      " [0.25520185]\n",
      " [0.25517714]\n",
      " [0.25515383]\n",
      " [0.25519085]\n",
      " [0.25518107]\n",
      " [0.2551717 ]\n",
      " [0.25514108]\n",
      " [0.25517675]\n",
      " [0.2551794 ]\n",
      " [0.25513062]\n",
      " [0.25516957]\n",
      " [0.25503588]\n",
      " [0.25494108]\n",
      " [0.25497553]\n",
      " [0.25509453]\n",
      " [0.2551405 ]\n",
      " [0.2551339 ]\n",
      " [0.2552316 ]\n",
      " [0.25517955]\n",
      " [0.25505856]]\n",
      "[0.25521505 0.2551991  0.2551933  0.25519162 0.2552653  0.25518563\n",
      " 0.25515214 0.25513345 0.25527173 0.25514823 0.25519705 0.255191\n",
      " 0.25522003 0.25506213 0.25522256 0.25517088 0.25520822 0.2550941\n",
      " 0.2551849  0.25524965 0.25522587 0.255234   0.25518963 0.25520626\n",
      " 0.2552848  0.25515252 0.25515386 0.25521502 0.25518256 0.25516266\n",
      " 0.25517178 0.25510982 0.25515914 0.25518432 0.25508764 0.25522327\n",
      " 0.25522283 0.25526184 0.2551363  0.2550861  0.25520027 0.25510293\n",
      " 0.25520092 0.2552549  0.25512916 0.25521833 0.25521037 0.25521728\n",
      " 0.25517958 0.2550764  0.25511095 0.25513792 0.25516403 0.2551295\n",
      " 0.25516462 0.25512138 0.25518817 0.2552152  0.25518227 0.25509238\n",
      " 0.25508845 0.25523177 0.25515497 0.25520793 0.25515455 0.25522208\n",
      " 0.25513896 0.25519747 0.25518334 0.25514355 0.25508755 0.2551371\n",
      " 0.25511268 0.2550931  0.25420544 0.25506675 0.25526813 0.25519508\n",
      " 0.2550102  0.25497004 0.2545662  0.2551921  0.25457957 0.25382626\n",
      " 0.2550272  0.25497723 0.25450173 0.25525853 0.25522673 0.2551895\n",
      " 0.25519705 0.25531426 0.25531286 0.2550356  0.25515392 0.2552329\n",
      " 0.25535017 0.25527647 0.2551751  0.25502232 0.25495416 0.25531092\n",
      " 0.255288   0.25510928 0.25527608 0.25537384 0.2553142  0.2551647\n",
      " 0.25528374 0.2550865  0.25524932 0.25513303 0.25495517 0.25529438\n",
      " 0.25521392 0.2547801  0.25498667 0.25532624 0.25519916 0.2552362\n",
      " 0.2552429  0.25484928 0.25539482 0.25534534 0.25526553 0.25505614\n",
      " 0.2553343  0.25530782 0.25523466 0.25531086 0.25526488 0.25527805\n",
      " 0.2550625  0.25495887 0.25521415 0.25513977 0.2552172  0.25516903\n",
      " 0.25525588 0.25528115 0.25524443 0.25504103 0.25516057 0.25491673\n",
      " 0.25513923 0.2552749  0.25513747 0.25510603 0.25525483 0.25532827\n",
      " 0.25532222 0.25512776 0.25514784 0.2552318  0.2550664  0.2552243\n",
      " 0.25514442 0.2550997  0.25521976 0.2550477  0.2551588  0.25486666\n",
      " 0.25524965 0.25497505 0.25528777 0.25523984 0.25514758 0.25524843\n",
      " 0.25519776 0.25522298 0.25506732 0.2551701  0.25517213 0.25512183\n",
      " 0.25503954 0.25502104 0.2552123  0.25511533 0.25510672 0.25519362\n",
      " 0.2551172  0.25514808 0.25509855 0.25518316 0.25508144 0.2551798\n",
      " 0.2552452  0.25527146 0.25519624 0.25517926 0.25524575 0.2550932\n",
      " 0.25523058 0.25497484 0.25503173 0.25527757 0.2550848  0.25505382\n",
      " 0.25519097 0.25524455 0.2553063  0.25513327 0.25525445 0.255234\n",
      " 0.25514644 0.25518328 0.25518483 0.2550837  0.25521797 0.25519317\n",
      " 0.2551636  0.25518256 0.25519502 0.25515974 0.2551497  0.25511822\n",
      " 0.2552126  0.25524133 0.2552496  0.25525635 0.2550851  0.255149\n",
      " 0.255215   0.25522894 0.2551779  0.25513223 0.25520656 0.25517392\n",
      " 0.2551398  0.25510392 0.25525633 0.25520185 0.25517714 0.25515383\n",
      " 0.25519085 0.25518107 0.2551717  0.25514108 0.25517675 0.2551794\n",
      " 0.25513062 0.25516957 0.25503588 0.25494108 0.25497553 0.25509453\n",
      " 0.2551405  0.2551339  0.2552316  0.25517955 0.25505856]\n",
      "JNJ 0.06405603628997583\n"
     ]
    }
   ],
   "source": [
    "mse = {}\n",
    "total_predicted_returns = pd.DataFrame()\n",
    "for ticker in stocks:\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "    df['RSI'] = calculate_rsi(df['Adj Close'])\n",
    "    df['ATR'] = calculate_atr(df[['High', 'Low', 'Close']])\n",
    "    stock_returns = returns[[ticker]].reset_index()\n",
    "    stock_returns['Date'] = stock_returns['Date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    df.reset_index(inplace=True)\n",
    "    df_stock = df[['RSI', 'ATR', 'Date', 'Volume']]\n",
    "    stock_returns.Date = pd.to_datetime(stock_returns.Date)\n",
    "    df_stock.Date = pd.to_datetime(df_stock.Date)\n",
    "    data = pd.merge(df_stock[['RSI', 'ATR', 'Date', 'Volume']],  stock_returns , on='Date')[1:].rename(columns={ticker : 'Returns'})\n",
    "    print(data)\n",
    "    # Split the data we try by years first\n",
    "    data = data[data['Date'] < '2019-01-01']\n",
    "    df_train = data[data['Date'] < '2018-01-01']\n",
    "    predicted_period = data[(data['Date'] >= '2018-01-01') & (data['Date'] < '2019-01-01')][['Date']]\n",
    "    print(len(data) - len(predicted_period) - 60)\n",
    "    df_test = data[len(data) - len(predicted_period) - 60:]\n",
    "    train_cols = [\"RSI\", \"ATR\", \"Volume\", \"Returns\"]\n",
    "    x = df_train[train_cols].values\n",
    "    print(x)\n",
    "    \n",
    "    #scaling\n",
    "    min_max_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    x_train = min_max_scaler.fit_transform(x)\n",
    "    x_test = min_max_scaler.transform(df_test[train_cols])\n",
    "    \n",
    "    x_t, y_t = build_timeseries(x_train, 1)\n",
    "    print(\"Training Size\", x_t.shape, y_t.shape)\n",
    "    \n",
    "    x_t_test, y_t_test = build_timeseries(x_test, 1)\n",
    "    print(\"Test Size\", x_t_test.shape, y_t_test.shape)\n",
    "    \n",
    "    x_left, x_val = train_test_split(x_t, test_size=0.2, shuffle=False)\n",
    "    y_left, y_val = train_test_split(y_t, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    tuner = kt.BayesianOptimization(\n",
    "        model_builder,\n",
    "        objective='val_loss',\n",
    "        max_trials=5)\n",
    "    tuner.search(x_t, y_t, epochs=5, validation_data=(x_val,y_val))\n",
    "    lstm_model  = tuner.get_best_models()[0]  \n",
    "    print(ticker,lstm_model.summary())\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    #Model Training\n",
    "    history_lstm = lstm_model.fit(x_t, y_t, epochs=params[\"EPOCHS\"], verbose=1, batch_size=BATCH_SIZE, callbacks=[callback],\n",
    "                                shuffle=False)\n",
    "    y_pred_lstm = lstm_model.predict(x_t_test, batch_size=BATCH_SIZE)\n",
    "    print(y_pred_lstm)\n",
    "    y_pred_lstm = y_pred_lstm.flatten()\n",
    "    print(y_pred_lstm)\n",
    "    error_lstm = mean_squared_error(y_t_test, y_pred_lstm)\n",
    "    mse[ticker] = error_lstm\n",
    "    print(ticker,error_lstm)\n",
    "    \n",
    "    y_pred_lstm_org = (y_pred_lstm * min_max_scaler.data_range_[1]) + min_max_scaler.data_min_[1]   #Inverse Transform \n",
    "    predicted_returns = pd.Series(y_pred_lstm_org).to_frame(ticker)\n",
    "    predicted_period = predicted_period.reset_index().drop(columns=['index'])\n",
    "    result = predicted_returns.join(predicted_period)\n",
    "    if  total_predicted_returns.empty:\n",
    "        total_predicted_returns= result \n",
    "    else: \n",
    "        total_predicted_returns = pd.merge(result, total_predicted_returns, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0ac1736",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_predicted_returns.to_csv(\"LSTM_total_predicted_returns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a6f5ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JNJ</th>\n",
       "      <th>Date</th>\n",
       "      <th>NVO</th>\n",
       "      <th>WMT</th>\n",
       "      <th>JPM</th>\n",
       "      <th>XOM</th>\n",
       "      <th>UNH</th>\n",
       "      <th>V</th>\n",
       "      <th>TSM</th>\n",
       "      <th>LLY</th>\n",
       "      <th>HSBC</th>\n",
       "      <th>META</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.187479</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0.400482</td>\n",
       "      <td>0.851292</td>\n",
       "      <td>1.276625</td>\n",
       "      <td>0.621484</td>\n",
       "      <td>2.294138</td>\n",
       "      <td>1.294258</td>\n",
       "      <td>0.405146</td>\n",
       "      <td>1.450552</td>\n",
       "      <td>0.481816</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.585421</td>\n",
       "      <td>0.387131</td>\n",
       "      <td>1.116622</td>\n",
       "      <td>0.633493</td>\n",
       "      <td>0.511192</td>\n",
       "      <td>1.013067</td>\n",
       "      <td>0.505889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.187405</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.400483</td>\n",
       "      <td>0.850694</td>\n",
       "      <td>1.275808</td>\n",
       "      <td>0.624629</td>\n",
       "      <td>2.294166</td>\n",
       "      <td>1.294105</td>\n",
       "      <td>0.405186</td>\n",
       "      <td>1.449007</td>\n",
       "      <td>0.481994</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.583733</td>\n",
       "      <td>0.387053</td>\n",
       "      <td>1.115542</td>\n",
       "      <td>0.631531</td>\n",
       "      <td>0.512316</td>\n",
       "      <td>1.012728</td>\n",
       "      <td>0.506011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.187378</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.400508</td>\n",
       "      <td>0.851775</td>\n",
       "      <td>1.275522</td>\n",
       "      <td>0.626881</td>\n",
       "      <td>2.294266</td>\n",
       "      <td>1.294041</td>\n",
       "      <td>0.405329</td>\n",
       "      <td>1.448271</td>\n",
       "      <td>0.482068</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.582224</td>\n",
       "      <td>0.387071</td>\n",
       "      <td>1.115245</td>\n",
       "      <td>0.629246</td>\n",
       "      <td>0.510768</td>\n",
       "      <td>1.012687</td>\n",
       "      <td>0.506570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.187370</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.400309</td>\n",
       "      <td>0.853269</td>\n",
       "      <td>1.276259</td>\n",
       "      <td>0.630076</td>\n",
       "      <td>2.294626</td>\n",
       "      <td>1.293972</td>\n",
       "      <td>0.405153</td>\n",
       "      <td>1.442085</td>\n",
       "      <td>0.481276</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.581937</td>\n",
       "      <td>0.387092</td>\n",
       "      <td>1.114246</td>\n",
       "      <td>0.628038</td>\n",
       "      <td>0.506848</td>\n",
       "      <td>1.012696</td>\n",
       "      <td>0.506658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.187713</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>0.400484</td>\n",
       "      <td>0.854592</td>\n",
       "      <td>1.275959</td>\n",
       "      <td>0.633020</td>\n",
       "      <td>2.295369</td>\n",
       "      <td>1.294471</td>\n",
       "      <td>0.405125</td>\n",
       "      <td>1.441783</td>\n",
       "      <td>0.481733</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.580866</td>\n",
       "      <td>0.386907</td>\n",
       "      <td>1.116062</td>\n",
       "      <td>0.629064</td>\n",
       "      <td>0.510714</td>\n",
       "      <td>1.012563</td>\n",
       "      <td>0.507224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1.187132</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>0.400016</td>\n",
       "      <td>0.844945</td>\n",
       "      <td>1.273794</td>\n",
       "      <td>0.922194</td>\n",
       "      <td>2.328972</td>\n",
       "      <td>1.293092</td>\n",
       "      <td>0.404957</td>\n",
       "      <td>1.443397</td>\n",
       "      <td>0.481059</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.720744</td>\n",
       "      <td>0.390519</td>\n",
       "      <td>1.114677</td>\n",
       "      <td>0.684076</td>\n",
       "      <td>0.487627</td>\n",
       "      <td>1.012886</td>\n",
       "      <td>0.507017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1.187102</td>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>0.399865</td>\n",
       "      <td>0.844872</td>\n",
       "      <td>1.272514</td>\n",
       "      <td>0.928911</td>\n",
       "      <td>2.330931</td>\n",
       "      <td>1.292784</td>\n",
       "      <td>0.404556</td>\n",
       "      <td>1.445079</td>\n",
       "      <td>0.480850</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.719267</td>\n",
       "      <td>0.390622</td>\n",
       "      <td>1.111705</td>\n",
       "      <td>0.685611</td>\n",
       "      <td>0.483614</td>\n",
       "      <td>1.012709</td>\n",
       "      <td>0.507593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1.187556</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>0.399684</td>\n",
       "      <td>0.845412</td>\n",
       "      <td>1.273991</td>\n",
       "      <td>0.934888</td>\n",
       "      <td>2.332260</td>\n",
       "      <td>1.293167</td>\n",
       "      <td>0.405026</td>\n",
       "      <td>1.445903</td>\n",
       "      <td>0.480990</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.724933</td>\n",
       "      <td>0.390542</td>\n",
       "      <td>1.113112</td>\n",
       "      <td>0.679355</td>\n",
       "      <td>0.485190</td>\n",
       "      <td>1.012827</td>\n",
       "      <td>0.508079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1.187314</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>0.399941</td>\n",
       "      <td>0.845919</td>\n",
       "      <td>1.274330</td>\n",
       "      <td>0.942548</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.292291</td>\n",
       "      <td>0.404412</td>\n",
       "      <td>1.446424</td>\n",
       "      <td>0.480522</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.721980</td>\n",
       "      <td>0.390401</td>\n",
       "      <td>1.109814</td>\n",
       "      <td>0.679753</td>\n",
       "      <td>0.485605</td>\n",
       "      <td>1.012765</td>\n",
       "      <td>0.508645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.186751</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>0.399778</td>\n",
       "      <td>0.845322</td>\n",
       "      <td>1.275024</td>\n",
       "      <td>0.950001</td>\n",
       "      <td>2.334056</td>\n",
       "      <td>1.292656</td>\n",
       "      <td>0.404821</td>\n",
       "      <td>1.447963</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>1.822085</td>\n",
       "      <td>0.720867</td>\n",
       "      <td>0.390385</td>\n",
       "      <td>1.111068</td>\n",
       "      <td>0.679225</td>\n",
       "      <td>0.483724</td>\n",
       "      <td>1.013070</td>\n",
       "      <td>0.509039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          JNJ       Date       NVO       WMT       JPM       XOM       UNH  \\\n",
       "0    1.187479 2018-01-02  0.400482  0.851292  1.276625  0.621484  2.294138   \n",
       "1    1.187405 2018-01-03  0.400483  0.850694  1.275808  0.624629  2.294166   \n",
       "2    1.187378 2018-01-04  0.400508  0.851775  1.275522  0.626881  2.294266   \n",
       "3    1.187370 2018-01-05  0.400309  0.853269  1.276259  0.630076  2.294626   \n",
       "4    1.187713 2018-01-08  0.400484  0.854592  1.275959  0.633020  2.295369   \n",
       "..        ...        ...       ...       ...       ...       ...       ...   \n",
       "246  1.187132 2018-12-24  0.400016  0.844945  1.273794  0.922194  2.328972   \n",
       "247  1.187102 2018-12-26  0.399865  0.844872  1.272514  0.928911  2.330931   \n",
       "248  1.187556 2018-12-27  0.399684  0.845412  1.273991  0.934888  2.332260   \n",
       "249  1.187314 2018-12-28  0.399941  0.845919  1.274330  0.942548  2.333333   \n",
       "250  1.186751 2018-12-31  0.399778  0.845322  1.275024  0.950001  2.334056   \n",
       "\n",
       "            V       TSM       LLY      HSBC      META      TSLA      NVDA  \\\n",
       "0    1.294258  0.405146  1.450552  0.481816  1.822085  0.585421  0.387131   \n",
       "1    1.294105  0.405186  1.449007  0.481994  1.822085  0.583733  0.387053   \n",
       "2    1.294041  0.405329  1.448271  0.482068  1.822085  0.582224  0.387071   \n",
       "3    1.293972  0.405153  1.442085  0.481276  1.822085  0.581937  0.387092   \n",
       "4    1.294471  0.405125  1.441783  0.481733  1.822085  0.580866  0.386907   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "246  1.293092  0.404957  1.443397  0.481059  1.822085  0.720744  0.390519   \n",
       "247  1.292784  0.404556  1.445079  0.480850  1.822085  0.719267  0.390622   \n",
       "248  1.293167  0.405026  1.445903  0.480990  1.822085  0.724933  0.390542   \n",
       "249  1.292291  0.404412  1.446424  0.480522  1.822085  0.721980  0.390401   \n",
       "250  1.292656  0.404821  1.447963  0.480665  1.822085  0.720867  0.390385   \n",
       "\n",
       "         AMZN     GOOGL      GOOG      MSFT      AAPL  \n",
       "0    1.116622  0.633493  0.511192  1.013067  0.505889  \n",
       "1    1.115542  0.631531  0.512316  1.012728  0.506011  \n",
       "2    1.115245  0.629246  0.510768  1.012687  0.506570  \n",
       "3    1.114246  0.628038  0.506848  1.012696  0.506658  \n",
       "4    1.116062  0.629064  0.510714  1.012563  0.507224  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "246  1.114677  0.684076  0.487627  1.012886  0.507017  \n",
       "247  1.111705  0.685611  0.483614  1.012709  0.507593  \n",
       "248  1.113112  0.679355  0.485190  1.012827  0.508079  \n",
       "249  1.109814  0.679753  0.485605  1.012765  0.508645  \n",
       "250  1.111068  0.679225  0.483724  1.013070  0.509039  \n",
       "\n",
       "[251 rows x 19 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_predicted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890c103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
